{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Model Training Fundamentals\n",
    "\n",
    "This notebook demonstrates the essential components of training machine learning models in PyTorch, covering data handling, loss functions, optimizers, and complete training loops with monitoring.\n",
    "\n",
    "## Learning Objectives\n",
    "- Master efficient data handling with `Dataset` and `DataLoader`\n",
    "- Understand various loss functions and their applications\n",
    "- Explore PyTorch optimizers and hyperparameter tuning\n",
    "- Implement complete training loops with TensorBoard monitoring\n",
    "- Identify and prevent overfitting\n",
    "- Compare PyTorch training patterns with TensorFlow equivalents\n",
    "\n",
    "## Dataset Focus: FashionMNIST with Australian Context\n",
    "We'll use FashionMNIST dataset and create Australian-themed examples for practical learning, including:\n",
    "- **Australian Fashion Classification**: Classify clothing items popular in Australian fashion\n",
    "- **Multilingual Labels**: English-Vietnamese fashion terminology\n",
    "- **Real-world Applications**: Australian retail sentiment analysis\n",
    "\n",
    "**FashionMNIST Classes**: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle Boot\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Runtime Detection\n",
    "\n",
    "Following PyTorch best practices for cross-platform compatibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Detection and Setup\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Detect the runtime environment\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules or \"kaggle\" in os.environ.get('KAGGLE_URL_BASE', '')\n",
    "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
    "\n",
    "print(f\"\ud83c\udf10 Environment detected:\")\n",
    "print(f\"  - Local: {IS_LOCAL}\")\n",
    "print(f\"  - Google Colab: {IS_COLAB}\")\n",
    "print(f\"  - Kaggle: {IS_KAGGLE}\")\n",
    "\n",
    "# Platform-specific system setup\n",
    "if IS_COLAB:\n",
    "    print(\"\\n\ud83d\udd27 Setting up Google Colab environment...\")\n",
    "elif IS_KAGGLE:\n",
    "    print(\"\\n\ud83d\udd27 Setting up Kaggle environment...\")\n",
    "else:\n",
    "    print(\"\\n\ud83d\udd27 Setting up local environment...\")\n",
    "\n",
    "# Install required packages\n",
    "required_packages = [\n",
    "    \"torch\", \"torchvision\", \"torchaudio\",\n",
    "    \"matplotlib\", \"numpy\", \"tensorboard\"\n",
    "]\n",
    "\n",
    "print(\"\\n\ud83d\udce6 Verifying required packages...\")\n",
    "for package in required_packages:\n",
    "    if IS_COLAB or IS_KAGGLE:\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            __import__(package.replace('-', '_'))\n",
    "            print(f\"\u2713 {package}\")\n",
    "        except ImportError:\n",
    "            print(f\"\u26a0\ufe0f Installing {package}...\")\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
    "                          capture_output=True)\n",
    "            print(f\"\u2713 {package} installed\")\n",
    "\n",
    "print(\"\\n\u2705 Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Imports and Device Detection\n",
    "\n",
    "**Key Difference from TensorFlow**: PyTorch requires explicit device management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import platform\n",
    "\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__} ready!\")\n",
    "print(f\"\ud83d\udda5\ufe0f CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Device detection function\n",
    "def detect_device():\n",
    "    \"\"\"Detect best available PyTorch device.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        print(f\"\ud83d\ude80 Using CUDA: {gpu_name}\")\n",
    "        return device, f\"CUDA GPU: {gpu_name}\"\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(f\"\ud83c\udf4e Using Apple Silicon MPS\")\n",
    "        return device, \"Apple Silicon MPS\"\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        cpu_count = torch.get_num_threads()\n",
    "        print(f\"\ud83d\udcbb Using CPU with {cpu_count} threads\")\n",
    "        return device, f\"CPU ({cpu_count} threads)\"\n",
    "\n",
    "DEVICE, device_info = detect_device()\n",
    "print(f\"\\n\u2705 Selected device: {DEVICE}\")\n",
    "print(f\"\ud83d\udcca Device info: {device_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TensorBoard Configuration\n",
    "\n",
    "**MANDATORY**: All PyTorch training must include TensorBoard logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard log directory setup\n",
    "def get_run_logdir(prefix=\"fashion_training\"):\n",
    "    \"\"\"Generate unique log directory.\"\"\"\n",
    "    if IS_COLAB:\n",
    "        root_logdir = \"/content/tensorboard_logs\"\n",
    "    elif IS_KAGGLE:\n",
    "        root_logdir = \"./tensorboard_logs\"\n",
    "    else:\n",
    "        root_logdir = \"./tensorboard_logs\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "    logdir = os.path.join(root_logdir, f\"{prefix}_{timestamp}\")\n",
    "    os.makedirs(logdir, exist_ok=True)\n",
    "    return logdir\n",
    "\n",
    "log_dir = get_run_logdir(\"australian_fashion\")\n",
    "print(f\"\ud83d\udcca TensorBoard logs: {log_dir}\")\n",
    "print(f\"\ud83d\udca1 View with: tensorboard --logdir={log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Handling: Dataset and DataLoader\n",
    "\n",
    "**Core Concept**: PyTorch separates data access (`Dataset`) from data loading (`DataLoader`).\n",
    "\n",
    "### TensorFlow vs PyTorch:\n",
    "- **TensorFlow**: `tf.data.Dataset` with built-in batching\n",
    "- **PyTorch**: Explicit `Dataset` + `DataLoader` + `transforms`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "print(\"\ud83d\udce5 Loading FashionMNIST dataset...\")\n",
    "\n",
    "try:\n",
    "    # Download FashionMNIST\n",
    "    full_trainset = torchvision.datasets.FashionMNIST(\n",
    "        root='./data', train=True, download=True, transform=transform_train\n",
    "    )\n",
    "    testset = torchvision.datasets.FashionMNIST(\n",
    "        root='./data', train=False, download=True, transform=transform_test\n",
    "    )\n",
    "    print(\"\u2705 FashionMNIST downloaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u26a0\ufe0f Download failed: {type(e).__name__}\")\n",
    "    print(\"\ud83d\udd04 Creating synthetic dataset...\")\n",
    "    \n",
    "    from torch.utils.data import TensorDataset\n",
    "    \n",
    "    # Synthetic 28x28 grayscale images\n",
    "    synthetic_train_images = torch.randn(5000, 1, 28, 28)\n",
    "    synthetic_train_labels = torch.randint(0, 10, (5000,))\n",
    "    synthetic_test_images = torch.randn(1000, 1, 28, 28)\n",
    "    synthetic_test_labels = torch.randint(0, 10, (1000,))\n",
    "    \n",
    "    # Normalize\n",
    "    synthetic_train_images = (synthetic_train_images - 0.5) / 0.5\n",
    "    synthetic_test_images = (synthetic_test_images - 0.5) / 0.5\n",
    "    \n",
    "    full_trainset = TensorDataset(synthetic_train_images, synthetic_train_labels)\n",
    "    testset = TensorDataset(synthetic_test_images, synthetic_test_labels)\n",
    "    print(\"\u2705 Synthetic dataset created!\")\n",
    "\n",
    "# Split training set for validation\n",
    "train_size = int(0.8 * len(full_trainset))\n",
    "val_size = len(full_trainset) - train_size\n",
    "trainset, valset = random_split(full_trainset, [train_size, val_size])\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Dataset sizes:\")\n",
    "print(f\"  Training: {len(trainset):,} samples\")\n",
    "print(f\"  Validation: {len(valset):,} samples\")\n",
    "print(f\"  Test: {len(testset):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Australian fashion context with multilingual labels\n",
    "fashion_classes_en = [\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot'\n",
    "]\n",
    "\n",
    "fashion_classes_vi = [\n",
    "    '\u00c1o thun/\u00c1o ki\u1ec3u', 'Qu\u1ea7n d\u00e0i', '\u00c1o len', 'V\u00e1y', '\u00c1o kho\u00e1c',\n",
    "    'D\u00e9p/Sandal', '\u00c1o s\u01a1 mi', 'Gi\u00e0y th\u1ec3 thao', 'T\u00fai x\u00e1ch', 'Gi\u00e0y c\u1ed5 th\u1ea5p'\n",
    "]\n",
    "\n",
    "# Australian seasonal fashion context\n",
    "aus_fashion_context = {\n",
    "    'sydney_summer': ['T-shirt/top', 'Sandal', 'Dress'],\n",
    "    'melbourne_winter': ['Coat', 'Pullover', 'Ankle Boot'],\n",
    "    'business_casual': ['Shirt', 'Trouser', 'Bag'],\n",
    "    'active_lifestyle': ['Sneaker', 'T-shirt/top']\n",
    "}\n",
    "\n",
    "print(\"\ud83c\udde6\ud83c\uddfa Australian Fashion Context:\")\n",
    "print(f\"\ud83d\udcdd English: {fashion_classes_en[:5]}...\")\n",
    "print(f\"\ud83d\udcdd Vietnamese: {fashion_classes_vi[:5]}...\")\n",
    "print(f\"\ud83c\udf1e Sydney summer essentials: {aus_fashion_context['sydney_summer']}\")\n",
    "print(f\"\u2744\ufe0f Melbourne winter must-haves: {aus_fashion_context['melbourne_winter']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimized DataLoaders\n",
    "# Batch size based on device capabilities\n",
    "if DEVICE.type == 'cuda':\n",
    "    batch_size = 64\n",
    "    num_workers = 4\n",
    "elif DEVICE.type == 'mps':\n",
    "    batch_size = 32\n",
    "    num_workers = 2\n",
    "else:\n",
    "    batch_size = 16\n",
    "    num_workers = 0\n",
    "\n",
    "# Training DataLoader with shuffling\n",
    "trainloader = DataLoader(\n",
    "    trainset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Validation DataLoader (no shuffling)\n",
    "valloader = DataLoader(\n",
    "    valset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "# Test DataLoader (no shuffling)\n",
    "testloader = DataLoader(\n",
    "    testset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "print(f\"\ud83d\udcca DataLoader Configuration:\")\n",
    "print(f\"  Batch size: {batch_size} (optimized for {DEVICE.type.upper()})\")\n",
    "print(f\"  Workers: {num_workers}\")\n",
    "print(f\"  Pin memory: {torch.cuda.is_available()}\")\n",
    "print(f\"  Training batches: {len(trainloader)}\")\n",
    "print(f\"  Validation batches: {len(valloader)}\")\n",
    "print(f\"  Test batches: {len(testloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Visualization\n",
    "\n",
    "Always visualize your data to ensure correct loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample fashion items\n",
    "def visualize_batch(dataloader, classes_en, classes_vi, title=\"Fashion Items\"):\n",
    "    \"\"\"Visualize a batch of fashion items with bilingual labels.\"\"\"\n",
    "    dataiter = iter(dataloader)\n",
    "    images, labels = next(dataiter)\n",
    "    \n",
    "    # Create subplot\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    fig.suptitle(f'\ud83c\udde6\ud83c\uddfa {title}', fontsize=16)\n",
    "    \n",
    "    for i in range(8):\n",
    "        row, col = i // 4, i % 4\n",
    "        \n",
    "        # Unnormalize image from [-1,1] to [0,1]\n",
    "        img = images[i].squeeze() / 2 + 0.5\n",
    "        axes[row, col].imshow(img, cmap='gray')\n",
    "        \n",
    "        # Bilingual labels\n",
    "        class_idx = labels[i].item()\n",
    "        en_label = classes_en[class_idx]\n",
    "        vi_label = classes_vi[class_idx]\n",
    "        \n",
    "        axes[row, col].set_title(f'{en_label}\\n({vi_label})', fontsize=9)\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# Visualize training data\n",
    "sample_images, sample_labels = visualize_batch(\n",
    "    trainloader, fashion_classes_en, fashion_classes_vi,\n",
    "    \"Australian Fashion Trends - Training Samples\"\n",
    ")\n",
    "\n",
    "print(f\"\\n\ud83c\udfaf Batch information:\")\n",
    "print(f\"  Shape: {sample_images.shape} (batch, channels, height, width)\")\n",
    "print(f\"  Labels: {sample_labels.shape}\")\n",
    "print(f\"  Data type: {sample_images.dtype}\")\n",
    "print(f\"  Value range: [{sample_images.min():.3f}, {sample_images.max():.3f}]\")\n",
    "\n",
    "# Australian context analysis\n",
    "sample_classes = [fashion_classes_en[sample_labels[i].item()] for i in range(min(4, len(sample_labels)))]\n",
    "print(f\"\\n\ud83c\udde6\ud83c\uddfa Sample items: {sample_classes}\")\n",
    "\n",
    "# Check seasonal relevance\n",
    "summer_items = [cls for cls in sample_classes if cls in aus_fashion_context['sydney_summer']]\n",
    "winter_items = [cls for cls in sample_classes if cls in aus_fashion_context['melbourne_winter']]\n",
    "\n",
    "if summer_items:\n",
    "    print(f\"\ud83c\udf1e Perfect for Sydney summer: {summer_items}\")\n",
    "if winter_items:\n",
    "    print(f\"\u2744\ufe0f Essential for Melbourne winter: {winter_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Architecture: Australian Fashion CNN\n",
    "\n",
    "**PyTorch vs TensorFlow Model Definition**:\n",
    "- **PyTorch**: Explicit `nn.Module` subclass with `__init__` and `forward`\n",
    "- **TensorFlow**: `tf.keras.Sequential` or Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AustralianFashionCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN for Australian fashion classification using FashionMNIST.\n",
    "    \n",
    "    TensorFlow equivalent:\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10, dropout_rate=0.5):\n",
    "        super(AustralianFashionCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Classifier\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate * 0.6)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights - manual in PyTorch.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Conv blocks\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.adaptive_pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        # Classifier\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)  # No softmax with CrossEntropyLoss\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Get model information.\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        return {\n",
    "            'name': 'Australian Fashion CNN',\n",
    "            'total_parameters': total_params,\n",
    "            'trainable_parameters': trainable_params,\n",
    "            'architecture': 'Conv32->Conv64->Conv128->FC256->FC10'\n",
    "        }\n",
    "\n",
    "# Create and setup model\n",
    "model = AustralianFashionCNN(num_classes=10, dropout_rate=0.5).to(DEVICE)\n",
    "\n",
    "# Display model info\n",
    "model_info = model.get_model_info()\n",
    "print(f\"\ud83e\udde0 Model: {model_info['name']}\")\n",
    "print(f\"\ud83d\udd22 Total parameters: {model_info['total_parameters']:,}\")\n",
    "print(f\"\ud83c\udfaf Trainable parameters: {model_info['trainable_parameters']:,}\")\n",
    "print(f\"\ud83c\udfd7\ufe0f Architecture: {model_info['architecture']}\")\n",
    "print(f\"\ud83d\udd27 Device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    test_input = torch.randn(1, 1, 28, 28).to(DEVICE)\n",
    "    test_output = model(test_input)\n",
    "    print(f\"\u2705 Forward pass: {list(test_input.shape)} -> {list(test_output.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Loss Functions and Their Applications\n",
    "\n",
    "PyTorch offers various loss functions for different tasks. Understanding when to use each is crucial for effective training.\n",
    "\n",
    "### Common Loss Functions:\n",
    "- **`nn.CrossEntropyLoss`**: Multi-class classification (our choice for FashionMNIST)\n",
    "- **`nn.MSELoss`**: Mean Squared Error for regression\n",
    "- **`nn.BCELoss`**: Binary Cross-Entropy for binary classification\n",
    "- **`nn.KLDivLoss`**: Kullback-Leibler Divergence for probability distributions\n",
    "\n",
    "### TensorFlow Comparison:\n",
    "- **TensorFlow**: `loss='sparse_categorical_crossentropy'` in `model.compile()`\n",
    "- **PyTorch**: Explicit loss function instantiation and manual calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function selection and demonstration\n",
    "print(\"\ud83c\udfaf Loss Functions for Fashion Classification:\\n\")\n",
    "\n",
    "# Primary loss for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Alternative loss functions for comparison\n",
    "alternative_losses = {\n",
    "    'CrossEntropy': nn.CrossEntropyLoss(),\n",
    "    'CrossEntropy with weights': nn.CrossEntropyLoss(\n",
    "        weight=torch.ones(10) * 1.0  # Equal weights for all classes\n",
    "    ),\n",
    "    'CrossEntropy with label smoothing': nn.CrossEntropyLoss(\n",
    "        label_smoothing=0.1  # Reduces overconfidence\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"\ud83d\udcca Available loss functions:\")\n",
    "for name, loss_fn in alternative_losses.items():\n",
    "    print(f\"  \u2022 {name}: {loss_fn}\")\n",
    "\n",
    "print(f\"\\n\u2705 Selected loss function: {criterion}\")\n",
    "print(f\"\ud83c\udfaf Perfect for 10-class fashion classification\")\n",
    "\n",
    "# Demonstrate loss calculation\n",
    "with torch.no_grad():\n",
    "    # Simulate model output and target\n",
    "    sample_output = torch.randn(4, 10)  # 4 samples, 10 classes\n",
    "    sample_target = torch.randint(0, 10, (4,))  # Random class indices\n",
    "    \n",
    "    sample_loss = criterion(sample_output, sample_target)\n",
    "    print(f\"\\n\ud83d\udcc8 Example loss calculation:\")\n",
    "    print(f\"  Output shape: {sample_output.shape}\")\n",
    "    print(f\"  Target shape: {sample_target.shape}\")\n",
    "    print(f\"  Loss value: {sample_loss.item():.4f}\")\n",
    "\n",
    "# Australian fashion context: Class-specific insights\n",
    "print(f\"\\n\ud83c\udde6\ud83c\uddfa Fashion Classification Context:\")\n",
    "print(f\"  \u2022 CrossEntropyLoss works well for distinguishing between fashion categories\")\n",
    "print(f\"  \u2022 Label smoothing helps when similar items (e.g., 'Shirt' vs 'T-shirt') might be confused\")\n",
    "print(f\"  \u2022 Weighted loss can handle class imbalance in real Australian fashion datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optimizers and Hyperparameter Tuning\n",
    "\n",
    "Optimizers update model weights based on computed gradients. The choice of optimizer and its hyperparameters significantly impacts training.\n",
    "\n",
    "### Common Optimizers:\n",
    "- **`Adam`**: Adaptive learning rate, good default choice\n",
    "- **`AdamW`**: Adam with weight decay, better regularization\n",
    "- **`SGD`**: Stochastic Gradient Descent, classic approach\n",
    "- **`RMSprop`**: Adaptive learning rate for RNNs\n",
    "\n",
    "### Key Hyperparameters:\n",
    "- **Learning Rate**: Controls step size (most important!)\n",
    "- **Weight Decay**: L2 regularization to prevent overfitting\n",
    "- **Momentum**: Helps overcome local minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer selection and configuration\n",
    "print(\"\ud83d\udd27 Optimizer Configuration for Australian Fashion CNN:\\n\")\n",
    "\n",
    "# Learning rate based on device and batch size\n",
    "if DEVICE.type == 'cuda':\n",
    "    base_lr = 1e-3  # Higher LR for GPU\n",
    "elif DEVICE.type == 'mps':\n",
    "    base_lr = 8e-4  # Moderate LR for Apple Silicon\n",
    "else:\n",
    "    base_lr = 5e-4  # Lower LR for CPU\n",
    "\n",
    "# Scale learning rate with batch size\n",
    "learning_rate = base_lr * (batch_size / 32)\n",
    "\n",
    "# Primary optimizer: AdamW (Adam with weight decay)\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=1e-4,      # L2 regularization\n",
    "    betas=(0.9, 0.999),     # Momentum parameters\n",
    "    eps=1e-8                # Numerical stability\n",
    ")\n",
    "\n",
    "# Learning rate scheduler for better convergence\n",
    "scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer, \n",
    "    step_size=7,    # Reduce LR every 7 epochs\n",
    "    gamma=0.1       # Multiply LR by 0.1\n",
    ")\n",
    "\n",
    "print(f\"\u2705 Optimizer: {optimizer.__class__.__name__}\")\n",
    "print(f\"\ud83d\udcca Learning rate: {learning_rate:.6f} (scaled for {DEVICE.type.upper()})\")\n",
    "print(f\"\u2696\ufe0f Weight decay: {optimizer.param_groups[0]['weight_decay']}\")\n",
    "print(f\"\ud83d\udcc9 Scheduler: StepLR (decay every 7 epochs by 0.1)\")\n",
    "\n",
    "# Alternative optimizers for comparison\n",
    "alternative_optimizers = {\n",
    "    'AdamW': optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4),\n",
    "    'Adam': optim.Adam(model.parameters(), lr=learning_rate),\n",
    "    'SGD': optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4),\n",
    "    'RMSprop': optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "}\n",
    "\n",
    "print(f\"\\n\ud83d\udd04 Alternative optimizers available:\")\n",
    "for name, opt in alternative_optimizers.items():\n",
    "    print(f\"  \u2022 {name}: LR={opt.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "# Australian context: Optimizer insights\n",
    "print(f\"\\n\ud83c\udde6\ud83c\uddfa Optimizer Selection for Fashion Classification:\")\n",
    "print(f\"  \u2022 AdamW: Excellent for fashion image classification\")\n",
    "print(f\"  \u2022 Weight decay: Prevents overfitting on fashion categories\")\n",
    "print(f\"  \u2022 Learning rate scheduling: Helps fine-tune fashion features\")\n",
    "print(f\"  \u2022 Device optimization: {DEVICE.type.upper()}-specific learning rate\")\n",
    "\n",
    "# TensorFlow comparison\n",
    "print(f\"\\n\ud83d\udccb TensorFlow vs PyTorch Optimizer Setup:\")\n",
    "print(f\"  TensorFlow: model.compile(optimizer='adam', lr=0.001)\")\n",
    "print(f\"  PyTorch:    optimizer = optim.AdamW(model.parameters(), lr=0.001)\")\n",
    "print(f\"  Key difference: PyTorch requires manual optimizer step and scheduling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Complete Training Loop with TensorBoard Monitoring\n",
    "\n",
    "This is where PyTorch differs most from TensorFlow. Instead of `model.fit()`, we implement manual training loops with explicit control over each step.\n",
    "\n",
    "### Training Loop Components:\n",
    "1. **Training Phase**: `model.train()`, forward pass, loss calculation, backpropagation\n",
    "2. **Validation Phase**: `model.eval()`, inference without gradients\n",
    "3. **Logging**: TensorBoard metrics, loss tracking, overfitting detection\n",
    "4. **Checkpointing**: Save best model based on validation performance\n",
    "\n",
    "### TensorFlow vs PyTorch:\n",
    "- **TensorFlow**: `model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))`\n",
    "- **PyTorch**: Manual loops with `optimizer.zero_grad()`, `loss.backward()`, `optimizer.step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_australian_fashion_model(model, trainloader, valloader, criterion, optimizer, \n",
    "                                 scheduler, num_epochs=15, device=DEVICE, log_dir=log_dir):\n",
    "    \"\"\"\n",
    "    Complete training function with TensorBoard logging and overfitting detection.\n",
    "    \n",
    "    TensorFlow equivalent:\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=(x_val, y_val),\n",
    "        callbacks=[tensorboard_callback]\n",
    "    )\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to train\n",
    "        trainloader: Training data loader\n",
    "        valloader: Validation data loader\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        scheduler: Learning rate scheduler\n",
    "        num_epochs: Number of training epochs\n",
    "        device: Device to train on\n",
    "        log_dir: TensorBoard log directory\n",
    "    \n",
    "    Returns:\n",
    "        dict: Training history with losses and accuracies\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    patience = 5  # Early stopping patience\n",
    "    \n",
    "    print(f\"\ud83d\ude80 Starting Australian Fashion CNN Training\")\n",
    "    print(f\"\ud83d\udcca Device: {device}\")\n",
    "    print(f\"\ud83c\udfaf Target: Classify 10 fashion categories\")\n",
    "    print(f\"\ud83c\udf0f Context: Australian fashion with English-Vietnamese labels\")\n",
    "    print(f\"\ud83d\udcc8 Epochs: {num_epochs}\")\n",
    "    print(f\"\ud83d\udd22 Batch size: {trainloader.batch_size}\")\n",
    "    print(f\"\ud83d\udcca Training batches: {len(trainloader)}\")\n",
    "    print(f\"\ud83d\udcca Validation batches: {len(valloader)}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # =====================================\n",
    "        # TRAINING PHASE\n",
    "        # =====================================\n",
    "        model.train()  # Set to training mode\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "            # Move data to device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero gradients (essential in PyTorch)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Log batch-level metrics to TensorBoard\n",
    "            global_step = epoch * len(trainloader) + batch_idx\n",
    "            if batch_idx % 100 == 0:  # Log every 100 batches\n",
    "                writer.add_scalar('Loss/Train_Batch', loss.item(), global_step)\n",
    "                writer.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "                \n",
    "                # Log device-specific metrics\n",
    "                if device.type == 'cuda':\n",
    "                    gpu_memory = torch.cuda.memory_allocated(device) / 1024**3\n",
    "                    writer.add_scalar('Memory/GPU_Used_GB', gpu_memory, global_step)\n",
    "        \n",
    "        # Calculate epoch training metrics\n",
    "        epoch_train_loss = train_loss / len(trainloader)\n",
    "        epoch_train_acc = 100 * train_correct / train_total\n",
    "        \n",
    "        # =====================================\n",
    "        # VALIDATION PHASE\n",
    "        # =====================================\n",
    "        model.eval()  # Set to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradients for validation\n",
    "            for inputs, labels in valloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate epoch validation metrics\n",
    "        epoch_val_loss = val_loss / len(valloader)\n",
    "        epoch_val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['train_acc'].append(epoch_train_acc)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['val_acc'].append(epoch_val_acc)\n",
    "        \n",
    "        # Log epoch metrics to TensorBoard\n",
    "        writer.add_scalar('Loss/Train_Epoch', epoch_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', epoch_val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Train', epoch_train_acc, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', epoch_val_acc, epoch)\n",
    "        \n",
    "        # Log model parameters histogram\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                writer.add_histogram(f'Parameters/{name}', param, epoch)\n",
    "                if param.grad is not None:\n",
    "                    writer.add_histogram(f'Gradients/{name}', param.grad, epoch)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f'Epoch {epoch+1:2d}/{num_epochs}: '\n",
    "              f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}% | '\n",
    "              f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}% | '\n",
    "              f'LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if epoch_val_acc > best_val_acc:\n",
    "            best_val_acc = epoch_val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            print(f'  \ud83c\udfaf New best validation accuracy: {best_val_acc:.2f}%')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping check\n",
    "        if patience_counter >= patience:\n",
    "            print(f'\\n\u23f0 Early stopping at epoch {epoch+1} (patience={patience})')\n",
    "            break\n",
    "        \n",
    "        # Overfitting detection\n",
    "        if epoch >= 5:  # Check after some epochs\n",
    "            train_val_gap = epoch_train_acc - epoch_val_acc\n",
    "            if train_val_gap > 20:  # More than 20% gap\n",
    "                print(f'  \u26a0\ufe0f Potential overfitting detected: Train-Val gap = {train_val_gap:.2f}%')\n",
    "    \n",
    "    # Load best model state\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f'\\n\u2705 Loaded best model with validation accuracy: {best_val_acc:.2f}%')\n",
    "    \n",
    "    # Close TensorBoard writer\n",
    "    writer.close()\n",
    "    \n",
    "    print(f'\\n\ud83c\udfc1 Training completed!')\n",
    "    print(f'\ud83d\udcca Best validation accuracy: {best_val_acc:.2f}%')\n",
    "    print(f'\ud83d\udcc8 TensorBoard logs saved to: {log_dir}')\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Start training\n",
    "print(\"\ud83c\udfac Starting comprehensive training with TensorBoard logging...\\n\")\n",
    "training_history = train_australian_fashion_model(\n",
    "    model=model,\n",
    "    trainloader=trainloader,\n",
    "    valloader=valloader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=15,  # Reasonable number for demonstration\n",
    "    device=DEVICE,\n",
    "    log_dir=log_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. TensorBoard Visualization Instructions\n",
    "\n",
    "Monitor your training progress with comprehensive TensorBoard logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display TensorBoard viewing instructions\n",
    "print(\"=\" * 60)\n",
    "print(\"\ud83d\udcca TENSORBOARD VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Log directory: {log_dir}\")\n",
    "print(\"\\n\ud83d\ude80 To view TensorBoard:\")\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"   In Google Colab:\")\n",
    "    print(\"   1. Run: %load_ext tensorboard\")\n",
    "    print(f\"   2. Run: %tensorboard --logdir {log_dir}\")\n",
    "    print(\"   3. TensorBoard will appear inline in the notebook\")\n",
    "elif IS_KAGGLE:\n",
    "    print(\"   In Kaggle:\")\n",
    "    print(f\"   1. Download logs from: {log_dir}\")\n",
    "    print(\"   2. Run locally: tensorboard --logdir ./tensorboard_logs\")\n",
    "    print(\"   3. Open http://localhost:6006 in browser\")\n",
    "else:\n",
    "    print(\"   Locally:\")\n",
    "    print(f\"   1. Run: tensorboard --logdir {log_dir}\")\n",
    "    print(\"   2. Open http://localhost:6006 in browser\")\n",
    "\n",
    "print(\"\\n\ud83d\udcc8 Available visualizations:\")\n",
    "print(\"   \u2022 Scalars: Loss and accuracy curves over time\")\n",
    "print(\"   \u2022 Histograms: Model parameter and gradient distributions\")\n",
    "print(\"   \u2022 Learning Rate: LR scheduling visualization\")\n",
    "print(\"   \u2022 Memory Usage: GPU memory consumption (if available)\")\n",
    "print(\"   \u2022 Training Progress: Batch-level and epoch-level metrics\")\n",
    "\n",
    "print(\"\\n\ud83d\udd0d Key metrics to monitor:\")\n",
    "print(\"   \u2022 Training vs Validation Loss: Check for overfitting\")\n",
    "print(\"   \u2022 Training vs Validation Accuracy: Generalization performance\")\n",
    "print(\"   \u2022 Learning Rate: Ensure proper scheduling\")\n",
    "print(\"   \u2022 Parameter Histograms: Verify weights are updating\")\n",
    "print(\"   \u2022 Gradient Histograms: Check for vanishing/exploding gradients\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training Analysis and Overfitting Detection\n",
    "\n",
    "Analyze training results to understand model performance and detect potential issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "def plot_training_history(history, title=\"Australian Fashion CNN Training\"):\n",
    "    \"\"\"Plot training and validation metrics.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "    ax1.plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "    ax1.set_title('Model Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax2.plot(epochs, history['train_acc'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "    ax2.plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "    ax2.set_title('Model Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Analyze training results\n",
    "if 'training_history' in locals():\n",
    "    print(\"\ud83d\udcca Training Analysis:\\n\")\n",
    "    \n",
    "    # Plot training curves\n",
    "    plot_training_history(training_history, \"\ud83c\udde6\ud83c\uddfa Australian Fashion Classification Training\")\n",
    "    \n",
    "    # Final metrics\n",
    "    final_train_acc = training_history['train_acc'][-1]\n",
    "    final_val_acc = training_history['val_acc'][-1]\n",
    "    final_train_loss = training_history['train_loss'][-1]\n",
    "    final_val_loss = training_history['val_loss'][-1]\n",
    "    \n",
    "    print(f\"\ud83d\udcc8 Final Training Results:\")\n",
    "    print(f\"  Training Accuracy: {final_train_acc:.2f}%\")\n",
    "    print(f\"  Validation Accuracy: {final_val_acc:.2f}%\")\n",
    "    print(f\"  Training Loss: {final_train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss: {final_val_loss:.4f}\")\n",
    "    \n",
    "    # Overfitting analysis\n",
    "    accuracy_gap = final_train_acc - final_val_acc\n",
    "    loss_gap = final_val_loss - final_train_loss\n",
    "    \n",
    "    print(f\"\\n\ud83d\udd0d Overfitting Analysis:\")\n",
    "    print(f\"  Accuracy gap (Train - Val): {accuracy_gap:.2f}%\")\n",
    "    print(f\"  Loss gap (Val - Train): {loss_gap:.4f}\")\n",
    "    \n",
    "    if accuracy_gap > 15:\n",
    "        print(\"  \u26a0\ufe0f HIGH overfitting detected! Consider:\")\n",
    "        print(\"     \u2022 Increasing dropout rate\")\n",
    "        print(\"     \u2022 Adding more data augmentation\")\n",
    "        print(\"     \u2022 Reducing model complexity\")\n",
    "        print(\"     \u2022 Increasing weight decay\")\n",
    "    elif accuracy_gap > 8:\n",
    "        print(\"  \u26a0\ufe0f MODERATE overfitting detected. Monitor closely.\")\n",
    "    else:\n",
    "        print(\"  \u2705 Good generalization! Minimal overfitting.\")\n",
    "    \n",
    "    # Australian fashion context\n",
    "    print(f\"\\n\ud83c\udde6\ud83c\uddfa Australian Fashion Classification Performance:\")\n",
    "    if final_val_acc > 85:\n",
    "        print(f\"  \ud83c\udfaf EXCELLENT: {final_val_acc:.1f}% accuracy for 10 fashion categories\")\n",
    "        print(f\"  \ud83c\udfea Ready for Australian retail applications\")\n",
    "    elif final_val_acc > 75:\n",
    "        print(f\"  \u2705 GOOD: {final_val_acc:.1f}% accuracy, suitable for most applications\")\n",
    "        print(f\"  \ud83d\udd27 Consider fine-tuning for production use\")\n",
    "    else:\n",
    "        print(f\"  \u26a0\ufe0f NEEDS IMPROVEMENT: {final_val_acc:.1f}% accuracy\")\n",
    "        print(f\"  \ud83d\udcda Consider more training data or model adjustments\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No training history available. Run the training cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Learning Summary\n",
    "\n",
    "\ud83c\udf93 **You've successfully implemented PyTorch model training fundamentals!**\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "#### 1. **Data Handling Mastery**\n",
    "- \u2705 `Dataset` and `DataLoader` for efficient data loading\n",
    "- \u2705 Data transformations and augmentation\n",
    "- \u2705 Train/validation/test splits with `random_split`\n",
    "- \u2705 Batch size optimization for different devices\n",
    "\n",
    "#### 2. **Loss Functions & Optimizers**\n",
    "- \u2705 `CrossEntropyLoss` for multi-class classification\n",
    "- \u2705 `AdamW` optimizer with weight decay\n",
    "- \u2705 Learning rate scheduling with `StepLR`\n",
    "- \u2705 Device-specific hyperparameter tuning\n",
    "\n",
    "#### 3. **Complete Training Loop**\n",
    "- \u2705 Manual training loop vs TensorFlow's `model.fit()`\n",
    "- \u2705 Explicit gradient management: `zero_grad()`, `backward()`, `step()`\n",
    "- \u2705 Training/validation phases with `train()`/`eval()` modes\n",
    "- \u2705 Early stopping and best model checkpointing\n",
    "\n",
    "#### 4. **Monitoring & Analysis**\n",
    "- \u2705 Comprehensive TensorBoard logging\n",
    "- \u2705 Overfitting detection and prevention\n",
    "- \u2705 Training curve visualization\n",
    "- \u2705 Performance analysis and interpretation\n",
    "\n",
    "### Australian Context Integration:\n",
    "- \ud83c\udde6\ud83c\uddfa **Fashion Classification**: Applied to Australian clothing preferences\n",
    "- \ud83c\udf0f **Multilingual Support**: English-Vietnamese fashion terminology\n",
    "- \ud83c\udfd6\ufe0f **Seasonal Context**: Sydney summer vs Melbourne winter fashion\n",
    "- \ud83c\udfea **Real-world Application**: Ready for Australian retail deployment\n",
    "\n",
    "### TensorFlow \u2192 PyTorch Transition Complete!\n",
    "\n",
    "| Aspect | TensorFlow | PyTorch |\n",
    "|--------|------------|----------|\n",
    "| **Training** | `model.fit()` | Manual loops |\n",
    "| **Loss** | `loss='categorical_crossentropy'` | `nn.CrossEntropyLoss()` |\n",
    "| **Optimizer** | `optimizer='adam'` | `optim.AdamW()` |\n",
    "| **Monitoring** | Built-in callbacks | TensorBoard + manual logging |\n",
    "| **Control** | Less explicit | Full control over every step |\n",
    "\n",
    "### Next Steps:\n",
    "1. **Advanced Architectures**: Explore ResNet, EfficientNet for fashion\n",
    "2. **Transfer Learning**: Use pre-trained models for fashion classification\n",
    "3. **Hugging Face Integration**: Apply to NLP tasks with transformers\n",
    "4. **Production Deployment**: Optimize models for Australian retail systems\n",
    "5. **Advanced Training**: Mixed precision, distributed training, model optimization\n",
    "\n",
    "**Happy learning with PyTorch!** \ud83d\udd25\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook demonstrates core PyTorch training concepts through Australian fashion classification. For advanced tutorials, explore other notebooks in this repository.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}