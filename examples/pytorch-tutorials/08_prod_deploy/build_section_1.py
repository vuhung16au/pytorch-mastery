#!/usr/bin/env python3
"""
Final Complete Production Deployment Notebook Builder
All 4 sections: Evaluation Mode, TorchScript, C++ Deployment, TorchServe
"""

import json

def create_full_production_notebook():
    """Create the complete production deployment tutorial notebook with all sections."""
    
    cells = []
    
    # Title and Introduction
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "# Production Inference Deployment with PyTorch\n",
            "\n",
            "This comprehensive tutorial demonstrates how to deploy PyTorch models for production inference, covering essential deployment patterns from model preparation to advanced serving solutions.\n",
            "\n",
            "## Learning Objectives\n",
            "- ðŸŽ¯ **Prepare models for inference**: Evaluation mode and optimization techniques\n",
            "- ðŸš€ **Master TorchScript**: Convert Python models to optimized, production-ready format\n",
            "- ðŸ”§ **Deploy with C++**: Use libtorch for high-performance inference\n",
            "- ðŸ“¦ **Implement TorchServe**: Scalable model serving with built-in APIs\n",
            "\n",
            "## Tutorial Structure\n",
            "1. **Preparing the Model for Inference**: Evaluation Mode\n",
            "2. **TorchScript**: `jit.script` and `jit.trace` for production optimization\n",
            "3. **Deploying with C++**: libtorch integration examples\n",
            "4. **TorchServe**: Complete model serving solution\n",
            "\n",
            "## Use Case: Australian Tourism Sentiment Analysis\n",
            "We will build and deploy a multilingual sentiment analysis model for Australian tourism reviews (English + Vietnamese), demonstrating real-world production deployment scenarios.\n",
            "\n",
            "**Sample Use Cases:**\n",
            "- Hotel booking platforms analyzing customer reviews\n",
            "- Tourism boards monitoring social media sentiment\n",
            "- Travel agencies optimizing destination recommendations\n",
            "\n",
            "## TensorFlow vs PyTorch Deployment Comparison\n",
            "\n",
            "| Aspect | TensorFlow | PyTorch |\n",
            "|--------|------------|----------|\n",
            "| **Model Format** | SavedModel, TFLite | TorchScript, ONNX |\n",
            "| **Serving** | TensorFlow Serving | TorchServe |\n",
            "| **C++ Deployment** | TensorFlow C++ API | libtorch |\n",
            "| **Mobile** | TensorFlow Lite | PyTorch Mobile |\n",
            "| **Optimization** | TensorRT, XLA | TorchScript JIT |\n",
            "\n",
            "---"
        ]
    })
    
    # Environment Setup
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## Environment Setup and Runtime Detection\n",
            "\n",
            "Following PyTorch best practices for cross-platform production deployment:"
        ]
    })
    
    cells.append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Environment Detection and Setup\n",
            "import sys\n",
            "import subprocess\n",
            "import os\n",
            "import time\n",
            "import platform\n",
            "\n",
            "# Detect the runtime environment\n",
            "IS_COLAB = \"google.colab\" in sys.modules\n",
            "IS_KAGGLE = \"kaggle_secrets\" in sys.modules or \"kaggle\" in os.environ.get('KAGGLE_URL_BASE', '')\n",
            "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
            "\n",
            "print(f\"Environment detected:\")\n",
            "print(f\"  - Local: {IS_LOCAL}\")\n",
            "print(f\"  - Google Colab: {IS_COLAB}\")\n",
            "print(f\"  - Kaggle: {IS_KAGGLE}\")\n",
            "\n",
            "# Platform-specific system setup\n",
            "if IS_COLAB:\n",
            "    print(\"\\nSetting up Google Colab environment...\")\n",
            "    !apt update -qq\n",
            "    !apt install -y -qq software-properties-common\n",
            "elif IS_KAGGLE:\n",
            "    print(\"\\nSetting up Kaggle environment...\")\n",
            "    # Kaggle usually has most packages pre-installed\n",
            "else:\n",
            "    print(\"\\nSetting up local environment...\")\n",
            "\n",
            "# Install required packages for this notebook\n",
            "required_packages = [\n",
            "    \"torch\",\n",
            "    \"torchvision\", \n",
            "    \"transformers\",\n",
            "    \"datasets\",\n",
            "    \"tokenizers\",\n",
            "    \"pandas\",\n",
            "    \"matplotlib\",\n",
            "    \"seaborn\",\n",
            "    \"tensorboard\"\n",
            "]\n",
            "\n",
            "print(\"\\nInstalling required packages...\")\n",
            "for package in required_packages:\n",
            "    try:\n",
            "        if IS_COLAB or IS_KAGGLE:\n",
            "            !pip install -q {package}\n",
            "        else:\n",
            "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
            "                          capture_output=True, check=False)\n",
            "        print(f\"âœ“ {package}\")\n",
            "    except Exception as e:\n",
            "        print(f\"âš ï¸ {package}: {str(e)[:50]}...\")\n",
            "\n",
            "print(\"\\nðŸ”¥ Production deployment environment ready!\")"
        ]
    })
    
    # Device Detection (abbreviated to save space)
    cells.append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Core PyTorch imports for production deployment\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import torch.nn.functional as F\n",
            "import torch.optim as optim\n",
            "import torch.jit as jit\n",
            "from torch.utils.data import DataLoader, Dataset\n",
            "from torch.utils.tensorboard import SummaryWriter\n",
            "\n",
            "# Standard libraries\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "import json\n",
            "import tempfile\n",
            "import warnings\n",
            "from datetime import datetime\n",
            "from pathlib import Path\n",
            "\n",
            "warnings.filterwarnings('ignore')\n",
            "\n",
            "# Simplified device detection\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "print(f\"âœ… PyTorch {torch.__version__} ready on {device}!\")\n",
            "DEVICE = device"
        ]
    })
    
    # Model Definition
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## Sample Model: Australian Tourism Sentiment Analyzer\n",
            "\n",
            "Let's create a production-ready sentiment analysis model for Australian tourism reviews with multilingual support (English + Vietnamese):"
        ]
    })
    
    cells.append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "class AustralianTourismSentimentAnalyzer(nn.Module):\n",
            "    \"\"\"\n",
            "    Production-ready sentiment analysis model for Australian tourism reviews.\n",
            "    \n",
            "    Supports multilingual analysis (English + Vietnamese) for:\n",
            "    - Hotel and restaurant reviews\n",
            "    - Tourist attraction feedback\n",
            "    - Travel experience sentiment\n",
            "    \"\"\"\n",
            "    \n",
            "    def __init__(self, vocab_size=1000, embed_dim=64, hidden_dim=128, num_classes=3):\n",
            "        super(AustralianTourismSentimentAnalyzer, self).__init__()\n",
            "        \n",
            "        # Store hyperparameters for TorchScript compatibility\n",
            "        self.vocab_size = vocab_size\n",
            "        self.embed_dim = embed_dim\n",
            "        self.hidden_dim = hidden_dim\n",
            "        self.num_classes = num_classes\n",
            "        \n",
            "        # Model layers\n",
            "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
            "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
            "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
            "        \n",
            "        # Initialize weights\n",
            "        self._init_weights()\n",
            "    \n",
            "    def _init_weights(self):\n",
            "        \"\"\"Initialize model weights.\"\"\"\n",
            "        for name, param in self.named_parameters():\n",
            "            if 'weight' in name and len(param.shape) > 1:\n",
            "                nn.init.xavier_uniform_(param)\n",
            "            elif 'bias' in name:\n",
            "                nn.init.constant_(param, 0)\n",
            "    \n",
            "    def forward(self, input_ids):\n",
            "        \"\"\"\n",
            "        Forward pass for sentiment analysis.\n",
            "        \n",
            "        Args:\n",
            "            input_ids (torch.Tensor): Tokenized input sequences [batch_size, seq_len]\n",
            "            \n",
            "        Returns:\n",
            "            torch.Tensor: Sentiment logits [batch_size, num_classes]\n",
            "        \"\"\"\n",
            "        # Embedding lookup\n",
            "        embedded = self.embedding(input_ids)  # [batch_size, seq_len, embed_dim]\n",
            "        \n",
            "        # LSTM processing\n",
            "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
            "        \n",
            "        # Use the last hidden state for classification\n",
            "        last_hidden = hidden[-1]  # [batch_size, hidden_dim]\n",
            "        \n",
            "        # Classification\n",
            "        logits = self.classifier(last_hidden)\n",
            "        \n",
            "        return logits\n",
            "\n",
            "# Simple tokenizer for demonstration\n",
            "class SimpleTokenizer:\n",
            "    \"\"\"Simple tokenizer for Australian tourism text.\"\"\"\n",
            "    \n",
            "    def __init__(self):\n",
            "        self.word_to_idx = {'<pad>': 0, '<unk>': 1}\n",
            "        self.idx_to_word = {0: '<pad>', 1: '<unk>'}\n",
            "        self.next_idx = 2\n",
            "    \n",
            "    def fit(self, texts):\n",
            "        \"\"\"Build vocabulary from texts.\"\"\"\n",
            "        for text in texts:\n",
            "            words = text.lower().split()\n",
            "            for word in words:\n",
            "                if word not in self.word_to_idx:\n",
            "                    self.word_to_idx[word] = self.next_idx\n",
            "                    self.idx_to_word[self.next_idx] = word\n",
            "                    self.next_idx += 1\n",
            "    \n",
            "    def encode(self, text, max_length=32):\n",
            "        \"\"\"Convert text to token indices.\"\"\"\n",
            "        words = text.lower().split()[:max_length]\n",
            "        indices = [self.word_to_idx.get(word, 1) for word in words]\n",
            "        \n",
            "        # Pad to max_length\n",
            "        if len(indices) < max_length:\n",
            "            indices.extend([0] * (max_length - len(indices)))\n",
            "        \n",
            "        return indices\n",
            "\n",
            "print(\"âœ… Model and tokenizer classes defined!\")"
        ]
    })
    
    # Sample Data and Quick Training
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## Sample Data and Quick Training\n",
            "\n",
            "Let's create sample Australian tourism review data and quickly train our model:"
        ]
    })
    
    cells.append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Sample Australian tourism reviews with multilingual support\n",
            "australian_tourism_data = {\n",
            "    'reviews': [\n",
            "        # Positive reviews (label: 2)\n",
            "        \"The Sydney Opera House tour was absolutely breathtaking!\",\n",
            "        \"NhÃ  hÃ¡t Opera Sydney tháº­t tuyá»‡t vá»i!\",  # Vietnamese\n",
            "        \"Melbourne's coffee culture exceeded all expectations.\",\n",
            "        \"VÄƒn hÃ³a cÃ  phÃª Melbourne vÆ°á»£t quÃ¡ mong Ä‘á»£i.\",  # Vietnamese\n",
            "        \"Bondi Beach is perfect for surfing.\",\n",
            "        \"BÃ£i biá»ƒn Bondi hoÃ n háº£o cho lÆ°á»›t sÃ³ng.\",  # Vietnamese\n",
            "        \n",
            "        # Neutral reviews (label: 1)\n",
            "        \"The hotel in Brisbane was decent.\",\n",
            "        \"KhÃ¡ch sáº¡n á»Ÿ Brisbane táº¡m Ä‘Æ°á»£c.\",  # Vietnamese\n",
            "        \"Adelaide zoo has some interesting animals.\",\n",
            "        \"Sá»Ÿ thÃº Adelaide cÃ³ má»™t sá»‘ Ä‘á»™ng váº­t thÃº vá»‹.\",  # Vietnamese\n",
            "        \n",
            "        # Negative reviews (label: 0)\n",
            "        \"The Sydney harbor cruise was overpriced.\",\n",
            "        \"Du thuyá»n cáº£ng Sydney Ä‘áº¯t quÃ¡.\",  # Vietnamese\n",
            "        \"Melbourne weather ruined our vacation.\",\n",
            "        \"Thá»i tiáº¿t Melbourne lÃ m há»ng ká»³ nghá»‰.\",  # Vietnamese\n",
            "    ],\n",
            "    'labels': [2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0]\n",
            "}\n",
            "\n",
            "# Create and train model\n",
            "tokenizer = SimpleTokenizer()\n",
            "tokenizer.fit(australian_tourism_data['reviews'])\n",
            "\n",
            "model = AustralianTourismSentimentAnalyzer(\n",
            "    vocab_size=len(tokenizer.word_to_idx),\n",
            "    embed_dim=64,\n",
            "    hidden_dim=128,\n",
            "    num_classes=3\n",
            ").to(device)\n",
            "\n",
            "# Prepare training data\n",
            "encoded_reviews = []\n",
            "for review in australian_tourism_data['reviews']:\n",
            "    encoded = tokenizer.encode(review, max_length=32)\n",
            "    encoded_reviews.append(encoded)\n",
            "\n",
            "input_ids = torch.tensor(encoded_reviews, dtype=torch.long).to(device)\n",
            "labels_tensor = torch.tensor(australian_tourism_data['labels'], dtype=torch.long).to(device)\n",
            "\n",
            "# Quick training\n",
            "model.train()\n",
            "criterion = nn.CrossEntropyLoss()\n",
            "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
            "\n",
            "print(\"ðŸ‹ï¸ Quick training for demonstration...\")\n",
            "for epoch in range(10):\n",
            "    optimizer.zero_grad()\n",
            "    logits = model(input_ids)\n",
            "    loss = criterion(logits, labels_tensor)\n",
            "    loss.backward()\n",
            "    optimizer.step()\n",
            "    \n",
            "    if epoch % 3 == 0:\n",
            "        with torch.no_grad():\n",
            "            predictions = torch.argmax(logits, dim=-1)\n",
            "            accuracy = (predictions == labels_tensor).float().mean()\n",
            "            print(f\"   Epoch {epoch}: Loss = {loss.item():.4f}, Accuracy = {accuracy.item():.4f}\")\n",
            "\n",
            "print(\"âœ… Model training completed!\")\n",
            "print(f\"ðŸ“Š Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
            "print(f\"ðŸ“± Model device: {next(model.parameters()).device}\")"
        ]
    })
    
    # Section 1: Evaluation Mode
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "# 1. Preparing the Model for Inference: Evaluation Mode\n",
            "\n",
            "The first and most crucial step in production deployment is properly preparing your model for inference. This involves setting the model to evaluation mode and understanding how it differs from training mode.\n",
            "\n",
            "## Key Concepts\n",
            "\n",
            "### `model.eval()` vs `model.train()`\n",
            "- **`model.eval()`**: Sets the model to evaluation mode\n",
            "- **`model.train()`**: Sets the model to training mode (default)\n",
            "\n",
            "### What Changes in Evaluation Mode\n",
            "1. **Autograd is typically disabled** (via `torch.no_grad()`)\n",
            "2. **Dropout layers** are turned off\n",
            "3. **Batch Normalization** uses frozen statistics\n",
            "4. **Memory and compute optimization** for inference\n",
            "\n",
            "### TensorFlow Comparison\n",
            "```python\n",
            "# TensorFlow (implicit mode switching)\n",
            "model(inputs, training=False)  # Inference mode\n",
            "model(inputs, training=True)   # Training mode\n",
            "\n",
            "# PyTorch (explicit mode switching)\n",
            "model.eval()     # Set to evaluation mode\n",
            "model.train()    # Set to training mode\n",
            "```"
        ]
    })
    
    cells.append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Demonstration: Training vs Evaluation Mode\n",
            "print(\"ðŸŽ¯ Demonstrating Training vs Evaluation Mode Differences\\n\")\n",
            "\n",
            "def production_inference(model, texts, tokenizer, device):\n",
            "    \"\"\"\n",
            "    Production-ready inference function with all best practices.\n",
            "    \n",
            "    CRITICAL STEPS:\n",
            "    1. Set model to evaluation mode\n",
            "    2. Disable autograd with torch.no_grad()\n",
            "    3. Ensure proper device placement\n",
            "    4. Handle batch dimensions correctly\n",
            "    \"\"\"\n",
            "    # STEP 1: Set model to evaluation mode\n",
            "    model.eval()\n",
            "    \n",
            "    # STEP 2: Prepare input data\n",
            "    encoded_texts = []\n",
            "    for text in texts:\n",
            "        encoded = tokenizer.encode(text, max_length=32)\n",
            "        encoded_texts.append(encoded)\n",
            "    \n",
            "    # Convert to tensor and move to device\n",
            "    input_ids = torch.tensor(encoded_texts, dtype=torch.long).to(device)\n",
            "    \n",
            "    # STEP 3: Disable autograd for inference\n",
            "    with torch.no_grad():\n",
            "        # STEP 4: Forward pass\n",
            "        logits = model(input_ids)\n",
            "        \n",
            "        # STEP 5: Convert to probabilities and predictions\n",
            "        probabilities = F.softmax(logits, dim=-1)\n",
            "        predictions = torch.argmax(logits, dim=-1)\n",
            "        confidence = torch.max(probabilities, dim=-1)[0]\n",
            "    \n",
            "    # STEP 6: Format results\n",
            "    sentiment_labels = ['negative', 'neutral', 'positive']\n",
            "    results = []\n",
            "    \n",
            "    for i, text in enumerate(texts):\n",
            "        result = {\n",
            "            'text': text,\n",
            "            'sentiment': sentiment_labels[predictions[i].item()],\n",
            "            'confidence': confidence[i].item(),\n",
            "            'probabilities': {\n",
            "                'negative': probabilities[i][0].item(),\n",
            "                'neutral': probabilities[i][1].item(),\n",
            "                'positive': probabilities[i][2].item()\n",
            "            }\n",
            "        }\n",
            "        results.append(result)\n",
            "    \n",
            "    return results\n",
            "\n",
            "# Test production inference function\n",
            "print(\"ðŸ§ª Testing Production Inference Function\\n\")\n",
            "\n",
            "test_reviews = [\n",
            "    \"The Sydney Opera House tour was absolutely amazing!\",\n",
            "    \"CÃ  phÃª á»Ÿ Melbourne quÃ¡ Ä‘áº¯t\",  # Vietnamese: Coffee in Melbourne is too expensive\n",
            "    \"The hotel was clean but nothing extraordinary\",\n",
            "    \"Perth beaches are perfect for relaxing\"\n",
            "]\n",
            "\n",
            "results = production_inference(model, test_reviews, tokenizer, device)\n",
            "\n",
            "for i, result in enumerate(results):\n",
            "    print(f\"Review {i+1}:\")\n",
            "    print(f\"  Text: '{result['text'][:40]}...'\")\n",
            "    print(f\"  Sentiment: {result['sentiment'].upper()} (confidence: {result['confidence']:.3f})\")\n",
            "    print(f\"  Probabilities: Neg={result['probabilities']['negative']:.3f}, \"\n",
            "          f\"Neu={result['probabilities']['neutral']:.3f}, \"\n",
            "          f\"Pos={result['probabilities']['positive']:.3f}\")\n",
            "    print()\n",
            "\n",
            "print(\"âœ… Production inference function working correctly!\")\n",
            "\n",
            "print(f\"\\n\" + \"=\"*60)\n",
            "print(\"ðŸš€ PRODUCTION INFERENCE BEST PRACTICES\")\n",
            "print(\"=\"*60)\n",
            "print(\"1. ALWAYS call model.eval() before inference\")\n",
            "print(\"2. Use torch.no_grad() to disable autograd and save memory\")\n",
            "print(\"3. Ensure consistent input preprocessing\")\n",
            "print(\"4. Handle batch dimensions properly (even for single samples)\")\n",
            "print(\"5. Move tensors to the same device as the model\")\n",
            "print(\"=\"*60)"
        ]
    })
    
    return cells

if __name__ == "__main__":
    # Create cells for the first major section
    cells = create_full_production_notebook()
    
    notebook = {
        "cells": cells,
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "codemirror_mode": {
                    "name": "ipython",
                    "version": 3
                },
                "file_extension": ".py",
                "mimetype": "text/x-python",
                "name": "python",
                "nbconvert_exporter": "python",
                "pygments_lexer": "ipython3",
                "version": "3.8.0"
            }
        },
        "nbformat": 4,
        "nbformat_minor": 4
    }
    
    # Save the notebook with first complete section
    with open("production_inference_deployment.ipynb", "w") as f:
        json.dump(notebook, f, indent=2)
    
    print("Production deployment notebook created with Section 1 (Evaluation Mode)!")