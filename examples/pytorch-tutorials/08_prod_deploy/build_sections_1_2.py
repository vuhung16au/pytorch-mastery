#!/usr/bin/env python3
"""
Complete Production Deployment Notebook with All Sections
"""

import json

def create_complete_production_notebook():
    """Create the complete production deployment tutorial notebook."""
    
    cells = []
    
    # Title and Introduction
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "# Production Inference Deployment with PyTorch\n",
            "\n",
            "This comprehensive tutorial demonstrates how to deploy PyTorch models for production inference, covering essential deployment patterns from model preparation to advanced serving solutions.\n",
            "\n",
            "## Learning Objectives\n",
            "- üéØ **Prepare models for inference**: Evaluation mode and optimization techniques\n",
            "- üöÄ **Master TorchScript**: Convert Python models to optimized, production-ready format\n",
            "- üîß **Deploy with C++**: Use libtorch for high-performance inference\n",
            "- üì¶ **Implement TorchServe**: Scalable model serving with built-in APIs\n",
            "\n",
            "## Tutorial Structure\n",
            "1. **Preparing the Model for Inference**: Evaluation Mode\n",
            "2. **TorchScript**: `jit.script` and `jit.trace` for production optimization\n",
            "3. **Deploying with C++**: libtorch integration examples\n",
            "4. **TorchServe**: Complete model serving solution\n",
            "\n",
            "## Use Case: Australian Tourism Sentiment Analysis\n",
            "We will build and deploy a multilingual sentiment analysis model for Australian tourism reviews (English + Vietnamese), demonstrating real-world production deployment scenarios.\n",
            "\n",
            "**Sample Use Cases:**\n",
            "- Hotel booking platforms analyzing customer reviews\n",
            "- Tourism boards monitoring social media sentiment\n",
            "- Travel agencies optimizing destination recommendations\n",
            "\n",
            "## TensorFlow vs PyTorch Deployment Comparison\n",
            "\n",
            "| Aspect | TensorFlow | PyTorch |\n",
            "|--------|------------|----------|\n",
            "| **Model Format** | SavedModel, TFLite | TorchScript, ONNX |\n",
            "| **Serving** | TensorFlow Serving | TorchServe |\n",
            "| **C++ Deployment** | TensorFlow C++ API | libtorch |\n",
            "| **Mobile** | TensorFlow Lite | PyTorch Mobile |\n",
            "| **Optimization** | TensorRT, XLA | TorchScript JIT |\n",
            "\n",
            "---"
        ]
    })
    
    # Environment Setup
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## Environment Setup and Runtime Detection\n",
            "\n",
            "Following PyTorch best practices for cross-platform production deployment:"
        ]
    })
    
    cells.append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Environment Detection and Setup\n",
            "import sys\n",
            "import subprocess\n",
            "import os\n",
            "import time\n",
            "import platform\n",
            "\n",
            "# Detect the runtime environment\n",
            "IS_COLAB = \"google.colab\" in sys.modules\n",
            "IS_KAGGLE = \"kaggle_secrets\" in sys.modules or \"kaggle\" in os.environ.get('KAGGLE_URL_BASE', '')\n",
            "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
            "\n",
            "print(f\"Environment detected:\")\n",
            "print(f\"  - Local: {IS_LOCAL}\")\n",
            "print(f\"  - Google Colab: {IS_COLAB}\")\n",
            "print(f\"  - Kaggle: {IS_KAGGLE}\")\n",
            "\n",
            "# Platform-specific system setup\n",
            "if IS_COLAB:\n",
            "    print(\"\\nSetting up Google Colab environment...\")\n",
            "    !apt update -qq\n",
            "    !apt install -y -qq software-properties-common\n",
            "elif IS_KAGGLE:\n",
            "    print(\"\\nSetting up Kaggle environment...\")\n",
            "    # Kaggle usually has most packages pre-installed\n",
            "else:\n",
            "    print(\"\\nSetting up local environment...\")\n",
            "\n",
            "# Install required packages for this notebook\n",
            "required_packages = [\n",
            "    \"torch\",\n",
            "    \"torchvision\", \n",
            "    \"transformers\",\n",
            "    \"datasets\",\n",
            "    \"tokenizers\",\n",
            "    \"pandas\",\n",
            "    \"matplotlib\",\n",
            "    \"seaborn\",\n",
            "    \"tensorboard\"\n",
            "]\n",
            "\n",
            "print(\"\\nInstalling required packages...\")\n",
            "for package in required_packages:\n",
            "    try:\n",
            "        if IS_COLAB or IS_KAGGLE:\n",
            "            !pip install -q {package}\n",
            "        else:\n",
            "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
            "                          capture_output=True, check=False)\n",
            "        print(f\"‚úì {package}\")\n",
            "    except Exception as e:\n",
            "        print(f\"‚ö†Ô∏è {package}: {str(e)[:50]}...\")\n",
            "\n",
            "print(\"\\nüî• Production deployment environment ready!\")"
        ]
    })
    
    # Device Detection
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## Import Dependencies and Device Detection"
        ]
    })
    
    cells.append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Core PyTorch imports for production deployment\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import torch.nn.functional as F\n",
            "import torch.optim as optim\n",
            "import torch.jit as jit\n",
            "from torch.utils.data import DataLoader, Dataset\n",
            "from torch.utils.tensorboard import SummaryWriter\n",
            "\n",
            "# Production deployment specific imports\n",
            "import torchvision.transforms as transforms\n",
            "\n",
            "# Standard libraries\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as sns\n",
            "import json\n",
            "import pickle\n",
            "import tempfile\n",
            "import warnings\n",
            "from datetime import datetime\n",
            "from pathlib import Path\n",
            "\n",
            "# Suppress warnings for cleaner output\n",
            "warnings.filterwarnings('ignore')\n",
            "\n",
            "def detect_device():\n",
            "    \"\"\"\n",
            "    Detect the best available PyTorch device with comprehensive hardware support.\n",
            "    \n",
            "    Priority order:\n",
            "    1. CUDA (NVIDIA GPUs) - Best performance for deep learning\n",
            "    2. MPS (Apple Silicon) - Optimized for M1/M2/M3 Macs  \n",
            "    3. CPU (Universal) - Always available fallback\n",
            "    \n",
            "    Returns:\n",
            "        torch.device: The optimal device for PyTorch operations\n",
            "        str: Human-readable device description for logging\n",
            "    \"\"\"\n",
            "    # Check for CUDA (NVIDIA GPU)\n",
            "    if torch.cuda.is_available():\n",
            "        device = torch.device(\"cuda\")\n",
            "        gpu_name = torch.cuda.get_device_name(0)\n",
            "        device_info = f\"CUDA GPU: {gpu_name}\"\n",
            "        \n",
            "        # Additional CUDA info for optimization\n",
            "        cuda_version = torch.version.cuda\n",
            "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
            "        \n",
            "        print(f\"üöÄ Using CUDA acceleration\")\n",
            "        print(f\"   GPU: {gpu_name}\")\n",
            "        print(f\"   CUDA Version: {cuda_version}\")\n",
            "        print(f\"   GPU Memory: {gpu_memory:.1f} GB\")\n",
            "        \n",
            "        return device, device_info\n",
            "    \n",
            "    # Check for MPS (Apple Silicon)\n",
            "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
            "        device = torch.device(\"mps\")\n",
            "        device_info = \"Apple Silicon MPS\"\n",
            "        \n",
            "        # Get system info for Apple Silicon\n",
            "        system_info = platform.uname()\n",
            "        \n",
            "        print(f\"üçé Using Apple Silicon MPS acceleration\")\n",
            "        print(f\"   System: {system_info.system} {system_info.release}\")\n",
            "        print(f\"   Machine: {system_info.machine}\")\n",
            "        print(f\"   Processor: {system_info.processor}\")\n",
            "        \n",
            "        return device, device_info\n",
            "    \n",
            "    # Fallback to CPU\n",
            "    else:\n",
            "        device = torch.device(\"cpu\")\n",
            "        device_info = \"CPU (No GPU acceleration available)\"\n",
            "        \n",
            "        # Get CPU info for optimization guidance\n",
            "        cpu_count = torch.get_num_threads()\n",
            "        system_info = platform.uname()\n",
            "        \n",
            "        print(f\"üíª Using CPU (no GPU acceleration detected)\")\n",
            "        print(f\"   Processor: {system_info.processor}\")\n",
            "        print(f\"   PyTorch Threads: {cpu_count}\")\n",
            "        print(f\"   System: {system_info.system} {system_info.release}\")\n",
            "        \n",
            "        # Provide optimization suggestions for CPU-only setups\n",
            "        print(f\"\\nüí° CPU Optimization Tips:\")\n",
            "        print(f\"   ‚Ä¢ Reduce batch size to prevent memory issues\")\n",
            "        print(f\"   ‚Ä¢ Consider using smaller models for faster inference\")\n",
            "        print(f\"   ‚Ä¢ Enable PyTorch optimizations: torch.set_num_threads({cpu_count})\")\n",
            "        \n",
            "        return device, device_info\n",
            "\n",
            "# Detect and set up device\n",
            "device, device_info = detect_device()\n",
            "\n",
            "print(f\"\\n‚úÖ PyTorch {torch.__version__} ready!\")\n",
            "print(f\"üì± Device selected: {device}\")\n",
            "print(f\"üìä Device info: {device_info}\")\n",
            "\n",
            "# Set global device for the notebook\n",
            "DEVICE = device"
        ]
    })
    
    # Model Definition
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## Sample Model: Australian Tourism Sentiment Analyzer\n",
            "\n",
            "Let's create a production-ready sentiment analysis model for Australian tourism reviews with multilingual support (English + Vietnamese):"
        ]
    })
    
    cells.append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "class AustralianTourismSentimentAnalyzer(nn.Module):\n",
            "    \"\"\"\n",
            "    Production-ready sentiment analysis model for Australian tourism reviews.\n",
            "    \n",
            "    Supports multilingual analysis (English + Vietnamese) for:\n",
            "    - Hotel and restaurant reviews\n",
            "    - Tourist attraction feedback\n",
            "    - Travel experience sentiment\n",
            "    \n",
            "    Args:\n",
            "        vocab_size (int): Size of vocabulary (combined English + Vietnamese)\n",
            "        embed_dim (int): Dimension of embedding layer\n",
            "        hidden_dim (int): Dimension of LSTM hidden states\n",
            "        num_classes (int): Number of sentiment classes (3: positive/neutral/negative)\n",
            "        dropout_rate (float): Dropout rate for regularization\n",
            "    \n",
            "    TensorFlow Comparison:\n",
            "        TF equivalent would use tf.keras.Sequential with:\n",
            "        - tf.keras.layers.Embedding\n",
            "        - tf.keras.layers.LSTM \n",
            "        - tf.keras.layers.Dense\n",
            "        But PyTorch gives more explicit control over forward pass.\n",
            "    \"\"\"\n",
            "    \n",
            "    def __init__(self, vocab_size=10000, embed_dim=128, hidden_dim=256, \n",
            "                 num_classes=3, dropout_rate=0.3):\n",
            "        super(AustralianTourismSentimentAnalyzer, self).__init__()\n",
            "        \n",
            "        # Store hyperparameters for TorchScript compatibility\n",
            "        self.vocab_size = vocab_size\n",
            "        self.embed_dim = embed_dim\n",
            "        self.hidden_dim = hidden_dim\n",
            "        self.num_classes = num_classes\n",
            "        self.dropout_rate = dropout_rate\n",
            "        \n",
            "        # Model layers\n",
            "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
            "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
            "        self.dropout = nn.Dropout(dropout_rate)\n",
            "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
            "        \n",
            "        # Initialize weights for better convergence\n",
            "        self._init_weights()\n",
            "    \n",
            "    def _init_weights(self):\n",
            "        \"\"\"Initialize model weights using Xavier uniform initialization.\"\"\"\n",
            "        for name, param in self.named_parameters():\n",
            "            if 'weight' in name and len(param.shape) > 1:\n",
            "                nn.init.xavier_uniform_(param)\n",
            "            elif 'bias' in name:\n",
            "                nn.init.constant_(param, 0)\n",
            "    \n",
            "    def forward(self, input_ids):\n",
            "        \"\"\"\n",
            "        Forward pass for sentiment analysis.\n",
            "        \n",
            "        Args:\n",
            "            input_ids (torch.Tensor): Tokenized input sequences [batch_size, seq_len]\n",
            "            \n",
            "        Returns:\n",
            "            torch.Tensor: Sentiment logits [batch_size, num_classes]\n",
            "        \"\"\"\n",
            "        # Embedding lookup\n",
            "        embedded = self.embedding(input_ids)  # [batch_size, seq_len, embed_dim]\n",
            "        \n",
            "        # LSTM processing\n",
            "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
            "        \n",
            "        # Use the last hidden state for classification\n",
            "        # In production, we could use attention pooling instead\n",
            "        last_hidden = hidden[-1]  # [batch_size, hidden_dim]\n",
            "        \n",
            "        # Apply dropout and classification\n",
            "        dropped = self.dropout(last_hidden)\n",
            "        logits = self.classifier(dropped)\n",
            "        \n",
            "        return logits\n",
            "    \n",
            "    def predict_sentiment(self, input_ids):\n",
            "        \"\"\"\n",
            "        Production-ready prediction method with human-readable outputs.\n",
            "        \n",
            "        Returns:\n",
            "            dict: Prediction results with sentiment labels and confidence\n",
            "        \"\"\"\n",
            "        self.eval()\n",
            "        with torch.no_grad():\n",
            "            logits = self.forward(input_ids)\n",
            "            probabilities = F.softmax(logits, dim=-1)\n",
            "            predictions = torch.argmax(logits, dim=-1)\n",
            "            confidence = torch.max(probabilities, dim=-1)[0]\n",
            "            \n",
            "            # Map predictions to human-readable labels\n",
            "            sentiment_labels = ['negative', 'neutral', 'positive']\n",
            "            \n",
            "            results = []\n",
            "            for i in range(len(predictions)):\n",
            "                results.append({\n",
            "                    'sentiment': sentiment_labels[predictions[i].item()],\n",
            "                    'confidence': confidence[i].item(),\n",
            "                    'probabilities': {\n",
            "                        'negative': probabilities[i][0].item(),\n",
            "                        'neutral': probabilities[i][1].item(),\n",
            "                        'positive': probabilities[i][2].item()\n",
            "                    }\n",
            "                })\n",
            "            \n",
            "            return results\n",
            "\n",
            "# Simple tokenizer for demonstration\n",
            "class SimpleTokenizer:\n",
            "    \"\"\"Simple tokenizer for Australian tourism text.\"\"\"\n",
            "    \n",
            "    def __init__(self, vocab_size=10000):\n",
            "        self.vocab_size = vocab_size\n",
            "        self.word_to_idx = {'<pad>': 0, '<unk>': 1}\n",
            "        self.idx_to_word = {0: '<pad>', 1: '<unk>'}\n",
            "        self.next_idx = 2\n",
            "    \n",
            "    def fit(self, texts):\n",
            "        \"\"\"Build vocabulary from texts.\"\"\"\n",
            "        for text in texts:\n",
            "            words = text.lower().split()\n",
            "            for word in words:\n",
            "                if word not in self.word_to_idx and self.next_idx < self.vocab_size:\n",
            "                    self.word_to_idx[word] = self.next_idx\n",
            "                    self.idx_to_word[self.next_idx] = word\n",
            "                    self.next_idx += 1\n",
            "    \n",
            "    def encode(self, text, max_length=128):\n",
            "        \"\"\"Convert text to token indices.\"\"\"\n",
            "        words = text.lower().split()[:max_length]\n",
            "        indices = [self.word_to_idx.get(word, 1) for word in words]  # 1 = <unk>\n",
            "        \n",
            "        # Pad to max_length\n",
            "        if len(indices) < max_length:\n",
            "            indices.extend([0] * (max_length - len(indices)))  # 0 = <pad>\n",
            "        \n",
            "        return indices\n",
            "\n",
            "print(\"‚úÖ Model and tokenizer classes defined!\")"
        ]
    })
    
    return cells

if __name__ == "__main__":
    # Create the cells
    cells = create_complete_production_notebook()
    
    notebook = {
        "cells": cells,
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "codemirror_mode": {
                    "name": "ipython",
                    "version": 3
                },
                "file_extension": ".py",
                "mimetype": "text/x-python",
                "name": "python",
                "nbconvert_exporter": "python",
                "pygments_lexer": "ipython3",
                "version": "3.8.0"
            }
        },
        "nbformat": 4,
        "nbformat_minor": 4
    }
    
    # Save the notebook with initial sections
    with open("production_inference_deployment.ipynb", "w") as f:
        json.dump(notebook, f, indent=2)
    
    print("Production deployment notebook created with initial sections!")