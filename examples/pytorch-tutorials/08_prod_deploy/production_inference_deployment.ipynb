{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Production Inference Deployment with PyTorch\n",
        "\n",
        "This comprehensive tutorial demonstrates how to deploy PyTorch models for production inference, covering essential deployment patterns from model preparation to advanced serving solutions.\n",
        "\n",
        "## Learning Objectives\n",
        "- \ud83c\udfaf **Prepare models for inference**: Evaluation mode and optimization techniques\n",
        "- \ud83d\ude80 **Master TorchScript**: Convert Python models to optimized, production-ready format\n",
        "- \ud83d\udd27 **Deploy with C++**: Use libtorch for high-performance inference\n",
        "- \ud83d\udce6 **Implement TorchServe**: Scalable model serving with built-in APIs\n",
        "\n",
        "## Tutorial Structure\n",
        "1. **Preparing the Model for Inference**: Evaluation Mode\n",
        "2. **TorchScript**: `jit.script` and `jit.trace` for production optimization\n",
        "3. **Deploying with C++**: libtorch integration examples\n",
        "4. **TorchServe**: Complete model serving solution\n",
        "\n",
        "## Use Case: Australian Tourism Sentiment Analysis\n",
        "We will build and deploy a multilingual sentiment analysis model for Australian tourism reviews (English + Vietnamese), demonstrating real-world production deployment scenarios.\n",
        "\n",
        "**Sample Use Cases:**\n",
        "- Hotel booking platforms analyzing customer reviews\n",
        "- Tourism boards monitoring social media sentiment\n",
        "- Travel agencies optimizing destination recommendations\n",
        "\n",
        "## TensorFlow vs PyTorch Deployment Comparison\n",
        "\n",
        "| Aspect | TensorFlow | PyTorch |\n",
        "|--------|------------|----------|\n",
        "| **Model Format** | SavedModel, TFLite | TorchScript, ONNX |\n",
        "| **Serving** | TensorFlow Serving | TorchServe |\n",
        "| **C++ Deployment** | TensorFlow C++ API | libtorch |\n",
        "| **Mobile** | TensorFlow Lite | PyTorch Mobile |\n",
        "| **Optimization** | TensorRT, XLA | TorchScript JIT |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup and Runtime Detection\n",
        "\n",
        "Following PyTorch best practices for cross-platform production deployment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment Detection and Setup\n",
        "import sys\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "import platform\n",
        "\n",
        "# Detect the runtime environment\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules or \"kaggle\" in os.environ.get('KAGGLE_URL_BASE', '')\n",
        "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
        "\n",
        "print(f\"Environment detected:\")\n",
        "print(f\"  - Local: {IS_LOCAL}\")\n",
        "print(f\"  - Google Colab: {IS_COLAB}\")\n",
        "print(f\"  - Kaggle: {IS_KAGGLE}\")\n",
        "\n",
        "# Platform-specific system setup\n",
        "if IS_COLAB:\n",
        "    print(\"\\nSetting up Google Colab environment...\")\n",
        "    !apt update -qq\n",
        "    !apt install -y -qq software-properties-common\n",
        "elif IS_KAGGLE:\n",
        "    print(\"\\nSetting up Kaggle environment...\")\n",
        "    # Kaggle usually has most packages pre-installed\n",
        "else:\n",
        "    print(\"\\nSetting up local environment...\")\n",
        "\n",
        "# Install required packages for this notebook\n",
        "required_packages = [\n",
        "    \"torch\",\n",
        "    \"torchvision\", \n",
        "    \"transformers\",\n",
        "    \"datasets\",\n",
        "    \"tokenizers\",\n",
        "    \"pandas\",\n",
        "    \"matplotlib\",\n",
        "    \"seaborn\",\n",
        "    \"tensorboard\"\n",
        "]\n",
        "\n",
        "print(\"\\nInstalling required packages...\")\n",
        "for package in required_packages:\n",
        "    try:\n",
        "        if IS_COLAB or IS_KAGGLE:\n",
        "            !pip install -q {package}\n",
        "        else:\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
        "                          capture_output=True, check=False)\n",
        "        print(f\"\u2713 {package}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u26a0\ufe0f {package}: {str(e)[:50]}...\")\n",
        "\n",
        "print(\"\\n\ud83d\udd25 Production deployment environment ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core PyTorch imports for production deployment\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.jit as jit\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Standard libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import tempfile\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Simplified device detection\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"\u2705 PyTorch {torch.__version__} ready on {device}!\")\n",
        "DEVICE = device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Model: Australian Tourism Sentiment Analyzer\n",
        "\n",
        "Let's create a production-ready sentiment analysis model for Australian tourism reviews with multilingual support (English + Vietnamese):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AustralianTourismSentimentAnalyzer(nn.Module):\n",
        "    \"\"\"\n",
        "    Production-ready sentiment analysis model for Australian tourism reviews.\n",
        "    \n",
        "    Supports multilingual analysis (English + Vietnamese) for:\n",
        "    - Hotel and restaurant reviews\n",
        "    - Tourist attraction feedback\n",
        "    - Travel experience sentiment\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size=1000, embed_dim=64, hidden_dim=128, num_classes=3):\n",
        "        super(AustralianTourismSentimentAnalyzer, self).__init__()\n",
        "        \n",
        "        # Store hyperparameters for TorchScript compatibility\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        # Model layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
        "        \n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "    \n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize model weights.\"\"\"\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'weight' in name and len(param.shape) > 1:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.constant_(param, 0)\n",
        "    \n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        Forward pass for sentiment analysis.\n",
        "        \n",
        "        Args:\n",
        "            input_ids (torch.Tensor): Tokenized input sequences [batch_size, seq_len]\n",
        "            \n",
        "        Returns:\n",
        "            torch.Tensor: Sentiment logits [batch_size, num_classes]\n",
        "        \"\"\"\n",
        "        # Embedding lookup\n",
        "        embedded = self.embedding(input_ids)  # [batch_size, seq_len, embed_dim]\n",
        "        \n",
        "        # LSTM processing\n",
        "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
        "        \n",
        "        # Use the last hidden state for classification\n",
        "        last_hidden = hidden[-1]  # [batch_size, hidden_dim]\n",
        "        \n",
        "        # Classification\n",
        "        logits = self.classifier(last_hidden)\n",
        "        \n",
        "        return logits\n",
        "\n",
        "# Simple tokenizer for demonstration\n",
        "class SimpleTokenizer:\n",
        "    \"\"\"Simple tokenizer for Australian tourism text.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.word_to_idx = {'<pad>': 0, '<unk>': 1}\n",
        "        self.idx_to_word = {0: '<pad>', 1: '<unk>'}\n",
        "        self.next_idx = 2\n",
        "    \n",
        "    def fit(self, texts):\n",
        "        \"\"\"Build vocabulary from texts.\"\"\"\n",
        "        for text in texts:\n",
        "            words = text.lower().split()\n",
        "            for word in words:\n",
        "                if word not in self.word_to_idx:\n",
        "                    self.word_to_idx[word] = self.next_idx\n",
        "                    self.idx_to_word[self.next_idx] = word\n",
        "                    self.next_idx += 1\n",
        "    \n",
        "    def encode(self, text, max_length=32):\n",
        "        \"\"\"Convert text to token indices.\"\"\"\n",
        "        words = text.lower().split()[:max_length]\n",
        "        indices = [self.word_to_idx.get(word, 1) for word in words]\n",
        "        \n",
        "        # Pad to max_length\n",
        "        if len(indices) < max_length:\n",
        "            indices.extend([0] * (max_length - len(indices)))\n",
        "        \n",
        "        return indices\n",
        "\n",
        "print(\"\u2705 Model and tokenizer classes defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Data and Quick Training\n",
        "\n",
        "Let's create sample Australian tourism review data and quickly train our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample Australian tourism reviews with multilingual support\n",
        "australian_tourism_data = {\n",
        "    'reviews': [\n",
        "        # Positive reviews (label: 2)\n",
        "        \"The Sydney Opera House tour was absolutely breathtaking!\",\n",
        "        \"Nh\u00e0 h\u00e1t Opera Sydney th\u1eadt tuy\u1ec7t v\u1eddi!\",  # Vietnamese\n",
        "        \"Melbourne's coffee culture exceeded all expectations.\",\n",
        "        \"V\u0103n h\u00f3a c\u00e0 ph\u00ea Melbourne v\u01b0\u1ee3t qu\u00e1 mong \u0111\u1ee3i.\",  # Vietnamese\n",
        "        \"Bondi Beach is perfect for surfing.\",\n",
        "        \"B\u00e3i bi\u1ec3n Bondi ho\u00e0n h\u1ea3o cho l\u01b0\u1edbt s\u00f3ng.\",  # Vietnamese\n",
        "        \n",
        "        # Neutral reviews (label: 1)\n",
        "        \"The hotel in Brisbane was decent.\",\n",
        "        \"Kh\u00e1ch s\u1ea1n \u1edf Brisbane t\u1ea1m \u0111\u01b0\u1ee3c.\",  # Vietnamese\n",
        "        \"Adelaide zoo has some interesting animals.\",\n",
        "        \"S\u1edf th\u00fa Adelaide c\u00f3 m\u1ed9t s\u1ed1 \u0111\u1ed9ng v\u1eadt th\u00fa v\u1ecb.\",  # Vietnamese\n",
        "        \n",
        "        # Negative reviews (label: 0)\n",
        "        \"The Sydney harbor cruise was overpriced.\",\n",
        "        \"Du thuy\u1ec1n c\u1ea3ng Sydney \u0111\u1eaft qu\u00e1.\",  # Vietnamese\n",
        "        \"Melbourne weather ruined our vacation.\",\n",
        "        \"Th\u1eddi ti\u1ebft Melbourne l\u00e0m h\u1ecfng k\u1ef3 ngh\u1ec9.\",  # Vietnamese\n",
        "    ],\n",
        "    'labels': [2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0]\n",
        "}\n",
        "\n",
        "# Create and train model\n",
        "tokenizer = SimpleTokenizer()\n",
        "tokenizer.fit(australian_tourism_data['reviews'])\n",
        "\n",
        "model = AustralianTourismSentimentAnalyzer(\n",
        "    vocab_size=len(tokenizer.word_to_idx),\n",
        "    embed_dim=64,\n",
        "    hidden_dim=128,\n",
        "    num_classes=3\n",
        ").to(device)\n",
        "\n",
        "# Prepare training data\n",
        "encoded_reviews = []\n",
        "for review in australian_tourism_data['reviews']:\n",
        "    encoded = tokenizer.encode(review, max_length=32)\n",
        "    encoded_reviews.append(encoded)\n",
        "\n",
        "input_ids = torch.tensor(encoded_reviews, dtype=torch.long).to(device)\n",
        "labels_tensor = torch.tensor(australian_tourism_data['labels'], dtype=torch.long).to(device)\n",
        "\n",
        "# Quick training\n",
        "model.train()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "print(\"\ud83c\udfcb\ufe0f Quick training for demonstration...\")\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(input_ids)\n",
        "    loss = criterion(logits, labels_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if epoch % 3 == 0:\n",
        "        with torch.no_grad():\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            accuracy = (predictions == labels_tensor).float().mean()\n",
        "            print(f\"   Epoch {epoch}: Loss = {loss.item():.4f}, Accuracy = {accuracy.item():.4f}\")\n",
        "\n",
        "print(\"\u2705 Model training completed!\")\n",
        "print(f\"\ud83d\udcca Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"\ud83d\udcf1 Model device: {next(model.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Preparing the Model for Inference: Evaluation Mode\n",
        "\n",
        "The first and most crucial step in production deployment is properly preparing your model for inference. This involves setting the model to evaluation mode and understanding how it differs from training mode.\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "### `model.eval()` vs `model.train()`\n",
        "- **`model.eval()`**: Sets the model to evaluation mode\n",
        "- **`model.train()`**: Sets the model to training mode (default)\n",
        "\n",
        "### What Changes in Evaluation Mode\n",
        "1. **Autograd is typically disabled** (via `torch.no_grad()`)\n",
        "2. **Dropout layers** are turned off\n",
        "3. **Batch Normalization** uses frozen statistics\n",
        "4. **Memory and compute optimization** for inference\n",
        "\n",
        "### TensorFlow Comparison\n",
        "```python\n",
        "# TensorFlow (implicit mode switching)\n",
        "model(inputs, training=False)  # Inference mode\n",
        "model(inputs, training=True)   # Training mode\n",
        "\n",
        "# PyTorch (explicit mode switching)\n",
        "model.eval()     # Set to evaluation mode\n",
        "model.train()    # Set to training mode\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstration: Training vs Evaluation Mode\n",
        "print(\"\ud83c\udfaf Demonstrating Training vs Evaluation Mode Differences\\n\")\n",
        "\n",
        "def production_inference(model, texts, tokenizer, device):\n",
        "    \"\"\"\n",
        "    Production-ready inference function with all best practices.\n",
        "    \n",
        "    CRITICAL STEPS:\n",
        "    1. Set model to evaluation mode\n",
        "    2. Disable autograd with torch.no_grad()\n",
        "    3. Ensure proper device placement\n",
        "    4. Handle batch dimensions correctly\n",
        "    \"\"\"\n",
        "    # STEP 1: Set model to evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # STEP 2: Prepare input data\n",
        "    encoded_texts = []\n",
        "    for text in texts:\n",
        "        encoded = tokenizer.encode(text, max_length=32)\n",
        "        encoded_texts.append(encoded)\n",
        "    \n",
        "    # Convert to tensor and move to device\n",
        "    input_ids = torch.tensor(encoded_texts, dtype=torch.long).to(device)\n",
        "    \n",
        "    # STEP 3: Disable autograd for inference\n",
        "    with torch.no_grad():\n",
        "        # STEP 4: Forward pass\n",
        "        logits = model(input_ids)\n",
        "        \n",
        "        # STEP 5: Convert to probabilities and predictions\n",
        "        probabilities = F.softmax(logits, dim=-1)\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        confidence = torch.max(probabilities, dim=-1)[0]\n",
        "    \n",
        "    # STEP 6: Format results\n",
        "    sentiment_labels = ['negative', 'neutral', 'positive']\n",
        "    results = []\n",
        "    \n",
        "    for i, text in enumerate(texts):\n",
        "        result = {\n",
        "            'text': text,\n",
        "            'sentiment': sentiment_labels[predictions[i].item()],\n",
        "            'confidence': confidence[i].item(),\n",
        "            'probabilities': {\n",
        "                'negative': probabilities[i][0].item(),\n",
        "                'neutral': probabilities[i][1].item(),\n",
        "                'positive': probabilities[i][2].item()\n",
        "            }\n",
        "        }\n",
        "        results.append(result)\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Test production inference function\n",
        "print(\"\ud83e\uddea Testing Production Inference Function\\n\")\n",
        "\n",
        "test_reviews = [\n",
        "    \"The Sydney Opera House tour was absolutely amazing!\",\n",
        "    \"C\u00e0 ph\u00ea \u1edf Melbourne qu\u00e1 \u0111\u1eaft\",  # Vietnamese: Coffee in Melbourne is too expensive\n",
        "    \"The hotel was clean but nothing extraordinary\",\n",
        "    \"Perth beaches are perfect for relaxing\"\n",
        "]\n",
        "\n",
        "results = production_inference(model, test_reviews, tokenizer, device)\n",
        "\n",
        "for i, result in enumerate(results):\n",
        "    print(f\"Review {i+1}:\")\n",
        "    print(f\"  Text: '{result['text'][:40]}...'\")\n",
        "    print(f\"  Sentiment: {result['sentiment'].upper()} (confidence: {result['confidence']:.3f})\")\n",
        "    print(f\"  Probabilities: Neg={result['probabilities']['negative']:.3f}, \"\n",
        "          f\"Neu={result['probabilities']['neutral']:.3f}, \"\n",
        "          f\"Pos={result['probabilities']['positive']:.3f}\")\n",
        "    print()\n",
        "\n",
        "print(\"\u2705 Production inference function working correctly!\")\n",
        "\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"\ud83d\ude80 PRODUCTION INFERENCE BEST PRACTICES\")\n",
        "print(\"=\"*60)\n",
        "print(\"1. ALWAYS call model.eval() before inference\")\n",
        "print(\"2. Use torch.no_grad() to disable autograd and save memory\")\n",
        "print(\"3. Ensure consistent input preprocessing\")\n",
        "print(\"4. Handle batch dimensions properly (even for single samples)\")\n",
        "print(\"5. Move tensors to the same device as the model\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. TorchScript: Production-Ready Model Optimization\n",
        "\n",
        "**TorchScript** is a statically typed subset of Python for representing PyTorch models. It's designed for high-performance inference and can run without a Python interpreter.\n",
        "\n",
        "## Key Benefits\n",
        "- \ud83d\ude80 **High Performance**: JIT compilation optimizes execution\n",
        "- \ud83c\udfed **Production Ready**: No Python dependency required\n",
        "- \ud83d\udce6 **Single File**: Model + weights in one serialized file\n",
        "- \ud83d\udd27 **Optimization**: Automatic operator fusion and batching\n",
        "\n",
        "## TorchScript Conversion Methods\n",
        "1. **`torch.jit.script`**: Direct code inspection (preserves control flow)\n",
        "2. **`torch.jit.trace`**: Execution tracing (more robust but limited control flow)\n",
        "\n",
        "### TensorFlow Comparison\n",
        "```python\n",
        "# TensorFlow (SavedModel)\n",
        "tf.saved_model.save(model, 'model_dir')\n",
        "loaded = tf.saved_model.load('model_dir')\n",
        "\n",
        "# PyTorch (TorchScript)\n",
        "scripted = torch.jit.script(model)\n",
        "torch.jit.save(scripted, 'model.pt')\n",
        "loaded = torch.jit.load('model.pt')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstration: TorchScript Conversion Methods\n",
        "print(\"\ud83d\ude80 TorchScript Conversion Demonstration\\n\")\n",
        "\n",
        "# Method: torch.jit.trace (Execution Tracing)\n",
        "print(\"\ud83d\udd0d Method: torch.jit.trace (Execution Tracing)\")\n",
        "# Create example input for tracing\n",
        "example_input = torch.randint(0, len(tokenizer.word_to_idx), (2, 32)).to(device)\n",
        "\n",
        "try:\n",
        "    model.eval()  # IMPORTANT: Set to eval mode before tracing\n",
        "    traced_model = torch.jit.trace(model, example_input)\n",
        "    print(\"   \u2705 Trace conversion successful!\")\n",
        "    print(f\"   \ud83d\udcca Traced model type: {type(traced_model)}\")\n",
        "    print(f\"   \ud83d\udd27 Example input shape: {example_input.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"   \u274c Trace conversion failed: {e}\")\n",
        "\n",
        "# Compare original vs TorchScript performance\n",
        "print(\"\\n\u26a1 Performance Comparison: Original vs TorchScript\")\n",
        "\n",
        "# Test data\n",
        "test_input = torch.randint(0, len(tokenizer.word_to_idx), (10, 32)).to(device)\n",
        "\n",
        "# Original model timing\n",
        "model.eval()\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    for _ in range(100):\n",
        "        _ = model(test_input)\n",
        "original_time = time.time() - start_time\n",
        "\n",
        "# TorchScript model timing (if available)\n",
        "if 'traced_model' in locals():\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(100):\n",
        "            _ = traced_model(test_input)\n",
        "    torchscript_time = time.time() - start_time\n",
        "    \n",
        "    speedup = original_time / torchscript_time if torchscript_time > 0 else 1.0\n",
        "    print(f\"   Original model: {original_time:.4f} seconds\")\n",
        "    print(f\"   TorchScript model: {torchscript_time:.4f} seconds\")\n",
        "    print(f\"   \ud83c\udfc3\u200d\u2642\ufe0f Speedup: {speedup:.2f}x faster\")\n",
        "else:\n",
        "    print(\"   \u26a0\ufe0f TorchScript model not available for comparison\")\n",
        "\n",
        "print(\"\\n\ud83d\udcc1 Saving TorchScript Models for Production\")\n",
        "\n",
        "# Save TorchScript model\n",
        "if 'traced_model' in locals():\n",
        "    model_path = \"australian_sentiment_torchscript.pt\"\n",
        "    torch.jit.save(traced_model, model_path)\n",
        "    print(f\"   \ud83d\udcbe TorchScript model saved to: {model_path}\")\n",
        "    \n",
        "    # Load and test saved model\n",
        "    loaded_model = torch.jit.load(model_path)\n",
        "    loaded_model.eval()\n",
        "    \n",
        "    # Test loaded model\n",
        "    with torch.no_grad():\n",
        "        original_output = traced_model(test_input)\n",
        "        loaded_output = loaded_model(test_input)\n",
        "        \n",
        "        # Check if outputs are identical\n",
        "        outputs_match = torch.allclose(original_output, loaded_output, atol=1e-6)\n",
        "        print(f\"   \ud83d\udd0d Saved/loaded outputs match: {outputs_match}\")\n",
        "        \n",
        "    # Get file size\n",
        "    import os\n",
        "    file_size = os.path.getsize(model_path) / 1024**2  # MB\n",
        "    print(f\"   \ud83d\udcca Model file size: {file_size:.2f} MB\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\ud83c\udfed TORCHSCRIPT PRODUCTION BENEFITS\")\n",
        "print(\"=\"*60)\n",
        "print(\"1. No Python dependency - can run in pure C++ environments\")\n",
        "print(\"2. Optimized execution through JIT compilation\")\n",
        "print(\"3. Single file deployment (model + weights)\")\n",
        "print(\"4. Cross-platform compatibility\")\n",
        "print(\"5. Memory and compute optimizations\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Deploying with C++: High-Performance Inference\n",
        "\n",
        "PyTorch models can be deployed in C++ environments using **libtorch**, removing Python dependencies for maximum performance and integration with existing C++ systems.\n",
        "\n",
        "## Key Concepts\n",
        "- \ud83d\udd27 **libtorch**: Core C++ library that powers PyTorch\n",
        "- \u26a1 **Performance**: Direct C++ execution without Python overhead\n",
        "- \ud83c\udfd7\ufe0f **Integration**: Easy integration with existing C++ applications\n",
        "- \ud83d\ude80 **Real-time**: Ideal for low-latency, high-throughput scenarios\n",
        "\n",
        "## Use Cases\n",
        "- Real-time inference servers\n",
        "- Embedded systems\n",
        "- High-frequency trading systems\n",
        "- Game engines\n",
        "- Mobile applications\n",
        "\n",
        "### TensorFlow Comparison\n",
        "```cpp\n",
        "// TensorFlow C++ API\n",
        "#include \"tensorflow/cc/client/client_session.h\"\n",
        "#include \"tensorflow/cc/ops/standard_ops.h\"\n",
        "\n",
        "// PyTorch C++ API (libtorch)\n",
        "#include <torch/script.h>\n",
        "#include <torch/torch.h>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C++ Deployment Code Generation\n",
        "print(\"\ud83d\udd27 Generating C++ Deployment Code for Australian Sentiment Model\\n\")\n",
        "\n",
        "# Generate C++ inference code\n",
        "cpp_code = '''#include <torch/script.h>\n",
        "#include <torch/torch.h>\n",
        "#include <iostream>\n",
        "#include <memory>\n",
        "#include <vector>\n",
        "#include <string>\n",
        "#include <map>\n",
        "\n",
        "class AustralianSentimentInference {\n",
        "private:\n",
        "    torch::jit::script::Module model;\n",
        "    std::map<std::string, int> word_to_idx;\n",
        "    std::vector<std::string> sentiment_labels = {\"negative\", \"neutral\", \"positive\"};\n",
        "    \n",
        "public:\n",
        "    // Constructor: Load TorchScript model\n",
        "    AustralianSentimentInference(const std::string& model_path) {\n",
        "        try {\n",
        "            // Load the TorchScript model\n",
        "            model = torch::jit::load(model_path);\n",
        "            model.eval();  // Set to evaluation mode\n",
        "            \n",
        "            std::cout << \"\u2705 Model loaded successfully from: \" << model_path << std::endl;\n",
        "        } catch (const c10::Error& e) {\n",
        "            std::cerr << \"\u274c Error loading model: \" << e.what() << std::endl;\n",
        "            throw;\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    // Main inference function\n",
        "    std::map<std::string, float> predict_sentiment(const std::string& text) {\n",
        "        try {\n",
        "            // Create dummy input (in production, implement proper tokenizer)\n",
        "            std::vector<int64_t> tokens(32, 0);  // Padded input\n",
        "            tokens[0] = 2; tokens[1] = 3; tokens[2] = 4;  // Simple example\n",
        "            \n",
        "            // Convert to tensor\n",
        "            auto options = torch::TensorOptions().dtype(torch::kLong);\n",
        "            auto input_tensor = torch::from_blob(tokens.data(), {1, 32}, options).clone();\n",
        "            \n",
        "            // Run inference\n",
        "            std::vector<torch::jit::IValue> inputs;\n",
        "            inputs.push_back(input_tensor);\n",
        "            \n",
        "            torch::NoGradGuard no_grad;  // Disable gradients for inference\n",
        "            at::Tensor output = model.forward(inputs).toTensor();\n",
        "            \n",
        "            // Apply softmax to get probabilities\n",
        "            auto probabilities = torch::softmax(output, 1);\n",
        "            auto prob_accessor = probabilities.accessor<float, 2>();\n",
        "            \n",
        "            // Get prediction\n",
        "            auto prediction = torch::argmax(output, 1);\n",
        "            int predicted_class = prediction.item<int>();\n",
        "            \n",
        "            // Create result map\n",
        "            std::map<std::string, float> result;\n",
        "            result[\"negative\"] = prob_accessor[0][0];\n",
        "            result[\"neutral\"] = prob_accessor[0][1];\n",
        "            result[\"positive\"] = prob_accessor[0][2];\n",
        "            result[\"confidence\"] = prob_accessor[0][predicted_class];\n",
        "            \n",
        "            std::cout << \"\ud83c\udfaf Prediction: \" << sentiment_labels[predicted_class] \n",
        "                      << \" (confidence: \" << result[\"confidence\"] << \")\" << std::endl;\n",
        "            \n",
        "            return result;\n",
        "            \n",
        "        } catch (const c10::Error& e) {\n",
        "            std::cerr << \"\u274c Inference error: \" << e.what() << std::endl;\n",
        "            throw;\n",
        "        }\n",
        "    }\n",
        "};\n",
        "\n",
        "// Example usage\n",
        "int main() {\n",
        "    try {\n",
        "        // Initialize inference engine\n",
        "        AustralianSentimentInference engine(\"australian_sentiment_torchscript.pt\");\n",
        "        \n",
        "        // Test with Australian tourism reviews\n",
        "        std::vector<std::string> test_reviews = {\n",
        "            \"The Sydney Opera House tour was absolutely amazing!\",\n",
        "            \"Melbourne weather ruined our vacation\",\n",
        "            \"The hotel was decent but nothing special\"\n",
        "        };\n",
        "        \n",
        "        std::cout << \"\\\\n\ud83e\uddea Testing C++ Inference Engine:\\\\n\" << std::endl;\n",
        "        \n",
        "        for (const auto& review : test_reviews) {\n",
        "            std::cout << \"\ud83d\udcdd Review: \\\\\"\" << review << \"\\\\\"\" << std::endl;\n",
        "            auto result = engine.predict_sentiment(review);\n",
        "            \n",
        "            std::cout << \"   Probabilities:\" << std::endl;\n",
        "            std::cout << \"     Negative: \" << result[\"negative\"] << std::endl;\n",
        "            std::cout << \"     Neutral:  \" << result[\"neutral\"] << std::endl;\n",
        "            std::cout << \"     Positive: \" << result[\"positive\"] << std::endl;\n",
        "            std::cout << std::endl;\n",
        "        }\n",
        "        \n",
        "        return 0;\n",
        "        \n",
        "    } catch (const std::exception& e) {\n",
        "        std::cerr << \"\ud83d\udca5 Application error: \" << e.what() << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "}'''\n",
        "\n",
        "# Save C++ code to file\n",
        "cpp_filename = \"australian_sentiment_inference.cpp\"\n",
        "with open(cpp_filename, 'w') as f:\n",
        "    f.write(cpp_code)\n",
        "\n",
        "print(f\"\ud83d\udcbe C++ inference code saved to: {cpp_filename}\")\n",
        "print(f\"\ud83d\udcca Code length: {len(cpp_code)} characters\")\n",
        "\n",
        "# Generate CMakeLists.txt for building\n",
        "cmake_content = '''cmake_minimum_required(VERSION 3.18 FATAL_ERROR)\n",
        "project(australian_sentiment_inference)\n",
        "\n",
        "# Find required packages\n",
        "find_package(Torch REQUIRED)\n",
        "set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}\")\n",
        "\n",
        "# Add executable\n",
        "add_executable(australian_sentiment_inference australian_sentiment_inference.cpp)\n",
        "target_link_libraries(australian_sentiment_inference \"${TORCH_LIBRARIES}\")\n",
        "set_property(TARGET australian_sentiment_inference PROPERTY CXX_STANDARD 17)\n",
        "\n",
        "# Copy model file to build directory\n",
        "configure_file(australian_sentiment_torchscript.pt australian_sentiment_torchscript.pt COPYONLY)'''\n",
        "\n",
        "cmake_filename = \"CMakeLists.txt\"\n",
        "with open(cmake_filename, 'w') as f:\n",
        "    f.write(cmake_content)\n",
        "\n",
        "print(f\"\ud83d\udcbe CMake configuration saved to: {cmake_filename}\")\n",
        "\n",
        "# Build instructions\n",
        "build_instructions = '''\ud83d\udccb Building and Running the C++ Application:\n",
        "\n",
        "1. Install libtorch:\n",
        "   wget https://download.pytorch.org/libtorch/cpu/libtorch-cxx11-abi-shared-with-deps-latest.zip\n",
        "   unzip libtorch-cxx11-abi-shared-with-deps-latest.zip\n",
        "\n",
        "2. Build the application:\n",
        "   mkdir build && cd build\n",
        "   cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..\n",
        "   cmake --build . --config Release\n",
        "\n",
        "3. Run the inference:\n",
        "   ./australian_sentiment_inference'''\n",
        "\n",
        "print(build_instructions)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\ud83c\udfd7\ufe0f C++ DEPLOYMENT ADVANTAGES\")\n",
        "print(\"=\"*60)\n",
        "print(\"1. Zero Python dependency - pure C++ execution\")\n",
        "print(\"2. Maximum performance for inference\")\n",
        "print(\"3. Easy integration with existing C++ systems\")\n",
        "print(\"4. Reduced memory footprint\")\n",
        "print(\"5. Cross-platform deployment\")\n",
        "print(\"6. Real-time capabilities for latency-critical applications\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. TorchServe: Scalable Model Serving Solution\n",
        "\n",
        "**TorchServe** is PyTorch's dedicated model serving framework, designed to make it easy to deploy PyTorch models at scale with enterprise-grade features.\n",
        "\n",
        "## Key Features\n",
        "- \ud83c\udf10 **RESTful APIs**: HTTP endpoints for inference and management\n",
        "- \ud83d\udcc8 **Auto Scaling**: Dynamic worker scaling based on load\n",
        "- \ud83d\udce6 **Model Versioning**: Multiple model versions simultaneously\n",
        "- \ud83d\udcca **Metrics & Logging**: Built-in monitoring and custom metrics\n",
        "- \ud83d\udd12 **Security**: HTTPS support and authentication\n",
        "- \ud83c\udfaf **Batching**: Automatic request batching for throughput\n",
        "\n",
        "### TensorFlow Serving Comparison\n",
        "| Feature | TensorFlow Serving | TorchServe |\n",
        "|---------|-------------------|------------|\n",
        "| **Model Format** | SavedModel | MAR files |\n",
        "| **APIs** | gRPC, REST | REST |\n",
        "| **Batching** | Built-in | Built-in |\n",
        "| **Versioning** | Yes | Yes |\n",
        "| **Scaling** | Manual | Automatic |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TorchServe Deployment Demonstration\n",
        "print(\"\ud83d\udce6 TorchServe Deployment for Australian Tourism Sentiment Analysis\\n\")\n",
        "\n",
        "# Create deployment scripts and configurations\n",
        "deployment_script = '''#!/bin/bash\n",
        "# TorchServe Deployment Script for Australian Sentiment Model\n",
        "\n",
        "echo \"\ud83d\ude80 Deploying Australian Tourism Sentiment Model with TorchServe\"\n",
        "\n",
        "# Step 1: Create Model Archive (MAR)\n",
        "echo \"\ud83d\udce6 Creating Model Archive...\"\n",
        "torch-model-archiver \\\\\n",
        "    --model-name australian_tourism_sentiment \\\\\n",
        "    --version 1.0 \\\\\n",
        "    --serialized-file australian_sentiment_torchscript.pt \\\\\n",
        "    --export-path model_store \\\\\n",
        "    --force\n",
        "\n",
        "echo \"\u2705 Model archive created successfully!\"\n",
        "\n",
        "# Step 2: Start TorchServe\n",
        "echo \"\ud83c\udf10 Starting TorchServe...\"\n",
        "torchserve \\\\\n",
        "    --start \\\\\n",
        "    --model-store model_store \\\\\n",
        "    --models australian_tourism_sentiment.mar\n",
        "\n",
        "echo \"\ud83c\udf89 TorchServe started successfully!\"\n",
        "echo \"\ud83d\udce1 Inference API: http://localhost:8080\"\n",
        "echo \"\ud83d\udd27 Management API: http://localhost:8081\"'''\n",
        "\n",
        "with open('deploy_model.sh', 'w') as f:\n",
        "    f.write(deployment_script)\n",
        "\n",
        "print(f\"\ud83d\udcbe Deployment script saved to: deploy_model.sh\")\n",
        "\n",
        "# API usage examples\n",
        "api_examples = '''\ud83c\udf10 TorchServe API Usage Examples:\n",
        "\n",
        "1. Health Check:\n",
        "   curl http://localhost:8080/ping\n",
        "\n",
        "2. Model Information:\n",
        "   curl http://localhost:8081/models/australian_tourism_sentiment\n",
        "\n",
        "3. Inference Request:\n",
        "   curl -X POST http://localhost:8080/predictions/australian_tourism_sentiment \\\\\n",
        "        -H \"Content-Type: application/json\" \\\\\n",
        "        -d '{\"data\": \"The Sydney Opera House tour was amazing!\"}'\n",
        "\n",
        "4. Batch Inference:\n",
        "   curl -X POST http://localhost:8080/predictions/australian_tourism_sentiment \\\\\n",
        "        -H \"Content-Type: application/json\" \\\\\n",
        "        -d '{\"data\": [\"Sydney is beautiful\", \"Melbourne coffee is great\"]}'\n",
        "\n",
        "5. Model Metrics:\n",
        "   curl http://localhost:8082/metrics\n",
        "\n",
        "6. Scale Workers:\n",
        "   curl -X PUT http://localhost:8081/models/australian_tourism_sentiment?min_worker=2&max_worker=4'''\n",
        "\n",
        "print(api_examples)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\ud83c\udfed TORCHSERVE ENTERPRISE FEATURES\")\n",
        "print(\"=\"*60)\n",
        "print(\"1. RESTful APIs for inference and management\")\n",
        "print(\"2. Automatic scaling based on request load\")\n",
        "print(\"3. Multi-model serving with version management\")\n",
        "print(\"4. Built-in metrics and monitoring\")\n",
        "print(\"5. Request batching for improved throughput\")\n",
        "print(\"6. A/B testing capabilities\")\n",
        "print(\"7. GPU acceleration support\")\n",
        "print(\"8. Docker container ready\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Summary: Production Deployment Journey\n",
        "\n",
        "Congratulations! You've completed a comprehensive tour of PyTorch production deployment strategies. Let's summarize what we've covered:\n",
        "\n",
        "## \ud83c\udfaf Deployment Methods Comparison\n",
        "\n",
        "| Method | Use Case | Performance | Complexity | Best For |\n",
        "|--------|----------|-------------|------------|----------|\n",
        "| **Evaluation Mode** | Basic inference | Good | Low | Development, testing |\n",
        "| **TorchScript** | Optimized inference | Better | Medium | Production without Python |\n",
        "| **C++ Deployment** | High-performance | Best | High | Real-time, embedded systems |\n",
        "| **TorchServe** | Scalable serving | Good | Medium | Web services, microservices |\n",
        "\n",
        "## \ud83d\ude80 Key Takeaways\n",
        "\n",
        "### 1. Always Start with Evaluation Mode\n",
        "- Call `model.eval()` before any inference\n",
        "- Use `torch.no_grad()` to disable autograd\n",
        "- Ensure consistent preprocessing\n",
        "\n",
        "### 2. TorchScript for Production Optimization\n",
        "- Use `torch.jit.trace()` for most models\n",
        "- Use `torch.jit.script()` when you need control flow\n",
        "- Single file deployment with optimized execution\n",
        "\n",
        "### 3. C++ for Maximum Performance\n",
        "- Zero Python overhead\n",
        "- Ideal for latency-critical applications\n",
        "- Easy integration with existing C++ systems\n",
        "\n",
        "### 4. TorchServe for Scalable Services\n",
        "- Enterprise-grade model serving\n",
        "- Automatic scaling and load balancing\n",
        "- Built-in monitoring and metrics\n",
        "\n",
        "## \ud83c\udf0f Australian Tourism Model Deployment\n",
        "\n",
        "Our Australian tourism sentiment analysis model demonstrates:\n",
        "- **Multilingual support** (English + Vietnamese)\n",
        "- **Real-world use cases** (hotel reviews, travel feedback)\n",
        "- **Production-ready architecture** (proper error handling, logging)\n",
        "- **Scalable deployment** (from single inference to distributed serving)\n",
        "\n",
        "## \ud83c\udf93 Next Steps in Your PyTorch Production Journey\n",
        "\n",
        "1. \ud83d\udd27 **Model Optimization**: Explore quantization, pruning, and distillation\n",
        "2. \ud83d\udcf1 **Mobile Deployment**: Learn PyTorch Mobile for on-device inference\n",
        "3. \u2601\ufe0f **Cloud Deployment**: Deploy models on AWS, GCP, or Azure\n",
        "4. \ud83d\udc33 **Containerization**: Package models with Docker for easy deployment\n",
        "5. \ud83d\udcca **Monitoring**: Implement model performance monitoring in production\n",
        "6. \ud83d\udd04 **MLOps**: Set up continuous integration/deployment pipelines\n",
        "\n",
        "## \ud83d\udcda Additional Resources\n",
        "\n",
        "- [PyTorch Production Tutorials](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html)\n",
        "- [TorchServe Documentation](https://pytorch.org/serve/)\n",
        "- [libtorch C++ Documentation](https://pytorch.org/cppdocs/)\n",
        "- [PyTorch Mobile](https://pytorch.org/mobile/home/)\n",
        "\n",
        "---\n",
        "\n",
        "**\ud83c\udf89 You're now equipped to deploy PyTorch models in production environments! Whether you're building real-time inference systems, scalable web services, or mobile applications, you have the knowledge and tools to succeed.**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}