{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1bzT9PxARlR"
      },
      "source": [
        "# PyTorch Autograd: Automatic Differentiation Fundamentals\n",
        "\n",
        "This notebook demonstrates PyTorch's **Autograd** system - the automatic differentiation engine that powers backpropagation-based learning in neural networks. It's designed for learners transitioning from TensorFlow to understand PyTorch's approach to gradient computation.\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand what Autograd does and why it's powerful\n",
        "- Learn gradient tracking with `requires_grad=True`\n",
        "- Master computation graph creation and traversal\n",
        "- Implement gradient computation in training loops\n",
        "- Control gradient tracking with context managers and methods\n",
        "- Compare TensorFlow vs PyTorch gradient computation approaches\n",
        "\n",
        "## What Makes Autograd Powerful?\n",
        "- **Dynamic Computation Graphs**: Built at runtime, perfect for dynamic models\n",
        "- **Chain Rule Automation**: Automatically applies calculus chain rule\n",
        "- **Memory Efficient**: Only stores necessary intermediate results\n",
        "- **Flexible**: Works with control flow, loops, and conditional operations\n",
        "\n",
        "**TensorFlow vs PyTorch**: While TensorFlow uses `tf.GradientTape`, PyTorch's autograd is built into every tensor operation, making gradient computation more intuitive.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9M_Fe76ARlS"
      },
      "source": [
        "## 1. Environment Setup and Runtime Detection\n",
        "\n",
        "Following PyTorch best practices for cross-platform compatibility:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AKAt33VkARlS",
        "outputId": "a7956374-6cfa-4769-e2fb-49e3cb7842d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment: Local=False, Colab=True, Kaggle=False\n",
            "‚úì torch\n",
            "‚úì matplotlib\n",
            "‚úì numpy\n",
            "‚úì tensorboard\n",
            "\n",
            "‚úÖ PyTorch 2.8.0+cu126 ready!\n",
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "# Environment Detection and Setup\n",
        "import sys\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Detect runtime environment\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
        "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
        "\n",
        "print(f\"Environment: Local={IS_LOCAL}, Colab={IS_COLAB}, Kaggle={IS_KAGGLE}\")\n",
        "\n",
        "# Install packages\n",
        "packages = [\"torch\", \"matplotlib\", \"numpy\", \"tensorboard\"]\n",
        "for pkg in packages:\n",
        "    if IS_COLAB or IS_KAGGLE:\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "    else:\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg], capture_output=True)\n",
        "    print(f\"‚úì {pkg}\")\n",
        "\n",
        "# Import PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "print(f\"\\n‚úÖ PyTorch {torch.__version__} ready!\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WikddH9LARlT"
      },
      "source": [
        "## 2. Basic Autograd: Australian Tourism Revenue\n",
        "\n",
        "Let's understand autograd with a Sydney tourism revenue example.\n",
        "\n",
        "**TensorFlow Equivalent**: `tf.GradientTape()` for gradient computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0fS6TtYVARlT",
        "outputId": "98af0bb8-ea24-434b-fb86-e6b722d8e36f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üá¶üá∫ Sydney Tourism Revenue Analysis with Autograd\n",
            "\n",
            "Initial revenue: $150k AUD\n",
            "Requires grad: True\n",
            "\n",
            "Revenue calculation:\n",
            "Base: $150k\n",
            "After boost: $180k\n",
            "After bonus: $230k\n",
            "After tax: $207k\n",
            "\n",
            "Computation graph:\n",
            "seasonal_boost.grad_fn: <MulBackward0 object at 0x7ee731e5f730>\n",
            "weekend_bonus.grad_fn: <AddBackward0 object at 0x7ee731e5f730>\n",
            "after_tax.grad_fn: <MulBackward0 object at 0x7ee731e5f730>\n",
            "\n",
            "Gradient: 1.08\n",
            "Interpretation: $1k increase in base revenue ‚Üí $1.08k increase in final revenue\n",
            "Expected: 1.08 ‚úì\n"
          ]
        }
      ],
      "source": [
        "# Basic autograd demonstration\n",
        "print(\"üá¶üá∫ Sydney Tourism Revenue Analysis with Autograd\\n\")\n",
        "\n",
        "# Create tensor with gradient tracking\n",
        "sydney_revenue = torch.tensor([150.0], requires_grad=True)\n",
        "print(f\"Initial revenue: ${sydney_revenue.item():.0f}k AUD\")\n",
        "print(f\"Requires grad: {sydney_revenue.requires_grad}\")\n",
        "\n",
        "# Build computation graph\n",
        "seasonal_boost = sydney_revenue * 1.2  # +20% summer boost\n",
        "weekend_bonus = seasonal_boost + 50.0  # +$50k weekend\n",
        "after_tax = weekend_bonus * 0.9       # -10% tax\n",
        "\n",
        "print(f\"\\nRevenue calculation:\")\n",
        "print(f\"Base: ${sydney_revenue.item():.0f}k\")\n",
        "print(f\"After boost: ${seasonal_boost.item():.0f}k\")\n",
        "print(f\"After bonus: ${weekend_bonus.item():.0f}k\")\n",
        "print(f\"After tax: ${after_tax.item():.0f}k\")\n",
        "\n",
        "# Check computation graph\n",
        "print(f\"\\nComputation graph:\")\n",
        "print(f\"seasonal_boost.grad_fn: {seasonal_boost.grad_fn}\")\n",
        "print(f\"weekend_bonus.grad_fn: {weekend_bonus.grad_fn}\")\n",
        "print(f\"after_tax.grad_fn: {after_tax.grad_fn}\")\n",
        "\n",
        "# Compute gradients\n",
        "after_tax.backward()\n",
        "gradient = sydney_revenue.grad.item()\n",
        "\n",
        "print(f\"\\nGradient: {gradient:.2f}\")\n",
        "print(f\"Interpretation: $1k increase in base revenue ‚Üí ${gradient:.2f}k increase in final revenue\")\n",
        "\n",
        "# Manual verification: 1.2 * 0.9 = 1.08\n",
        "expected = 1.2 * 0.9\n",
        "print(f\"Expected: {expected:.2f} ‚úì\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDR1AZX9ARlT"
      },
      "source": [
        "## 3. Autograd in Training: Australian City Classifier\n",
        "\n",
        "See how autograd works in a real training loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JFF5CqJ1ARlT",
        "outputId": "1eba1a7b-0c9b-4eec-926e-aef7a61bca79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèôÔ∏è Australian City Classification Training\n",
            "\n",
            "Cities: ['Sydney', 'Melbourne', 'Brisbane', 'Perth', 'Adelaide', 'Darwin', 'Hobart', 'Canberra']\n",
            "\n",
            "Model: CityClassifier(\n",
            "  (fc1): Linear(in_features=10, out_features=16, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc2): Linear(in_features=16, out_features=8, bias=True)\n",
            ")\n",
            "Parameters: 312\n",
            "\n",
            "üöÇ Training Step Analysis:\n",
            "1. Zeroing gradients...\n",
            "2. Forward pass...\n",
            "   Loss: 2.0795\n",
            "   Loss requires_grad: True\n",
            "3. Backward pass...\n",
            "   Gradients computed:\n",
            "     fc1.weight: 0.258289\n",
            "     fc1.bias: 0.079799\n",
            "     fc2.weight: 0.306681\n",
            "     fc2.bias: 0.205234\n",
            "4. Updating parameters...\n",
            "   ‚úÖ Parameters updated!\n",
            "\n",
            "üìä TensorFlow vs PyTorch Training:\n",
            "   TensorFlow: with tf.GradientTape() as tape:\n",
            "               gradients = tape.gradient(loss, variables)\n",
            "   PyTorch:    loss.backward(); optimizer.step()\n"
          ]
        }
      ],
      "source": [
        "# Australian city classification with autograd\n",
        "print(\"üèôÔ∏è Australian City Classification Training\\n\")\n",
        "\n",
        "cities = [\"Sydney\", \"Melbourne\", \"Brisbane\", \"Perth\", \"Adelaide\", \"Darwin\", \"Hobart\", \"Canberra\"]\n",
        "print(f\"Cities: {cities}\\n\")\n",
        "\n",
        "# Simple model\n",
        "class CityClassifier(nn.Module):\n",
        "    def __init__(self, input_size=10, num_cities=8):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 16)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(16, num_cities)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "model = CityClassifier()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(f\"Model: {model}\")\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "\n",
        "# Training step demonstration\n",
        "print(f\"\\nüöÇ Training Step Analysis:\")\n",
        "\n",
        "# Generate synthetic data\n",
        "X = torch.randn(32, 10)\n",
        "y = torch.randint(0, 8, (32,))\n",
        "\n",
        "# 1. Zero gradients\n",
        "print(\"1. Zeroing gradients...\")\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# 2. Forward pass\n",
        "print(\"2. Forward pass...\")\n",
        "outputs = model(X)\n",
        "loss = criterion(outputs, y)\n",
        "print(f\"   Loss: {loss.item():.4f}\")\n",
        "print(f\"   Loss requires_grad: {loss.requires_grad}\")\n",
        "\n",
        "# 3. Backward pass\n",
        "print(\"3. Backward pass...\")\n",
        "loss.backward()\n",
        "\n",
        "# Check gradients\n",
        "print(\"   Gradients computed:\")\n",
        "for name, param in model.named_parameters():\n",
        "    if param.grad is not None:\n",
        "        grad_norm = param.grad.norm().item()\n",
        "        print(f\"     {name}: {grad_norm:.6f}\")\n",
        "\n",
        "# 4. Update parameters\n",
        "print(\"4. Updating parameters...\")\n",
        "optimizer.step()\n",
        "print(\"   ‚úÖ Parameters updated!\")\n",
        "\n",
        "print(f\"\\nüìä TensorFlow vs PyTorch Training:\")\n",
        "print(f\"   TensorFlow: with tf.GradientTape() as tape:\")\n",
        "print(f\"               gradients = tape.gradient(loss, variables)\")\n",
        "print(f\"   PyTorch:    loss.backward(); optimizer.step()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNjQxx4NARlT"
      },
      "source": [
        "## 4. Controlling Gradient Tracking\n",
        "\n",
        "Learn when and how to disable gradient computation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GNPuJwutARlT",
        "outputId": "6e60fc6c-d6d5-4e5a-e709-811cf51ff2d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéõÔ∏è Controlling Gradient Tracking\n",
            "\n",
            "x requires_grad: True\n",
            "y1 = x¬≤ requires_grad: True\n",
            "y2 = x¬≤ (no_grad) requires_grad: False\n",
            "\n",
            "Using .detach():\n",
            "z requires_grad: True\n",
            "z_detached requires_grad: False\n",
            "\n",
            "üîç Practical Example: Inference\n",
            "Inference with gradients (slower):\n",
            "   Time: 0.000455s\n",
            "Inference without gradients (faster):\n",
            "   Time: 0.000809s\n",
            "   Speedup: 0.56x\n",
            "\n",
            "üìä TensorFlow vs PyTorch Gradient Control:\n",
            "   TensorFlow: @tf.function, tf.stop_gradient()\n",
            "   PyTorch:    torch.no_grad(), .detach()\n"
          ]
        }
      ],
      "source": [
        "# Gradient control methods\n",
        "print(\"üéõÔ∏è Controlling Gradient Tracking\\n\")\n",
        "\n",
        "# Method 1: torch.no_grad()\n",
        "x = torch.tensor([2.0], requires_grad=True)\n",
        "print(f\"x requires_grad: {x.requires_grad}\")\n",
        "\n",
        "y1 = x ** 2\n",
        "print(f\"y1 = x¬≤ requires_grad: {y1.requires_grad}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    y2 = x ** 2\n",
        "    print(f\"y2 = x¬≤ (no_grad) requires_grad: {y2.requires_grad}\")\n",
        "\n",
        "# Method 2: .detach()\n",
        "print(f\"\\nUsing .detach():\")\n",
        "z = x ** 2\n",
        "z_detached = z.detach()\n",
        "print(f\"z requires_grad: {z.requires_grad}\")\n",
        "print(f\"z_detached requires_grad: {z_detached.requires_grad}\")\n",
        "\n",
        "# Practical example: Model evaluation\n",
        "print(f\"\\nüîç Practical Example: Inference\")\n",
        "test_input = torch.randn(5, 10)\n",
        "\n",
        "model.eval()\n",
        "print(\"Inference with gradients (slower):\")\n",
        "start = time.time()\n",
        "with_grad_out = model(test_input)\n",
        "with_grad_time = time.time() - start\n",
        "print(f\"   Time: {with_grad_time:.6f}s\")\n",
        "\n",
        "print(\"Inference without gradients (faster):\")\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "    no_grad_out = model(test_input)\n",
        "no_grad_time = time.time() - start\n",
        "print(f\"   Time: {no_grad_time:.6f}s\")\n",
        "print(f\"   Speedup: {with_grad_time/no_grad_time:.2f}x\")\n",
        "\n",
        "print(f\"\\nüìä TensorFlow vs PyTorch Gradient Control:\")\n",
        "print(f\"   TensorFlow: @tf.function, tf.stop_gradient()\")\n",
        "print(f\"   PyTorch:    torch.no_grad(), .detach()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiQb4FJMARlU"
      },
      "source": [
        "## 5. Why optimizer.zero_grad() is Critical\n",
        "\n",
        "PyTorch accumulates gradients - see why zero_grad() is essential:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EqVh0d4EARlU",
        "outputId": "9a83e8a7-e948-425c-c090-6f373dd682c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Gradient Accumulation: Why zero_grad() Matters\n",
            "\n",
            "Initial weight: -0.303381085395813\n",
            "\n",
            "‚ùå Training WITHOUT zero_grad():\n",
            "Sample 1: gradient = -5.7233 (accumulating!)\n",
            "Sample 2: gradient = -26.3835 (accumulating!)\n",
            "Sample 3: gradient = -71.1940 (accumulating!)\n",
            "Weight after wrong training: 0.4086\n",
            "\n",
            "‚úÖ Training WITH zero_grad():\n",
            "Sample 1: gradient = -2.0000 (fresh each time)\n",
            "Sample 2: gradient = -7.7600 (fresh each time)\n",
            "Sample 3: gradient = -15.8904 (fresh each time)\n",
            "Weight after correct training: 1.2565\n",
            "\n",
            "üéØ Key Takeaway:\n",
            "   ALWAYS call optimizer.zero_grad() before loss.backward()\n",
            "   PyTorch accumulates gradients for flexibility\n",
            "   But this interferes with normal training if not cleared\n"
          ]
        }
      ],
      "source": [
        "# Demonstrate gradient accumulation problem\n",
        "print(\"üìö Gradient Accumulation: Why zero_grad() Matters\\n\")\n",
        "\n",
        "# Simple linear model\n",
        "simple_model = nn.Linear(1, 1)\n",
        "data_x = torch.tensor([[1.0], [2.0], [3.0]])\n",
        "data_y = torch.tensor([[2.0], [4.0], [6.0]])\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(simple_model.parameters(), lr=0.01)\n",
        "\n",
        "print(\"Initial weight:\", simple_model.weight.item())\n",
        "\n",
        "# Training WITHOUT zero_grad() - WRONG!\n",
        "print(f\"\\n‚ùå Training WITHOUT zero_grad():\")\n",
        "for i, (x, y) in enumerate(zip(data_x, data_y)):\n",
        "    # NO zero_grad() call!\n",
        "    pred = simple_model(x)\n",
        "    loss = criterion(pred, y)\n",
        "    loss.backward()\n",
        "\n",
        "    grad = simple_model.weight.grad.item()\n",
        "    print(f\"Sample {i+1}: gradient = {grad:.4f} (accumulating!)\")\n",
        "\n",
        "optimizer.step()\n",
        "print(f\"Weight after wrong training: {simple_model.weight.item():.4f}\")\n",
        "\n",
        "# Reset model\n",
        "simple_model.weight.data.fill_(1.0)\n",
        "simple_model.bias.data.fill_(0.0)\n",
        "\n",
        "# Training WITH zero_grad() - CORRECT!\n",
        "print(f\"\\n‚úÖ Training WITH zero_grad():\")\n",
        "for i, (x, y) in enumerate(zip(data_x, data_y)):\n",
        "    optimizer.zero_grad()  # Clear gradients!\n",
        "    pred = simple_model(x)\n",
        "    loss = criterion(pred, y)\n",
        "    loss.backward()\n",
        "\n",
        "    grad = simple_model.weight.grad.item()\n",
        "    print(f\"Sample {i+1}: gradient = {grad:.4f} (fresh each time)\")\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "print(f\"Weight after correct training: {simple_model.weight.item():.4f}\")\n",
        "\n",
        "print(f\"\\nüéØ Key Takeaway:\")\n",
        "print(f\"   ALWAYS call optimizer.zero_grad() before loss.backward()\")\n",
        "print(f\"   PyTorch accumulates gradients for flexibility\")\n",
        "print(f\"   But this interferes with normal training if not cleared\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Lo9q-EfARlU"
      },
      "source": [
        "## 6. TensorBoard Gradient Monitoring\n",
        "\n",
        "Monitor gradients with TensorBoard for debugging:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ptX-zCLxARlU",
        "outputId": "1ac9e42e-73bc-4d90-e110-dff657eaa10f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä TensorBoard Gradient Monitoring\n",
            "\n",
            "Logging to: /content/tensorboard_logs/autograd_demo\n",
            "Step 0: Loss = 2.1367\n",
            "Step 5: Loss = 2.1358\n",
            "\n",
            "‚úÖ Gradient monitoring complete!\n",
            "üìä View in TensorBoard:\n",
            "   %load_ext tensorboard\n",
            "   %tensorboard --logdir /content/tensorboard_logs/autograd_demo\n"
          ]
        }
      ],
      "source": [
        "# TensorBoard gradient monitoring\n",
        "print(\"üìä TensorBoard Gradient Monitoring\\n\")\n",
        "\n",
        "# Setup logging\n",
        "if IS_COLAB:\n",
        "    log_dir = \"/content/tensorboard_logs/autograd_demo\"\n",
        "else:\n",
        "    log_dir = \"./tensorboard_logs/autograd_demo\"\n",
        "\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "writer = SummaryWriter(log_dir)\n",
        "\n",
        "# Create fresh model for monitoring\n",
        "monitor_model = CityClassifier()\n",
        "monitor_optimizer = optim.Adam(monitor_model.parameters(), lr=0.001)\n",
        "monitor_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"Logging to: {log_dir}\")\n",
        "\n",
        "# Train with gradient logging\n",
        "for step in range(10):\n",
        "    # Generate data\n",
        "    batch_x = torch.randn(16, 10)\n",
        "    batch_y = torch.randint(0, 8, (16,))\n",
        "\n",
        "    # Training step\n",
        "    monitor_optimizer.zero_grad()\n",
        "    outputs = monitor_model(batch_x)\n",
        "    loss = monitor_criterion(outputs, batch_y)\n",
        "    loss.backward()\n",
        "\n",
        "    # Log gradients\n",
        "    for name, param in monitor_model.named_parameters():\n",
        "        if param.grad is not None:\n",
        "            grad_norm = param.grad.norm().item()\n",
        "            writer.add_scalar(f\"Gradients/{name}_norm\", grad_norm, step)\n",
        "            writer.add_histogram(f\"Gradients/{name}\", param.grad, step)\n",
        "\n",
        "    writer.add_scalar(\"Loss\", loss.item(), step)\n",
        "    monitor_optimizer.step()\n",
        "\n",
        "    if step % 5 == 0:\n",
        "        print(f\"Step {step}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "writer.close()\n",
        "print(f\"\\n‚úÖ Gradient monitoring complete!\")\n",
        "print(f\"üìä View in TensorBoard:\")\n",
        "if IS_COLAB:\n",
        "    print(f\"   %load_ext tensorboard\")\n",
        "    print(f\"   %tensorboard --logdir {log_dir}\")\n",
        "else:\n",
        "    print(f\"   tensorboard --logdir {log_dir}\")\n",
        "    print(f\"   Open http://localhost:6006\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fj3qMslARlU"
      },
      "source": [
        "## 7. Summary: PyTorch Autograd Mastery\n",
        "\n",
        "üéì **Congratulations!** You've mastered PyTorch Autograd fundamentals!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TRTckiAYARlU",
        "outputId": "d3d2f8ac-53f3-4e08-e6d6-d7d1924fc0cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéì PyTorch Autograd Mastery Summary\n",
            "\n",
            "‚úÖ Concepts Mastered:\n",
            "  1. Dynamic computation graphs with requires_grad=True\n",
            "  2. Gradient computation using .backward()\n",
            "  3. Autograd in training loops with zero_grad()\n",
            "  4. Gradient control with torch.no_grad() and .detach()\n",
            "  5. Understanding gradient accumulation\n",
            "  6. TensorBoard gradient monitoring\n",
            "  7. TensorFlow vs PyTorch comparisons\n",
            "\n",
            "üåè Australian Examples:\n",
            "  1. Sydney tourism revenue optimization\n",
            "  2. Australian city classification\n",
            "  3. Real-world gradient computation scenarios\n",
            "\n",
            "üöÄ Next Steps:\n",
            "  1. üß† Neural Network architectures with nn.Module\n",
            "  2. üìö Data loading with DataLoader and Dataset\n",
            "  3. üèãÔ∏è Advanced training techniques\n",
            "  4. ü§ó Hugging Face transformers integration\n",
            "  5. ‚ö° Performance optimization\n",
            "\n",
            "üéØ Key Autograd Rules:\n",
            "  1. Always call optimizer.zero_grad() before loss.backward()\n",
            "  2. Use torch.no_grad() for inference to save memory\n",
            "  3. Monitor gradient norms to detect training issues\n",
            "  4. Use .detach() when you need values without gradients\n",
            "\n",
            "üî• PyTorch Advantages:\n",
            "  1. Intuitive gradient computation\n",
            "  2. Dynamic graphs for flexible models\n",
            "  3. Easier debugging with immediate execution\n",
            "  4. Strong research ecosystem\n",
            "\n",
            "üèÜ You're ready for advanced PyTorch development!\n",
            "Welcome to the PyTorch community! üî•\n"
          ]
        }
      ],
      "source": [
        "# Summary\n",
        "print(\"üéì PyTorch Autograd Mastery Summary\\n\")\n",
        "\n",
        "print(\"‚úÖ Concepts Mastered:\")\n",
        "concepts = [\n",
        "    \"Dynamic computation graphs with requires_grad=True\",\n",
        "    \"Gradient computation using .backward()\",\n",
        "    \"Autograd in training loops with zero_grad()\",\n",
        "    \"Gradient control with torch.no_grad() and .detach()\",\n",
        "    \"Understanding gradient accumulation\",\n",
        "    \"TensorBoard gradient monitoring\",\n",
        "    \"TensorFlow vs PyTorch comparisons\"\n",
        "]\n",
        "\n",
        "for i, concept in enumerate(concepts, 1):\n",
        "    print(f\"  {i}. {concept}\")\n",
        "\n",
        "print(f\"\\nüåè Australian Examples:\")\n",
        "examples = [\n",
        "    \"Sydney tourism revenue optimization\",\n",
        "    \"Australian city classification\",\n",
        "    \"Real-world gradient computation scenarios\"\n",
        "]\n",
        "\n",
        "for i, example in enumerate(examples, 1):\n",
        "    print(f\"  {i}. {example}\")\n",
        "\n",
        "print(f\"\\nüöÄ Next Steps:\")\n",
        "next_steps = [\n",
        "    \"üß† Neural Network architectures with nn.Module\",\n",
        "    \"üìö Data loading with DataLoader and Dataset\",\n",
        "    \"üèãÔ∏è Advanced training techniques\",\n",
        "    \"ü§ó Hugging Face transformers integration\",\n",
        "    \"‚ö° Performance optimization\"\n",
        "]\n",
        "\n",
        "for i, step in enumerate(next_steps, 1):\n",
        "    print(f\"  {i}. {step}\")\n",
        "\n",
        "print(f\"\\nüéØ Key Autograd Rules:\")\n",
        "rules = [\n",
        "    \"Always call optimizer.zero_grad() before loss.backward()\",\n",
        "    \"Use torch.no_grad() for inference to save memory\",\n",
        "    \"Monitor gradient norms to detect training issues\",\n",
        "    \"Use .detach() when you need values without gradients\"\n",
        "]\n",
        "\n",
        "for i, rule in enumerate(rules, 1):\n",
        "    print(f\"  {i}. {rule}\")\n",
        "\n",
        "print(f\"\\nüî• PyTorch Advantages:\")\n",
        "advantages = [\n",
        "    \"Intuitive gradient computation\",\n",
        "    \"Dynamic graphs for flexible models\",\n",
        "    \"Easier debugging with immediate execution\",\n",
        "    \"Strong research ecosystem\"\n",
        "]\n",
        "\n",
        "for i, advantage in enumerate(advantages, 1):\n",
        "    print(f\"  {i}. {advantage}\")\n",
        "\n",
        "print(f\"\\nüèÜ You're ready for advanced PyTorch development!\")\n",
        "print(f\"Welcome to the PyTorch community! üî•\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}