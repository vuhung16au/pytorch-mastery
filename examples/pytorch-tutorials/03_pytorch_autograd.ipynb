{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Autograd: Automatic Differentiation Fundamentals\n",
    "\n",
    "This notebook demonstrates PyTorch's **Autograd** system - the automatic differentiation engine that powers backpropagation-based learning in neural networks. It's designed for learners transitioning from TensorFlow to understand PyTorch's approach to gradient computation.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand what Autograd does and why it's powerful\n",
    "- Learn gradient tracking with `requires_grad=True`\n",
    "- Master computation graph creation and traversal\n",
    "- Implement gradient computation in training loops\n",
    "- Control gradient tracking with context managers and methods\n",
    "- Compare TensorFlow vs PyTorch gradient computation approaches\n",
    "\n",
    "## What Makes Autograd Powerful?\n",
    "- **Dynamic Computation Graphs**: Built at runtime, perfect for dynamic models\n",
    "- **Chain Rule Automation**: Automatically applies calculus chain rule\n",
    "- **Memory Efficient**: Only stores necessary intermediate results\n",
    "- **Flexible**: Works with control flow, loops, and conditional operations\n",
    "\n",
    "**TensorFlow vs PyTorch**: While TensorFlow uses `tf.GradientTape`, PyTorch's autograd is built into every tensor operation, making gradient computation more intuitive.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Runtime Detection\n",
    "\n",
    "Following PyTorch best practices for cross-platform compatibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Detection and Setup\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Detect runtime environment\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
    "\n",
    "print(f\"Environment: Local={IS_LOCAL}, Colab={IS_COLAB}, Kaggle={IS_KAGGLE}\")\n",
    "\n",
    "# Install packages\n",
    "packages = [\"torch\", \"matplotlib\", \"numpy\", \"tensorboard\"]\n",
    "for pkg in packages:\n",
    "    if IS_COLAB or IS_KAGGLE:\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "    else:\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg], capture_output=True)\n",
    "    print(f\"\u2713 {pkg}\")\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "print(f\"\\n\u2705 PyTorch {torch.__version__} ready!\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Autograd: Australian Tourism Revenue\n",
    "\n",
    "Let's understand autograd with a Sydney tourism revenue example.\n",
    "\n",
    "**TensorFlow Equivalent**: `tf.GradientTape()` for gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic autograd demonstration\n",
    "print(\"\ud83c\udde6\ud83c\uddfa Sydney Tourism Revenue Analysis with Autograd\\n\")\n",
    "\n",
    "# Create tensor with gradient tracking\n",
    "sydney_revenue = torch.tensor([150.0], requires_grad=True)\n",
    "print(f\"Initial revenue: ${sydney_revenue.item():.0f}k AUD\")\n",
    "print(f\"Requires grad: {sydney_revenue.requires_grad}\")\n",
    "\n",
    "# Build computation graph\n",
    "seasonal_boost = sydney_revenue * 1.2  # +20% summer boost\n",
    "weekend_bonus = seasonal_boost + 50.0  # +$50k weekend\n",
    "after_tax = weekend_bonus * 0.9       # -10% tax\n",
    "\n",
    "print(f\"\\nRevenue calculation:\")\n",
    "print(f\"Base: ${sydney_revenue.item():.0f}k\")\n",
    "print(f\"After boost: ${seasonal_boost.item():.0f}k\")\n",
    "print(f\"After bonus: ${weekend_bonus.item():.0f}k\")\n",
    "print(f\"After tax: ${after_tax.item():.0f}k\")\n",
    "\n",
    "# Check computation graph\n",
    "print(f\"\\nComputation graph:\")\n",
    "print(f\"seasonal_boost.grad_fn: {seasonal_boost.grad_fn}\")\n",
    "print(f\"weekend_bonus.grad_fn: {weekend_bonus.grad_fn}\")\n",
    "print(f\"after_tax.grad_fn: {after_tax.grad_fn}\")\n",
    "\n",
    "# Compute gradients\n",
    "after_tax.backward()\n",
    "gradient = sydney_revenue.grad.item()\n",
    "\n",
    "print(f\"\\nGradient: {gradient:.2f}\")\n",
    "print(f\"Interpretation: $1k increase in base revenue \u2192 ${gradient:.2f}k increase in final revenue\")\n",
    "\n",
    "# Manual verification: 1.2 * 0.9 = 1.08\n",
    "expected = 1.2 * 0.9\n",
    "print(f\"Expected: {expected:.2f} \u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Autograd in Training: Australian City Classifier\n",
    "\n",
    "See how autograd works in a real training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Australian city classification with autograd\n",
    "print(\"\ud83c\udfd9\ufe0f Australian City Classification Training\\n\")\n",
    "\n",
    "cities = [\"Sydney\", \"Melbourne\", \"Brisbane\", \"Perth\", \"Adelaide\", \"Darwin\", \"Hobart\", \"Canberra\"]\n",
    "print(f\"Cities: {cities}\\n\")\n",
    "\n",
    "# Simple model\n",
    "class CityClassifier(nn.Module):\n",
    "    def __init__(self, input_size=10, num_cities=8):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16, num_cities)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "model = CityClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Model: {model}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Training step demonstration\n",
    "print(f\"\\n\ud83d\ude82 Training Step Analysis:\")\n",
    "\n",
    "# Generate synthetic data\n",
    "X = torch.randn(32, 10)\n",
    "y = torch.randint(0, 8, (32,))\n",
    "\n",
    "# 1. Zero gradients\n",
    "print(\"1. Zeroing gradients...\")\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 2. Forward pass\n",
    "print(\"2. Forward pass...\")\n",
    "outputs = model(X)\n",
    "loss = criterion(outputs, y)\n",
    "print(f\"   Loss: {loss.item():.4f}\")\n",
    "print(f\"   Loss requires_grad: {loss.requires_grad}\")\n",
    "\n",
    "# 3. Backward pass\n",
    "print(\"3. Backward pass...\")\n",
    "loss.backward()\n",
    "\n",
    "# Check gradients\n",
    "print(\"   Gradients computed:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        grad_norm = param.grad.norm().item()\n",
    "        print(f\"     {name}: {grad_norm:.6f}\")\n",
    "\n",
    "# 4. Update parameters\n",
    "print(\"4. Updating parameters...\")\n",
    "optimizer.step()\n",
    "print(\"   \u2705 Parameters updated!\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca TensorFlow vs PyTorch Training:\")\n",
    "print(f\"   TensorFlow: with tf.GradientTape() as tape:\")\n",
    "print(f\"               gradients = tape.gradient(loss, variables)\")\n",
    "print(f\"   PyTorch:    loss.backward(); optimizer.step()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Controlling Gradient Tracking\n",
    "\n",
    "Learn when and how to disable gradient computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient control methods\n",
    "print(\"\ud83c\udf9b\ufe0f Controlling Gradient Tracking\\n\")\n",
    "\n",
    "# Method 1: torch.no_grad()\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "print(f\"x requires_grad: {x.requires_grad}\")\n",
    "\n",
    "y1 = x ** 2\n",
    "print(f\"y1 = x\u00b2 requires_grad: {y1.requires_grad}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    y2 = x ** 2\n",
    "    print(f\"y2 = x\u00b2 (no_grad) requires_grad: {y2.requires_grad}\")\n",
    "\n",
    "# Method 2: .detach()\n",
    "print(f\"\\nUsing .detach():\")\n",
    "z = x ** 2\n",
    "z_detached = z.detach()\n",
    "print(f\"z requires_grad: {z.requires_grad}\")\n",
    "print(f\"z_detached requires_grad: {z_detached.requires_grad}\")\n",
    "\n",
    "# Practical example: Model evaluation\n",
    "print(f\"\\n\ud83d\udd0d Practical Example: Inference\")\n",
    "test_input = torch.randn(5, 10)\n",
    "\n",
    "model.eval()\n",
    "print(\"Inference with gradients (slower):\")\n",
    "start = time.time()\n",
    "with_grad_out = model(test_input)\n",
    "with_grad_time = time.time() - start\n",
    "print(f\"   Time: {with_grad_time:.6f}s\")\n",
    "\n",
    "print(\"Inference without gradients (faster):\")\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    no_grad_out = model(test_input)\n",
    "no_grad_time = time.time() - start\n",
    "print(f\"   Time: {no_grad_time:.6f}s\")\n",
    "print(f\"   Speedup: {with_grad_time/no_grad_time:.2f}x\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca TensorFlow vs PyTorch Gradient Control:\")\n",
    "print(f\"   TensorFlow: @tf.function, tf.stop_gradient()\")\n",
    "print(f\"   PyTorch:    torch.no_grad(), .detach()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Why optimizer.zero_grad() is Critical\n",
    "\n",
    "PyTorch accumulates gradients - see why zero_grad() is essential:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate gradient accumulation problem\n",
    "print(\"\ud83d\udcda Gradient Accumulation: Why zero_grad() Matters\\n\")\n",
    "\n",
    "# Simple linear model\n",
    "simple_model = nn.Linear(1, 1)\n",
    "data_x = torch.tensor([[1.0], [2.0], [3.0]])\n",
    "data_y = torch.tensor([[2.0], [4.0], [6.0]])\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(simple_model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Initial weight:\", simple_model.weight.item())\n",
    "\n",
    "# Training WITHOUT zero_grad() - WRONG!\n",
    "print(f\"\\n\u274c Training WITHOUT zero_grad():\")\n",
    "for i, (x, y) in enumerate(zip(data_x, data_y)):\n",
    "    # NO zero_grad() call!\n",
    "    pred = simple_model(x)\n",
    "    loss = criterion(pred, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    grad = simple_model.weight.grad.item()\n",
    "    print(f\"Sample {i+1}: gradient = {grad:.4f} (accumulating!)\")\n",
    "\n",
    "optimizer.step()\n",
    "print(f\"Weight after wrong training: {simple_model.weight.item():.4f}\")\n",
    "\n",
    "# Reset model\n",
    "simple_model.weight.data.fill_(1.0)\n",
    "simple_model.bias.data.fill_(0.0)\n",
    "\n",
    "# Training WITH zero_grad() - CORRECT!\n",
    "print(f\"\\n\u2705 Training WITH zero_grad():\")\n",
    "for i, (x, y) in enumerate(zip(data_x, data_y)):\n",
    "    optimizer.zero_grad()  # Clear gradients!\n",
    "    pred = simple_model(x)\n",
    "    loss = criterion(pred, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    grad = simple_model.weight.grad.item()\n",
    "    print(f\"Sample {i+1}: gradient = {grad:.4f} (fresh each time)\")\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"Weight after correct training: {simple_model.weight.item():.4f}\")\n",
    "\n",
    "print(f\"\\n\ud83c\udfaf Key Takeaway:\")\n",
    "print(f\"   ALWAYS call optimizer.zero_grad() before loss.backward()\")\n",
    "print(f\"   PyTorch accumulates gradients for flexibility\")\n",
    "print(f\"   But this interferes with normal training if not cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TensorBoard Gradient Monitoring\n",
    "\n",
    "Monitor gradients with TensorBoard for debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard gradient monitoring\n",
    "print(\"\ud83d\udcca TensorBoard Gradient Monitoring\\n\")\n",
    "\n",
    "# Setup logging\n",
    "if IS_COLAB:\n",
    "    log_dir = \"/content/tensorboard_logs/autograd_demo\"\n",
    "else:\n",
    "    log_dir = \"./tensorboard_logs/autograd_demo\"\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "# Create fresh model for monitoring\n",
    "monitor_model = CityClassifier()\n",
    "monitor_optimizer = optim.Adam(monitor_model.parameters(), lr=0.001)\n",
    "monitor_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Logging to: {log_dir}\")\n",
    "\n",
    "# Train with gradient logging\n",
    "for step in range(10):\n",
    "    # Generate data\n",
    "    batch_x = torch.randn(16, 10)\n",
    "    batch_y = torch.randint(0, 8, (16,))\n",
    "    \n",
    "    # Training step\n",
    "    monitor_optimizer.zero_grad()\n",
    "    outputs = monitor_model(batch_x)\n",
    "    loss = monitor_criterion(outputs, batch_y)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Log gradients\n",
    "    for name, param in monitor_model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            writer.add_scalar(f\"Gradients/{name}_norm\", grad_norm, step)\n",
    "            writer.add_histogram(f\"Gradients/{name}\", param.grad, step)\n",
    "    \n",
    "    writer.add_scalar(\"Loss\", loss.item(), step)\n",
    "    monitor_optimizer.step()\n",
    "    \n",
    "    if step % 5 == 0:\n",
    "        print(f\"Step {step}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "writer.close()\n",
    "print(f\"\\n\u2705 Gradient monitoring complete!\")\n",
    "print(f\"\ud83d\udcca View in TensorBoard:\")\n",
    "if IS_COLAB:\n",
    "    print(f\"   %load_ext tensorboard\")\n",
    "    print(f\"   %tensorboard --logdir {log_dir}\")\n",
    "else:\n",
    "    print(f\"   tensorboard --logdir {log_dir}\")\n",
    "    print(f\"   Open http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary: PyTorch Autograd Mastery\n",
    "\n",
    "\ud83c\udf93 **Congratulations!** You've mastered PyTorch Autograd fundamentals!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\ud83c\udf93 PyTorch Autograd Mastery Summary\\n\")\n",
    "\n",
    "print(\"\u2705 Concepts Mastered:\")\n",
    "concepts = [\n",
    "    \"Dynamic computation graphs with requires_grad=True\",\n",
    "    \"Gradient computation using .backward()\",\n",
    "    \"Autograd in training loops with zero_grad()\",\n",
    "    \"Gradient control with torch.no_grad() and .detach()\",\n",
    "    \"Understanding gradient accumulation\",\n",
    "    \"TensorBoard gradient monitoring\",\n",
    "    \"TensorFlow vs PyTorch comparisons\"\n",
    "]\n",
    "\n",
    "for i, concept in enumerate(concepts, 1):\n",
    "    print(f\"  {i}. {concept}\")\n",
    "\n",
    "print(f\"\\n\ud83c\udf0f Australian Examples:\")\n",
    "examples = [\n",
    "    \"Sydney tourism revenue optimization\",\n",
    "    \"Australian city classification\",\n",
    "    \"Real-world gradient computation scenarios\"\n",
    "]\n",
    "\n",
    "for i, example in enumerate(examples, 1):\n",
    "    print(f\"  {i}. {example}\")\n",
    "\n",
    "print(f\"\\n\ud83d\ude80 Next Steps:\")\n",
    "next_steps = [\n",
    "    \"\ud83e\udde0 Neural Network architectures with nn.Module\",\n",
    "    \"\ud83d\udcda Data loading with DataLoader and Dataset\",\n",
    "    \"\ud83c\udfcb\ufe0f Advanced training techniques\",\n",
    "    \"\ud83e\udd17 Hugging Face transformers integration\",\n",
    "    \"\u26a1 Performance optimization\"\n",
    "]\n",
    "\n",
    "for i, step in enumerate(next_steps, 1):\n",
    "    print(f\"  {i}. {step}\")\n",
    "\n",
    "print(f\"\\n\ud83c\udfaf Key Autograd Rules:\")\n",
    "rules = [\n",
    "    \"Always call optimizer.zero_grad() before loss.backward()\",\n",
    "    \"Use torch.no_grad() for inference to save memory\",\n",
    "    \"Monitor gradient norms to detect training issues\",\n",
    "    \"Use .detach() when you need values without gradients\"\n",
    "]\n",
    "\n",
    "for i, rule in enumerate(rules, 1):\n",
    "    print(f\"  {i}. {rule}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udd25 PyTorch Advantages:\")\n",
    "advantages = [\n",
    "    \"Intuitive gradient computation\",\n",
    "    \"Dynamic graphs for flexible models\",\n",
    "    \"Easier debugging with immediate execution\",\n",
    "    \"Strong research ecosystem\"\n",
    "]\n",
    "\n",
    "for i, advantage in enumerate(advantages, 1):\n",
    "    print(f\"  {i}. {advantage}\")\n",
    "\n",
    "print(f\"\\n\ud83c\udfc6 You're ready for advanced PyTorch development!\")\n",
    "print(f\"Welcome to the PyTorch community! \ud83d\udd25\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}