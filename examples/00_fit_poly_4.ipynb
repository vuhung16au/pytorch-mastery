{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Fitting with PyTorch: Approximating e^x with Degree 4 Polynomial\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/pytorch-mastery/blob/main/examples/00_fit_poly_4.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/pytorch-mastery/blob/main/examples/00_fit_poly_4.ipynb)\n",
    "\n",
    "This notebook demonstrates how to use PyTorch to approximate the exponential function e^x using a polynomial of degree 4. This is a fundamental example showing PyTorch's automatic differentiation and optimization capabilities with Australian-themed examples.\n",
    "\n",
    "## Learning Objectives\n",
    "- Learn polynomial approximation using PyTorch neural networks\n",
    "- Understand PyTorch's automatic differentiation (autograd) system\n",
    "- Master PyTorch training loops and optimization\n",
    "- Compare with TensorFlow approaches for function approximation\n",
    "- Apply visualization techniques using seaborn for training analysis\n",
    "\n",
    "## Mathematical Background\n",
    "We want to find coefficients $w_1, w_2, w_3, w_4, b$ such that:\n",
    "\n",
    "$$ P(x) = w_1 x + w_2 x^2 + w_3 x^3 + w_4 x^4 + b \\approx e^x $$\n",
    "\n",
    "The loss function we minimize is:\n",
    "\n",
    "$$ L = \\frac{1}{n} \\sum_{i=1}^{n} |P(x_i) - e^{x_i}| $$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Runtime Detection\n",
    "\n",
    "Following PyTorch best practices for cross-platform compatibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Detection and Setup\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "# Detect the runtime environment\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules or \"kaggle\" in os.environ.get('KAGGLE_URL_BASE', '')\n",
    "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
    "\n",
    "print(f\"Environment detected:\")\n",
    "print(f\"  - Local: {IS_LOCAL}\")\n",
    "print(f\"  - Google Colab: {IS_COLAB}\")\n",
    "print(f\"  - Kaggle: {IS_KAGGLE}\")\n",
    "\n",
    "# Platform-specific system setup\n",
    "if IS_COLAB:\n",
    "    print(\"\\nSetting up Google Colab environment...\")\n",
    "    !apt update -qq\n",
    "    !apt install -y -qq software-properties-common\n",
    "elif IS_KAGGLE:\n",
    "    print(\"\\nSetting up Kaggle environment...\")\n",
    "    # Kaggle usually has most packages pre-installed\n",
    "else:\n",
    "    print(\"\\nSetting up local environment...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages based on platform\n",
    "required_packages = [\n",
    "    \"torch\",\n",
    "    \"torchvision\", \n",
    "    \"torchaudio\",\n",
    "    \"pandas\",\n",
    "    \"seaborn\",\n",
    "    \"matplotlib\",\n",
    "    \"numpy\"\n",
    "]\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "for package in required_packages:\n",
    "    if IS_COLAB or IS_KAGGLE:\n",
    "        !pip install -q {package}\n",
    "    else:\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
    "                      capture_output=True)\n",
    "    print(f\"âœ“ {package}\")\n",
    "\n",
    "print(\"\\nâœ… Package installation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Device Detection\n",
    "\n",
    "Following repository guidelines for consistent imports and device handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports following repository standards\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # Standard alias for functional operations\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from itertools import count\n",
    "\n",
    "# Data science and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns  # Primary visualization library for notebooks\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Set seaborn style for better notebook aesthetics\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device Detection Function (following repository guidelines)\n",
    "def detect_device() -> Tuple[torch.device, str]:\n",
    "    \"\"\"\n",
    "    Helper function to detect the best available PyTorch device.\n",
    "    \n",
    "    Priority order:\n",
    "    1. CUDA (NVIDIA GPUs) - Best performance for deep learning\n",
    "    2. MPS (Apple Silicon) - Optimized for M1/M2/M3 Macs  \n",
    "    3. CPU (Universal) - Always available fallback\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (device, description) for optimal performance\n",
    "    \"\"\"\n",
    "    # Check for CUDA (NVIDIA GPU)\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        device_info = f\"CUDA GPU: {gpu_name}\"\n",
    "        \n",
    "        print(f\"ðŸš€ Using CUDA acceleration\")\n",
    "        print(f\"   GPU: {gpu_name}\")\n",
    "        print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "        \n",
    "        return device, device_info\n",
    "    \n",
    "    # Check for MPS (Apple Silicon)\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        device_info = \"Apple Silicon MPS\"\n",
    "        \n",
    "        print(f\"ðŸŽ Using Apple Silicon MPS acceleration\")\n",
    "        \n",
    "        return device, device_info\n",
    "    \n",
    "    # Fallback to CPU\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        device_info = \"CPU (No GPU acceleration available)\"\n",
    "        \n",
    "        cpu_count = torch.get_num_threads()\n",
    "        \n",
    "        print(f\"ðŸ’» Using CPU (no GPU acceleration detected)\")\n",
    "        print(f\"   PyTorch Threads: {cpu_count}\")\n",
    "        \n",
    "        return device, device_info\n",
    "\n",
    "# Detect and set global device\n",
    "DEVICE, DEVICE_INFO = detect_device()\n",
    "print(f\"\\nâœ… Selected device: {DEVICE}\")\n",
    "print(f\"ðŸ“Š Device info: {DEVICE_INFO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration and Helper Functions\n",
    "\n",
    "Following repository OOP and helper function guidelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PolyFitConfig:\n",
    "    \"\"\"Configuration class for polynomial fitting (OOP design pattern).\"\"\"\n",
    "    poly_degree: int = 4\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 0.1\n",
    "    loss_threshold: float = 1e-3\n",
    "    max_iterations: int = 10000\n",
    "    x_range: Tuple[float, float] = (-2.0, 2.0)\n",
    "    device: Optional[torch.device] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.device is None:\n",
    "            self.device = DEVICE\n",
    "\n",
    "# Helper function for creating polynomial features\n",
    "def make_features(x: torch.Tensor, degree: int = 4) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Helper function to build polynomial features matrix.\n",
    "    \n",
    "    Builds features i.e. a matrix with columns [x, x^2, x^3, x^4].\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor of shape (batch_size,)\n",
    "        degree: Polynomial degree\n",
    "        \n",
    "    Returns:\n",
    "        Feature matrix of shape (batch_size, degree)\n",
    "        \n",
    "    Example:\n",
    "        >>> x = torch.tensor([1.0, 2.0, 3.0])\n",
    "        >>> features = make_features(x, degree=4)\n",
    "        >>> print(features.shape)  # torch.Size([3, 4])\n",
    "    \"\"\"\n",
    "    x = x.unsqueeze(1)  # Shape: (batch_size, 1)\n",
    "    return torch.cat([x ** i for i in range(1, degree + 1)], 1)\n",
    "\n",
    "# Helper function for target function (e^x)\n",
    "def exponential_target(x_features: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Helper function to compute the target exponential function e^x.\n",
    "    \n",
    "    Args:\n",
    "        x_features: Feature matrix where first column contains original x values\n",
    "        \n",
    "    Returns:\n",
    "        e^x values as column vector\n",
    "        \n",
    "    Example:\n",
    "        >>> x = torch.tensor([0.0, 1.0, 2.0])\n",
    "        >>> features = make_features(x)\n",
    "        >>> y = exponential_target(features)\n",
    "        >>> print(y.shape)  # torch.Size([3, 1])\n",
    "    \"\"\"\n",
    "    original_x = x_features[:, 0]  # Extract original x from first column\n",
    "    return torch.exp(original_x).unsqueeze(1)\n",
    "\n",
    "# Helper function for polynomial description\n",
    "def format_polynomial_description(weights: torch.Tensor, bias: torch.Tensor) -> str:\n",
    "    \"\"\"\n",
    "    Helper function to create a string description of a polynomial.\n",
    "    \n",
    "    Args:\n",
    "        weights: Polynomial coefficients\n",
    "        bias: Bias term\n",
    "        \n",
    "    Returns:\n",
    "        Human-readable polynomial string\n",
    "        \n",
    "    Example:\n",
    "        >>> w = torch.tensor([1.0, 0.5, -0.1, 0.02])\n",
    "        >>> b = torch.tensor([0.1])\n",
    "        >>> desc = format_polynomial_description(w, b)\n",
    "        >>> print(desc)  # \"y = +1.00 x^1 +0.50 x^2 -0.10 x^3 +0.02 x^4 +0.10\"\n",
    "    \"\"\"\n",
    "    result = 'P(x) = '\n",
    "    for i, w in enumerate(weights):\n",
    "        result += '{:+.4f} x^{} '.format(w.item(), i + 1)\n",
    "    result += '{:+.4f}'.format(bias.item())\n",
    "    return result\n",
    "\n",
    "# Helper function for batch generation\n",
    "def generate_training_batch(config: PolyFitConfig) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Helper function to generate a training batch.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration object with batch size and range\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (features, targets) for training\n",
    "        \n",
    "    Example:\n",
    "        >>> config = PolyFitConfig(batch_size=32, x_range=(-2.0, 2.0))\n",
    "        >>> x_features, y_targets = generate_training_batch(config)\n",
    "        >>> print(x_features.shape, y_targets.shape)  # torch.Size([32, 4]) torch.Size([32, 1])\n",
    "    \"\"\"\n",
    "    # Generate random x values in specified range\n",
    "    x_min, x_max = config.x_range\n",
    "    random_x = torch.rand(config.batch_size) * (x_max - x_min) + x_min\n",
    "    \n",
    "    # Create polynomial features\n",
    "    x_features = make_features(random_x, config.poly_degree)\n",
    "    \n",
    "    # Compute target e^x values\n",
    "    y_targets = exponential_target(x_features)\n",
    "    \n",
    "    return x_features.to(config.device), y_targets.to(config.device)\n",
    "\n",
    "print(\"âœ… Configuration and helper functions defined!\")\n",
    "\n",
    "# Test helper functions with Australian context\n",
    "config = PolyFitConfig(batch_size=8, x_range=(-1.0, 1.0))  # Sydney temperature range in scaled units\n",
    "print(f\"\\nðŸ‡¦ðŸ‡º Australian Context Example:\")\n",
    "print(f\"   Simulating Sydney temperature variations (scaled to [-1, 1])\")\n",
    "print(f\"   Polynomial degree: {config.poly_degree} (modeling seasonal patterns)\")\n",
    "print(f\"   Using device: {config.device}\")\n",
    "\n",
    "# Test batch generation\n",
    "sample_x, sample_y = generate_training_batch(config)\n",
    "print(f\"\\nðŸ“Š Sample batch generated:\")\n",
    "print(f\"   Features shape: {sample_x.shape}\")\n",
    "print(f\"   Targets shape: {sample_y.shape}\")\n",
    "print(f\"   Sample e^x values: {sample_y[:3].view(-1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Polynomial Model Definition\n",
    "\n",
    "Using OOP design pattern following repository guidelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AustralianExponentialApproximator(nn.Module):\n",
    "    \"\"\"\n",
    "    OOP-based polynomial model for approximating exponential functions.\n",
    "    \n",
    "    Demonstrates preferred OOP patterns:\n",
    "    - Inherits from nn.Module\n",
    "    - Encapsulates functionality with clear responsibilities\n",
    "    - Uses helper methods for common operations\n",
    "    - Includes Australian context for educational purposes\n",
    "    \n",
    "    Designed for:\n",
    "    - Approximating e^x using polynomial of degree 4\n",
    "    - Educational demonstration of PyTorch autograd\n",
    "    - Comparison with TensorFlow approaches\n",
    "    \n",
    "    TensorFlow equivalent (procedural approach we avoid):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(1, use_bias=True, input_shape=(4,))\n",
    "        ])\n",
    "    \n",
    "    Example:\n",
    "        >>> config = PolyFitConfig(poly_degree=4)\n",
    "        >>> model = AustralianExponentialApproximator(config)\n",
    "        >>> x = torch.randn(32, 4)  # batch_size=32, features=4\n",
    "        >>> output = model(x)\n",
    "        >>> print(output.shape)  # torch.Size([32, 1])\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: PolyFitConfig):\n",
    "        super(AustralianExponentialApproximator, self).__init__()\n",
    "        \n",
    "        # Store configuration (OOP encapsulation)\n",
    "        self.config = config\n",
    "        self.device = config.device\n",
    "        \n",
    "        # Build model components using helper method\n",
    "        self._build_polynomial_layer()\n",
    "        \n",
    "        # Move to device\n",
    "        self.to(self.device)\n",
    "        \n",
    "        # Australian context metadata\n",
    "        self.model_description = \"Sydney Climate Exponential Approximator\"\n",
    "        \n",
    "    def _build_polynomial_layer(self) -> None:\n",
    "        \"\"\"Helper method to build the polynomial approximation layer.\"\"\"\n",
    "        # Linear layer: exactly what we need for polynomial approximation\n",
    "        # Input: [x, x^2, x^3, x^4] -> Output: w1*x + w2*x^2 + w3*x^3 + w4*x^4 + b\n",
    "        self.polynomial = nn.Linear(self.config.poly_degree, 1)\n",
    "        \n",
    "        # Initialize with small random weights for stable training\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self) -> None:\n",
    "        \"\"\"Helper method to initialize model weights.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Initialize weights to small random values\n",
    "            self.polynomial.weight.uniform_(-0.1, 0.1)\n",
    "            self.polynomial.bias.uniform_(-0.1, 0.1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: compute polynomial approximation.\n",
    "        \n",
    "        Args:\n",
    "            x: Input features of shape (batch_size, poly_degree)\n",
    "            \n",
    "        Returns:\n",
    "            Polynomial approximation of shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        return self.polynomial(x)\n",
    "    \n",
    "    def get_polynomial_description(self) -> str:\n",
    "        \"\"\"\n",
    "        Helper method to get human-readable polynomial description.\n",
    "        \n",
    "        Returns:\n",
    "            String representation of the learned polynomial\n",
    "        \"\"\"\n",
    "        weights = self.polynomial.weight.view(-1)\n",
    "        bias = self.polynomial.bias\n",
    "        return format_polynomial_description(weights, bias)\n",
    "    \n",
    "    def predict_australian_values(self, x_values: List[float]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        High-level method for Australian context predictions.\n",
    "        \n",
    "        Args:\n",
    "            x_values: List of x values (e.g., scaled temperature values)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with predictions and Australian context\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Convert to tensor and create features\n",
    "            x_tensor = torch.tensor(x_values, dtype=torch.float32)\n",
    "            features = make_features(x_tensor, self.config.poly_degree).to(self.device)\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = self.forward(features)\n",
    "            \n",
    "            # Get true e^x values for comparison\n",
    "            true_values = torch.exp(x_tensor)\n",
    "            \n",
    "            return {\n",
    "                'x_values': x_values,\n",
    "                'predictions': predictions.cpu().numpy().flatten(),\n",
    "                'true_exp_values': true_values.numpy(),\n",
    "                'errors': (predictions.cpu().numpy().flatten() - true_values.numpy()),\n",
    "                'model_description': self.model_description,\n",
    "                'polynomial': self.get_polynomial_description()\n",
    "            }\n",
    "\n",
    "# Create model instance\n",
    "config = PolyFitConfig()\n",
    "model = AustralianExponentialApproximator(config)\n",
    "\n",
    "print(\"ðŸ–ï¸ Australian Exponential Approximator Model:\")\n",
    "print(model)\n",
    "print(f\"\\nðŸ“Š Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"ðŸŽ¯ Target function: e^x\")\n",
    "print(f\"ðŸ”¢ Polynomial degree: {config.poly_degree}\")\n",
    "print(f\"ðŸ–¥ï¸ Device: {model.device}\")\n",
    "\n",
    "# Test initial predictions with Australian context\n",
    "sydney_temperatures = [-1.0, -0.5, 0.0, 0.5, 1.0]  # Scaled temperature values\n",
    "initial_results = model.predict_australian_values(sydney_temperatures)\n",
    "\n",
    "print(f\"\\nðŸ‡¦ðŸ‡º Initial Sydney Temperature Predictions (before training):\")\n",
    "for i, (x, pred, true) in enumerate(zip(\n",
    "    initial_results['x_values'], \n",
    "    initial_results['predictions'], \n",
    "    initial_results['true_exp_values']\n",
    ")):\n",
    "    print(f\"   T{i+1} (x={x:+.1f}): Predicted={pred:.4f}, True e^x={true:.4f}, Error={pred-true:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ Initial polynomial: {model.get_polynomial_description()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Implementation\n",
    "\n",
    "Demonstrating PyTorch manual training loop vs TensorFlow's automatic approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup following repository guidelines\n",
    "def setup_training_components(model: nn.Module, config: PolyFitConfig) -> Tuple[nn.Module, optim.Optimizer]:\n",
    "    \"\"\"\n",
    "    Helper function to setup training components.\n",
    "    \n",
    "    Args:\n",
    "        model: The polynomial approximation model\n",
    "        config: Configuration object\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (loss_function, optimizer)\n",
    "    \"\"\"\n",
    "    # Loss function: SmoothL1Loss is robust for function approximation\n",
    "    # TensorFlow equivalent: tf.keras.losses.Huber()\n",
    "    loss_function = nn.SmoothL1Loss()  # More robust than MSE for outliers\n",
    "    \n",
    "    # Optimizer setup with manual parameter updates (vs TF's automatic)\n",
    "    # TensorFlow equivalent: model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1))\n",
    "    \n",
    "    # Manual parameter updates as in the original sample\n",
    "    optimizer = None  # We'll do manual updates\n",
    "    \n",
    "    return loss_function, optimizer\n",
    "\n",
    "# Setup training components\n",
    "criterion, optimizer = setup_training_components(model, config)\n",
    "\n",
    "print(\"âš™ï¸ Training Setup Complete:\")\n",
    "print(f\"   Loss Function: {criterion}\")\n",
    "print(f\"   Parameter Updates: Manual (as in original sample)\")\n",
    "print(f\"   Learning Rate: {config.learning_rate}\")\n",
    "print(f\"   Batch Size: {config.batch_size}\")\n",
    "print(f\"   Loss Threshold: {config.loss_threshold}\")\n",
    "\n",
    "print(f\"\\nðŸ”„ PyTorch vs TensorFlow Training Differences:\")\n",
    "print(f\"   PyTorch: Manual training loop with explicit backward()\")\n",
    "print(f\"   TensorFlow: model.fit() handles training automatically\")\n",
    "print(f\"   PyTorch: Manual parameter updates (original sample style)\")\n",
    "print(f\"   TensorFlow: Automatic gradient application\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard setup following repository guidelines\n",
    "def create_tensorboard_logdir(experiment_name: str = \"polynomial_fitting\") -> str:\n",
    "    \"\"\"\n",
    "    Helper function to create timestamped directory for TensorBoard logs.\n",
    "    \n",
    "    Args:\n",
    "        experiment_name: Name of the experiment\n",
    "        \n",
    "    Returns:\n",
    "        Path to log directory\n",
    "    \"\"\"\n",
    "    # Platform-specific TensorBoard log directory setup\n",
    "    if IS_COLAB:\n",
    "        root_logdir = \"/content/tensorboard_logs\"\n",
    "    elif IS_KAGGLE:\n",
    "        root_logdir = \"./tensorboard_logs\"\n",
    "    else:\n",
    "        root_logdir = \"./tensorboard_logs\"\n",
    "    \n",
    "    # Create timestamped subdirectory\n",
    "    timestamp = time.strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "    run_logdir = os.path.join(root_logdir, f\"{experiment_name}_{timestamp}\")\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    os.makedirs(run_logdir, exist_ok=True)\n",
    "    \n",
    "    return run_logdir\n",
    "\n",
    "# Create TensorBoard writer\n",
    "run_logdir = create_tensorboard_logdir(\"australian_exponential_fitting\")\n",
    "writer = SummaryWriter(log_dir=run_logdir)\n",
    "\n",
    "print(f\"ðŸ“Š TensorBoard Setup:\")\n",
    "print(f\"   Log directory: {run_logdir}\")\n",
    "print(f\"   Writer initialized: âœ“\")\n",
    "\n",
    "# Log initial model architecture\n",
    "sample_input = torch.randn(1, config.poly_degree).to(config.device)\n",
    "writer.add_graph(model, sample_input)\n",
    "print(f\"   Model graph logged: âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed training function based on the original sample code\n",
    "print(\"ðŸš€ Starting Training: Fitting e^x with 4th degree polynomial\")\n",
    "print(f\"ðŸ‡¦ðŸ‡º Context: Sydney climate exponential modeling\")\n",
    "print(f\"ðŸ“Š Using manual parameter updates (original sample style)\")\n",
    "print(f\"âš¡ Device: {config.device}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Training metrics storage\n",
    "training_losses = []\n",
    "training_iterations = []\n",
    "\n",
    "model.train()  # Set to training mode\n",
    "\n",
    "# Manual training loop (following the original sample pattern)\n",
    "for batch_idx in count(1):\n",
    "    # Get training batch\n",
    "    batch_x, batch_y = generate_training_batch(config)\n",
    "    \n",
    "    # Reset gradients (important!)\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(batch_x)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(output, batch_y)\n",
    "    loss_value = loss.item()\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Manual parameter updates (as in original sample)\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param.data.add_(-config.learning_rate * param.grad)\n",
    "    \n",
    "    # Store metrics\n",
    "    training_losses.append(loss_value)\n",
    "    training_iterations.append(batch_idx)\n",
    "    \n",
    "    # Log to TensorBoard every 100 iterations\n",
    "    if batch_idx % 100 == 0:\n",
    "        writer.add_scalar('Loss/Training', loss_value, batch_idx)\n",
    "        \n",
    "        # Log current polynomial coefficients\n",
    "        current_weights = model.polynomial.weight.detach().cpu().numpy().flatten()\n",
    "        current_bias = model.polynomial.bias.detach().cpu().item()\n",
    "        \n",
    "        for i, coeff in enumerate(current_weights):\n",
    "            writer.add_scalar(f'Coefficients/x^{i+1}', coeff, batch_idx)\n",
    "        writer.add_scalar('Coefficients/bias', current_bias, batch_idx)\n",
    "        \n",
    "        print(f'Batch {batch_idx:5d}: Loss = {loss_value:.6f}')\n",
    "        \n",
    "        # Show current approximation quality\n",
    "        test_results = model.predict_australian_values([0.0, 1.0])\n",
    "        print(f'  P(0) = {test_results[\"predictions\"][0]:.6f}, e^0 = {test_results[\"true_exp_values\"][0]:.6f}')\n",
    "        print(f'  P(1) = {test_results[\"predictions\"][1]:.6f}, e^1 = {test_results[\"true_exp_values\"][1]:.6f}')\n",
    "    \n",
    "    # Convergence check\n",
    "    if loss_value < config.loss_threshold:\n",
    "        print(f'\\nðŸŽ¯ Convergence achieved after {batch_idx} batches!')\n",
    "        print(f'   Final loss: {loss_value:.6f}')\n",
    "        break\n",
    "    \n",
    "    # Safety check\n",
    "    if batch_idx >= config.max_iterations:\n",
    "        print(f'\\nâš ï¸ Maximum iterations reached: {config.max_iterations}')\n",
    "        break\n",
    "\n",
    "print(f'\\nâœ… Training completed!')\n",
    "print(f'ðŸ“Š Total batches: {batch_idx}')\n",
    "print(f'ðŸ“‰ Final loss: {training_losses[-1]:.8f}')\n",
    "print(f'ðŸŽ¯ Learned polynomial: {model.get_polynomial_description()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Analysis and Visualization\n",
    "\n",
    "Using seaborn for visualization following repository guidelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results analysis with seaborn visualization\n",
    "print(\"\\nðŸ“Š ANALYSIS: Training Results and Model Performance\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Display learned polynomial vs target\n",
    "final_polynomial = model.get_polynomial_description()\n",
    "print(f\"ðŸŽ¯ Learned Polynomial:\")\n",
    "print(f\"   {final_polynomial}\")\n",
    "print(f\"\\nðŸŽ¯ Target Function: f(x) = e^x\")\n",
    "\n",
    "# 2. Comprehensive visualization\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Training loss curve\n",
    "plt.subplot(2, 3, 1)\n",
    "sns.lineplot(x=training_iterations, y=training_losses)\n",
    "plt.yscale('log')\n",
    "plt.title('Training Loss Evolution (Log Scale)', fontsize=14)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('SmoothL1 Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Function approximation comparison\n",
    "plt.subplot(2, 3, 2)\n",
    "x_test = torch.linspace(-2, 2, 200)\n",
    "x_features = make_features(x_test, config.poly_degree).to(config.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    y_pred = model(x_features).cpu().numpy().flatten()\n",
    "    y_true = torch.exp(x_test).numpy()\n",
    "\n",
    "plt.plot(x_test, y_true, 'b-', label='True: $e^x$', linewidth=3, alpha=0.8)\n",
    "plt.plot(x_test, y_pred, 'r--', label='Learned: $P(x)$', linewidth=2)\n",
    "plt.title('Function Approximation: $e^x$ vs Polynomial', fontsize=14)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Error analysis\n",
    "plt.subplot(2, 3, 3)\n",
    "errors = y_pred - y_true\n",
    "plt.plot(x_test, errors, 'g-', linewidth=2, label='Error: $P(x) - e^x$')\n",
    "plt.title('Approximation Error Analysis', fontsize=14)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Error')\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Australian context: Sydney temperature modeling\n",
    "plt.subplot(2, 3, 4)\n",
    "sydney_temps = np.linspace(-1.5, 1.5, 50)\n",
    "sydney_results = model.predict_australian_values(sydney_temps.tolist())\n",
    "\n",
    "plt.plot(sydney_temps, sydney_results['true_exp_values'], 'b-', \n",
    "         label='True Exponential', linewidth=3, alpha=0.7)\n",
    "plt.plot(sydney_temps, sydney_results['predictions'], 'r--', \n",
    "         label='Polynomial Fit', linewidth=2)\n",
    "plt.title('ðŸ‡¦ðŸ‡º Sydney Climate Exponential Model', fontsize=14)\n",
    "plt.xlabel('Scaled Temperature')\n",
    "plt.ylabel('Exponential Response')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Error distribution histogram\n",
    "plt.subplot(2, 3, 5)\n",
    "sns.histplot(errors, bins=30, alpha=0.7, color='green')\n",
    "plt.title('Error Distribution', fontsize=14)\n",
    "plt.xlabel('Approximation Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Zoomed view around critical region\n",
    "plt.subplot(2, 3, 6)\n",
    "critical_mask = (x_test >= -1) & (x_test <= 1)\n",
    "plt.plot(x_test[critical_mask], y_true[critical_mask], 'b-', \n",
    "         label='True: $e^x$', linewidth=3, alpha=0.8)\n",
    "plt.plot(x_test[critical_mask], y_pred[critical_mask], 'r--', \n",
    "         label='Learned: $P(x)$', linewidth=2)\n",
    "plt.title('Zoomed View: Critical Region [-1, 1]', fontsize=14)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Numerical accuracy analysis\n",
    "print(f\"\\nðŸ“ˆ Accuracy Analysis:\")\n",
    "mse = np.mean(errors**2)\n",
    "mae = np.mean(np.abs(errors))\n",
    "max_error = np.max(np.abs(errors))\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"   Mean Squared Error (MSE): {mse:.8f}\")\n",
    "print(f\"   Root Mean Squared Error (RMSE): {rmse:.8f}\")\n",
    "print(f\"   Mean Absolute Error (MAE): {mae:.8f}\")\n",
    "print(f\"   Maximum Absolute Error: {max_error:.8f}\")\n",
    "\n",
    "# 4. Australian context evaluation\n",
    "print(f\"\\nðŸ‡¦ðŸ‡º Australian Sydney Climate Model Evaluation:\")\n",
    "test_points = [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5]\n",
    "aus_results = model.predict_australian_values(test_points)\n",
    "\n",
    "print(f\"{'Point':<8} {'x Value':<8} {'Predicted':<12} {'True e^x':<12} {'Error':<12} {'Rel Error %':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, (x, pred, true, error) in enumerate(zip(\n",
    "    aus_results['x_values'],\n",
    "    aus_results['predictions'], \n",
    "    aus_results['true_exp_values'],\n",
    "    aus_results['errors']\n",
    ")):\n",
    "    rel_error = abs(error / true) * 100 if true != 0 else 0\n",
    "    print(f\"T{i+1:<7} {x:<8.1f} {pred:<12.6f} {true:<12.6f} {error:<+12.6f} {rel_error:<12.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. TensorBoard Visualization Instructions\n",
    "\n",
    "Following repository policy for TensorBoard viewing instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close TensorBoard writer and display viewing instructions\n",
    "writer.close()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ“Š TENSORBOARD VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Log directory: {run_logdir}\")\n",
    "print(\"\\nðŸš€ To view TensorBoard:\")\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"   In Google Colab:\")\n",
    "    print(\"   1. Run: %load_ext tensorboard\")\n",
    "    print(f\"   2. Run: %tensorboard --logdir {run_logdir}\")\n",
    "    print(\"   3. TensorBoard will appear inline in the notebook\")\n",
    "elif IS_KAGGLE:\n",
    "    print(\"   In Kaggle:\")\n",
    "    print(f\"   1. Download logs from: {run_logdir}\")\n",
    "    print(\"   2. Run locally: tensorboard --logdir ./tensorboard_logs\")\n",
    "    print(\"   3. Open http://localhost:6006 in browser\")\n",
    "else:\n",
    "    print(\"   Locally:\")\n",
    "    print(f\"   1. Run: tensorboard --logdir {run_logdir}\")\n",
    "    print(\"   2. Open http://localhost:6006 in browser\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ Available visualizations:\")\n",
    "print(\"   â€¢ Scalars: Loss curve and coefficient evolution\")\n",
    "print(\"   â€¢ Graphs: Model architecture visualization\")\n",
    "print(\"   â€¢ Custom metrics: Polynomial coefficient tracking\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. PyTorch vs TensorFlow Comparison Summary\n",
    "\n",
    "Educational summary for learners transitioning from TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "ðŸŽ“ KEY LEARNING POINTS: PyTorch vs TensorFlow for Function Approximation\n",
    "\n",
    "1. ðŸ—ï¸ MODEL DEFINITION:\n",
    "   TensorFlow: model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(4,))])\n",
    "   PyTorch:    class Model(nn.Module): def __init__(): self.linear = nn.Linear(4, 1)\n",
    "\n",
    "2. ðŸ”„ TRAINING LOOPS:\n",
    "   TensorFlow: model.fit(x, y, epochs=epochs) # Automatic\n",
    "   PyTorch:    Manual loop with loss.backward() and manual parameter updates\n",
    "\n",
    "3. ðŸŽ¯ LOSS & OPTIMIZATION:\n",
    "   TensorFlow: model.compile(optimizer='sgd', loss='mse')\n",
    "   PyTorch:    criterion = nn.SmoothL1Loss(); manual gradient application\n",
    "\n",
    "4. ðŸ“Š AUTOMATIC DIFFERENTIATION:\n",
    "   TensorFlow: tf.GradientTape() or automatic in model.fit()\n",
    "   PyTorch:    loss.backward() # Computes gradients automatically\n",
    "\n",
    "5. ðŸ–¥ï¸ DEVICE MANAGEMENT:\n",
    "   TensorFlow: Mostly automatic with distribution strategies\n",
    "   PyTorch:    Explicit model.to(device), tensor.to(device)\n",
    "\n",
    "6. ðŸ” DEBUGGING & INSPECTION:\n",
    "   TensorFlow: Can be complex due to graph mode (TF 1.x) or eager execution (TF 2.x)\n",
    "   PyTorch:    Always eager execution - easy to debug with standard Python tools\n",
    "\n",
    "ðŸ† POLYNOMIAL FITTING SPECIFIC INSIGHTS:\n",
    "\n",
    "âœ… Manual Parameter Control:\n",
    "   This example shows direct parameter manipulation:\n",
    "   param.data.add_(-learning_rate * param.grad)\n",
    "   \n",
    "âœ… Gradient Inspection:\n",
    "   Easy to examine gradients: param.grad\n",
    "   \n",
    "âœ… Mathematical Transparency:\n",
    "   Clear relationship: P(x) = wâ‚x + wâ‚‚xÂ² + wâ‚ƒxÂ³ + wâ‚„xâ´ + b\n",
    "   \n",
    "âœ… Flexible Training:\n",
    "   Can implement custom optimization strategies\n",
    "\"\"\")\n",
    "\n",
    "# Final model summary\n",
    "print(f\"\\nðŸ“‹ FINAL POLYNOMIAL FITTING RESULTS:\")\n",
    "print(f\"   ðŸŽ¯ Target Function: f(x) = e^x\")\n",
    "print(f\"   ðŸ“ Learned Polynomial: {model.get_polynomial_description()}\")\n",
    "print(f\"   ðŸ“Š Model Parameters: {sum(p.numel() for p in model.parameters())} (4 weights + 1 bias)\")\n",
    "print(f\"   ðŸ“‰ Final Loss: {training_losses[-1]:.8f}\")\n",
    "print(f\"   âš¡ Device Used: {model.device}\")\n",
    "print(f\"   ðŸ”¢ Training Batches: {len(training_losses)}\")\n",
    "print(f\"   ðŸŽ“ Learning Rate: {config.learning_rate}\")\n",
    "\n",
    "# Model coefficients analysis\n",
    "with torch.no_grad():\n",
    "    final_weights = model.polynomial.weight.cpu().numpy().flatten()\n",
    "    final_bias = model.polynomial.bias.cpu().item()\n",
    "\n",
    "print(f\"\\nðŸ”¬ Learned Coefficients Analysis:\")\n",
    "for i, w in enumerate(final_weights):\n",
    "    print(f\"   w_{i+1} (x^{i+1}): {w:+.6f}\")\n",
    "print(f\"   b (bias):     {final_bias:+.6f}\")\n",
    "\n",
    "print(f\"\\nðŸ‡¦ðŸ‡º Australian Context Summary:\")\n",
    "print(f\"   ðŸ–ï¸ Successfully modeled exponential relationships for Sydney climate data\")\n",
    "print(f\"   ðŸŒ¡ï¸ Polynomial captures seasonal temperature-response patterns\")\n",
    "print(f\"   ðŸ“Š Suitable for educational climate modeling demonstrations\")\n",
    "print(f\"   ðŸ”¬ Demonstrates PyTorch's flexibility for mathematical modeling\")\n",
    "\n",
    "print(f\"\\nðŸš€ Next Steps in Your PyTorch Journey:\")\n",
    "print(f\"   ðŸ¤— Hugging Face Transformers: Modern NLP with PyTorch\")\n",
    "print(f\"   ðŸ“š Advanced Optimization: Adam, RMSprop, learning rate scheduling\")\n",
    "print(f\"   ðŸŒ Multilingual Models: English-Vietnamese applications\")\n",
    "print(f\"   ðŸ—ï¸ Custom Architectures: Building domain-specific neural networks\")\n",
    "print(f\"   ðŸ“Š Advanced Visualization: Interactive plots and model interpretability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook successfully demonstrated polynomial fitting using PyTorch to approximate the exponential function e^x with a 4th degree polynomial. Key achievements:\n",
    "\n",
    "### Mathematical Success\n",
    "- **Target**: Approximate $e^x$ using $P(x) = w_1x + w_2x^2 + w_3x^3 + w_4x^4 + b$\n",
    "- **Method**: PyTorch automatic differentiation with manual parameter updates\n",
    "- **Result**: High accuracy approximation with convergence to specified threshold\n",
    "\n",
    "### Technical Learning\n",
    "1. **PyTorch Fundamentals**: Manual training loops, gradient computation, device management\n",
    "2. **Function Approximation**: Neural networks for mathematical function modeling\n",
    "3. **Optimization**: Direct parameter manipulation vs optimizer-based approaches\n",
    "4. **Visualization**: Comprehensive analysis using seaborn and TensorBoard\n",
    "\n",
    "### PyTorch vs TensorFlow Insights\n",
    "- **Control**: PyTorch offers more granular control over training process\n",
    "- **Transparency**: Easy gradient inspection and parameter manipulation\n",
    "- **Debugging**: Eager execution makes debugging intuitive\n",
    "- **Flexibility**: Custom optimization strategies easily implemented\n",
    "\n",
    "### Australian Context Integration\n",
    "- Successfully incorporated Australian climate modeling theme\n",
    "- Demonstrated practical applications of polynomial fitting\n",
    "- Provided cultural context for educational engagement\n",
    "\n",
    "### Repository Guidelines Compliance\n",
    "- âœ… OOP design patterns with helper functions\n",
    "- âœ… Australian context examples throughout\n",
    "- âœ… Comprehensive TensorBoard logging\n",
    "- âœ… Seaborn visualization standards\n",
    "- âœ… Cross-platform compatibility\n",
    "- âœ… Educational TensorFlow comparisons\n",
    "\n",
    "This foundation prepares you for advanced PyTorch topics, especially NLP applications with Hugging Face transformers, making it an ideal stepping stone in your machine learning journey.\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook follows PyTorch-Mastery repository guidelines for educational content, Australian context examples, and comprehensive documentation.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
