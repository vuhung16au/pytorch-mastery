{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World: PyTorch CIFAR-10 Image Classification\n",
    "\n",
    "This notebook demonstrates the fundamentals of PyTorch through a simple image classification task using the CIFAR-10 dataset. It's designed as a \"Hello World\" introduction to PyTorch for learners transitioning from TensorFlow.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand PyTorch tensor operations and autograd\n",
    "- Build a basic CNN model using `nn.Module`\n",
    "- Implement manual training loops (vs TensorFlow's `model.fit()`)\n",
    "- Use PyTorch data loaders and transformations\n",
    "- Monitor training with TensorBoard\n",
    "\n",
    "## CIFAR-10 Dataset\n",
    "CIFAR-10 contains 60,000 32x32 color images in 10 classes:\n",
    "- üõ©Ô∏è airplane, üöó automobile, üê¶ bird, üê± cat, ü¶å deer\n",
    "- üê∂ dog, üê∏ frog, üê¥ horse, üö¢ ship, üöõ truck\n",
    "\n",
    "**Dataset Source**: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Runtime Detection\n",
    "\n",
    "Following PyTorch best practices for cross-platform compatibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Detection and Setup\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Detect the runtime environment\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules or \"kaggle\" in os.environ.get('KAGGLE_URL_BASE', '')\n",
    "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
    "\n",
    "print(f\"üåê Environment detected:\")\n",
    "print(f\"  - Local: {IS_LOCAL}\")\n",
    "print(f\"  - Google Colab: {IS_COLAB}\")\n",
    "print(f\"  - Kaggle: {IS_KAGGLE}\")\n",
    "\n",
    "# Platform-specific system setup\n",
    "if IS_COLAB:\n",
    "    print(\"\\nüîß Setting up Google Colab environment...\")\n",
    "    # Colab usually has PyTorch pre-installed\n",
    "elif IS_KAGGLE:\n",
    "    print(\"\\nüîß Setting up Kaggle environment...\")\n",
    "    # Kaggle usually has most packages pre-installed\n",
    "else:\n",
    "    print(\"\\nüîß Setting up local environment...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages based on platform\n",
    "required_packages = [\n",
    "    \"torch\",\n",
    "    \"torchvision\", \n",
    "    \"matplotlib\",\n",
    "    \"tensorboard\"\n",
    "]\n",
    "\n",
    "print(\"üì¶ Installing required packages...\")\n",
    "for package in required_packages:\n",
    "    if IS_COLAB or IS_KAGGLE:\n",
    "        # Use IPython magic commands for notebook environments\n",
    "        try:\n",
    "            exec(f\"!pip install -q {package}\")\n",
    "            print(f\"‚úÖ {package}\")\n",
    "        except:\n",
    "            print(f\"‚ö†Ô∏è {package} (may already be installed)\")\n",
    "    else:\n",
    "        try:\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
    "                          capture_output=True, check=True)\n",
    "            print(f\"‚úÖ {package}\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"‚ö†Ô∏è {package} (may already be installed)\")\n",
    "\n",
    "print(\"\\nüéâ Package installation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify PyTorch installation and import core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "\n",
    "print(f\"üî• PyTorch {torch.__version__} ready!\")\n",
    "print(f\"üñ•Ô∏è CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"üéØ Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# Set device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nüíæ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TensorBoard Setup for Training Monitoring\n",
    "\n",
    "PyTorch requires explicit TensorBoard setup (unlike TensorFlow's integrated callbacks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platform-specific TensorBoard log directory setup\n",
    "def get_run_logdir(name=\"cifar10_training\"):\n",
    "    \"\"\"Create unique log directory for this training run.\"\"\"\n",
    "    if IS_COLAB:\n",
    "        root_logdir = \"/content/tensorboard_logs\"\n",
    "    elif IS_KAGGLE:\n",
    "        root_logdir = \"./tensorboard_logs\"\n",
    "    else:\n",
    "        root_logdir = \"./tensorboard_logs\"\n",
    "    \n",
    "    # Create root directory if it doesn't exist\n",
    "    os.makedirs(root_logdir, exist_ok=True)\n",
    "    \n",
    "    # Generate unique run directory\n",
    "    timestamp = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "    run_logdir = os.path.join(root_logdir, f\"{name}_{timestamp}\")\n",
    "    return run_logdir\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "log_dir = get_run_logdir(\"hello_pytorch_cifar\")\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "print(f\"üìä TensorBoard logs will be saved to: {log_dir}\")\n",
    "print(f\"üí° To view logs, run: tensorboard --logdir={log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing\n",
    "\n",
    "**Key Difference from TensorFlow**: PyTorch uses explicit transforms and DataLoader objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformations for training and testing\n",
    "# TensorFlow equivalent: tf.keras.preprocessing.image.ImageDataGenerator\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Data augmentation\n",
    "    transforms.ToTensor(),                   # Convert PIL Image to tensor\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # CIFAR-10 statistics\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Download and load CIFAR-10 dataset\n",
    "print(\"üì• Loading CIFAR-10 dataset...\")\n",
    "\n",
    "try:\n",
    "    # Try to download CIFAR-10 dataset (requires internet)\n",
    "    print(\"üåê Attempting to download CIFAR-10 dataset...\")\n",
    "    \n",
    "    # Training dataset\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', \n",
    "        train=True, \n",
    "        download=True, \n",
    "        transform=transform_train\n",
    "    )\n",
    "    \n",
    "    # Test dataset\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', \n",
    "        train=False, \n",
    "        download=True, \n",
    "        transform=transform_test\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ CIFAR-10 dataset downloaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Cannot download CIFAR-10 dataset: {type(e).__name__}\")\n",
    "    print(\"üîÑ Creating synthetic dataset for demonstration purposes...\")\n",
    "    \n",
    "    # Create synthetic CIFAR-10-like dataset for demo when offline\n",
    "    from torch.utils.data import TensorDataset\n",
    "    \n",
    "    # Generate synthetic data: 32x32 RGB images\n",
    "    synthetic_train_images = torch.randn(1000, 3, 32, 32)  # 1000 training samples\n",
    "    synthetic_train_labels = torch.randint(0, 10, (1000,))  # Random labels 0-9\n",
    "    \n",
    "    synthetic_test_images = torch.randn(200, 3, 32, 32)   # 200 test samples  \n",
    "    synthetic_test_labels = torch.randint(0, 10, (200,))   # Random labels 0-9\n",
    "    \n",
    "    # Apply transformations manually to synthetic data\n",
    "    # Normalize using CIFAR-10 statistics\n",
    "    mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)\n",
    "    \n",
    "    synthetic_train_images = (synthetic_train_images - mean) / std\n",
    "    synthetic_test_images = (synthetic_test_images - mean) / std\n",
    "    \n",
    "    # Create tensor datasets\n",
    "    trainset = TensorDataset(synthetic_train_images, synthetic_train_labels)\n",
    "    testset = TensorDataset(synthetic_test_images, synthetic_test_labels)\n",
    "    \n",
    "    print(\"‚úÖ Synthetic dataset created for demonstration!\")\n",
    "    print(\"üìù Note: This is random data, not real CIFAR-10 images\")\n",
    "\n",
    "# Data loaders (equivalent to TensorFlow's tf.data.Dataset)\n",
    "batch_size = 32  # Small batch size for educational purposes\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    trainset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,      # Shuffle training data\n",
    "    num_workers=0      # Use 0 for synthetic data to avoid pickling issues\n",
    ")\n",
    "\n",
    "testloader = DataLoader(\n",
    "    testset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,     # Don't shuffle test data\n",
    "    num_workers=0      # Use 0 for synthetic data to avoid pickling issues\n",
    ")\n",
    "\n",
    "# CIFAR-10 class names\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"üìä Training samples: {len(trainset)}\")\n",
    "print(f\"üìä Test samples: {len(testset)}\")\n",
    "print(f\"üè∑Ô∏è Classes: {classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Visualization\n",
    "\n",
    "Let's visualize some sample images from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display images\n",
    "def imshow(img, title=None):\n",
    "    \"\"\"Display a tensor image.\"\"\"\n",
    "    # Handle both normalized and unnormalized images\n",
    "    if img.min() < 0:  # If normalized, unnormalize\n",
    "        img = img / 2 + 0.5\n",
    "    \n",
    "    # Clamp values to [0, 1] range\n",
    "    img = torch.clamp(img, 0, 1)\n",
    "    \n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "# Get a batch of training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Create a grid of sample images\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(8):  # Show first 8 images\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    imshow(images[i], title=f'{classes[labels[i]]}')\n",
    "\n",
    "plt.suptitle('üñºÔ∏è Sample Images from Training Set (CIFAR-10 or Synthetic)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üéØ Batch shape: {images.shape} (batch_size, channels, height, width)\")\n",
    "print(f\"üè∑Ô∏è Labels shape: {labels.shape}\")\n",
    "print(f\"üìã Sample labels: {[classes[label] for label in labels[:8]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Definition - CNN Architecture\n",
    "\n",
    "**PyTorch vs TensorFlow Model Definition**:\n",
    "- **PyTorch**: Explicit `nn.Module` subclass with `__init__` and `forward` methods\n",
    "- **TensorFlow**: `tf.keras.Sequential` or Functional API\n",
    "\n",
    "Our simple CNN architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCIFAR10CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple CNN for CIFAR-10 classification - Hello World PyTorch model.\n",
    "    \n",
    "    Architecture:\n",
    "    - 2 Convolutional blocks (Conv2d + ReLU + MaxPool)\n",
    "    - 2 Fully connected layers with dropout\n",
    "    - Output: 10 classes (CIFAR-10)\n",
    "    \n",
    "    TensorFlow equivalent:\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(10)\n",
    "    ])\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCIFAR10CNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)   # 3 input channels (RGB)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # 32 -> 64 channels\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Reduce spatial dimensions by half\n",
    "        \n",
    "        # Calculate size after convolutions: 32x32 -> 16x16 -> 8x8\n",
    "        # Final feature map: 64 channels * 8 * 8 = 4096\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 64)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(64, num_classes)  # Output layer\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input batch of images [batch_size, 3, 32, 32]\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Class logits [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        # Convolutional block 1: Conv -> ReLU -> Pool\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # [batch, 32, 16, 16]\n",
    "        \n",
    "        # Convolutional block 2: Conv -> ReLU -> Pool  \n",
    "        x = self.pool(F.relu(self.conv2(x)))  # [batch, 64, 8, 8]\n",
    "        \n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(-1, 64 * 8 * 8)            # [batch, 4096]\n",
    "        \n",
    "        # Fully connected layers with dropout\n",
    "        x = F.relu(self.fc1(x))               # [batch, 64]\n",
    "        x = self.dropout(x)                   # Apply dropout during training\n",
    "        x = self.fc2(x)                       # [batch, 10] - final logits\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Return model architecture information.\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        return {\n",
    "            'model_name': 'SimpleCIFAR10CNN',\n",
    "            'total_parameters': total_params,\n",
    "            'trainable_parameters': trainable_params,\n",
    "            'architecture': 'Conv2d(32) -> Conv2d(64) -> FC(64) -> FC(10)'\n",
    "        }\n",
    "\n",
    "# Create model instance and move to device\n",
    "model = SimpleCIFAR10CNN(num_classes=10).to(device)\n",
    "\n",
    "# Display model information\n",
    "model_info = model.get_model_info()\n",
    "print(f\"üß† Model: {model_info['model_name']}\")\n",
    "print(f\"üî¢ Total parameters: {model_info['total_parameters']:,}\")\n",
    "print(f\"üéØ Trainable parameters: {model_info['trainable_parameters']:,}\")\n",
    "print(f\"üèóÔ∏è Architecture: {model_info['architecture']}\")\n",
    "print(f\"\\nüìã Model summary:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loss Function and Optimizer Setup\n",
    "\n",
    "**Key PyTorch Pattern**: Explicit loss and optimizer definition (vs TensorFlow's `model.compile()`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer setup\n",
    "# TensorFlow equivalent: model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
    "\n",
    "# Optional: Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "print(f\"üéØ Loss function: {criterion}\")\n",
    "print(f\"üîß Optimizer: {optimizer}\")\n",
    "print(f\"üìâ Learning rate scheduler: Step LR (decay by 0.5 every 5 epochs)\")\n",
    "print(f\"üìä Initial learning rate: {optimizer.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop Implementation\n",
    "\n",
    "**Major Difference from TensorFlow**: PyTorch requires manual training loops instead of `model.fit()`.\n",
    "\n",
    "This gives more control but requires more code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, testloader, criterion, optimizer, scheduler, \n",
    "                num_epochs=5, device=device):\n",
    "    \"\"\"\n",
    "    Training function with TensorBoard logging.\n",
    "    \n",
    "    TensorFlow equivalent:\n",
    "    model.fit(x_train, y_train, epochs=num_epochs, validation_data=(x_test, y_test))\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to train\n",
    "        trainloader: Training data loader\n",
    "        testloader: Test data loader\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        scheduler: Learning rate scheduler\n",
    "        num_epochs: Number of training epochs\n",
    "        device: Device to train on (CPU/GPU)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üöÄ Starting training for {num_epochs} epochs...\")\n",
    "    print(f\"üíæ Device: {device}\")\n",
    "    print(f\"üî¢ Batch size: {batch_size}\")\n",
    "    print(f\"üìä Training batches per epoch: {len(trainloader)}\")\n",
    "    print(f\"üìä Test batches: {len(testloader)}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()  # Set model to training mode (enables dropout, batch norm)\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "            # Move data to device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero gradients (PyTorch accumulates gradients by default)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()    # Compute gradients\n",
    "            optimizer.step()   # Update weights\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Log batch-level metrics to TensorBoard\n",
    "            if batch_idx % 200 == 0:  # Log every 200 batches\n",
    "                writer.add_scalar('Loss/Train_Batch', loss.item(), \n",
    "                                epoch * len(trainloader) + batch_idx)\n",
    "        \n",
    "        # Calculate epoch training metrics\n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "        epoch_acc = 100 * correct_train / total_train\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluation mode (disables dropout)\n",
    "        test_loss = 0.0\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "            for inputs, labels in testloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_test += labels.size(0)\n",
    "                correct_test += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate test metrics\n",
    "        test_loss = test_loss / len(testloader)\n",
    "        test_acc = 100 * correct_test / total_test\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Log epoch-level metrics to TensorBoard\n",
    "        writer.add_scalar('Loss/Train_Epoch', epoch_loss, epoch)\n",
    "        writer.add_scalar('Loss/Test', test_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Train', epoch_acc, epoch)\n",
    "        writer.add_scalar('Accuracy/Test', test_acc, epoch)\n",
    "        writer.add_scalar('Learning_Rate', current_lr, epoch)\n",
    "        \n",
    "        # Log model parameters histogram\n",
    "        for name, param in model.named_parameters():\n",
    "            writer.add_histogram(f'Parameters/{name}', param, epoch)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}]:')\n",
    "        print(f'  Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%')\n",
    "        print(f'  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "        print(f'  Learning Rate: {current_lr:.6f}')\n",
    "        \n",
    "        # Early stopping for demo (if we reach good accuracy)\n",
    "        if test_acc > 65.0:  # CIFAR-10 is challenging, 65% is decent for a simple model\n",
    "            print(f\"\\nüéâ Early stopping! Achieved {test_acc:.2f}% test accuracy.\")\n",
    "            break\n",
    "    \n",
    "    print(\"\\n‚úÖ Training completed!\")\n",
    "    return model\n",
    "\n",
    "# Train the model (using small number of epochs for demo)\n",
    "num_epochs = 8  # Small number for educational purposes\n",
    "trained_model = train_model(\n",
    "    model, trainloader, testloader, criterion, optimizer, scheduler, \n",
    "    num_epochs=num_epochs, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation and Testing\n",
    "\n",
    "Let's evaluate our trained model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "def evaluate_model(model, testloader, classes, device=device):\n",
    "    \"\"\"\n",
    "    Evaluate the trained model and show per-class accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = list(0. for i in range(len(classes)))\n",
    "    class_total = list(0. for i in range(len(classes)))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Per-class accuracy\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "    \n",
    "    # Overall accuracy\n",
    "    overall_accuracy = 100 * correct / total\n",
    "    print(f\"üéØ Overall Test Accuracy: {overall_accuracy:.2f}%\")\n",
    "    print(\"\\nüìä Per-class accuracy:\")\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    for i in range(len(classes)):\n",
    "        if class_total[i] > 0:\n",
    "            accuracy = 100 * class_correct[i] / class_total[i]\n",
    "            print(f\"  {classes[i]:>12}: {accuracy:.1f}%\")\n",
    "    \n",
    "    return overall_accuracy\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"üîç Evaluating trained model...\")\n",
    "final_accuracy = evaluate_model(trained_model, testloader, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Predictions on Sample Images\n",
    "\n",
    "Let's see how our model performs on some test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on sample test images\n",
    "def predict_samples(model, testloader, classes, num_samples=8, device=device):\n",
    "    \"\"\"\n",
    "    Show predictions on sample test images.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of test images\n",
    "    dataiter = iter(testloader)\n",
    "    images, labels = next(dataiter)\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(2, 4, i + 1)\n",
    "        \n",
    "        # Move image back to CPU for plotting\n",
    "        img = images[i].cpu()\n",
    "        imshow(img)\n",
    "        \n",
    "        # Get prediction info\n",
    "        true_label = classes[labels[i]]\n",
    "        pred_label = classes[predicted[i]]\n",
    "        confidence = probabilities[i][predicted[i]].item() * 100\n",
    "        \n",
    "        # Color: green if correct, red if wrong\n",
    "        color = 'green' if predicted[i] == labels[i] else 'red'\n",
    "        \n",
    "        plt.title(f'True: {true_label}\\nPred: {pred_label} ({confidence:.1f}%)', \n",
    "                 color=color, fontsize=10)\n",
    "    \n",
    "    plt.suptitle('üîÆ Model Predictions on Test Images', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate accuracy for this batch\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    batch_accuracy = 100 * correct / len(labels)\n",
    "    print(f\"üìä Batch accuracy: {batch_accuracy:.1f}% ({correct}/{len(labels)} correct)\")\n",
    "\n",
    "# Show sample predictions\n",
    "predict_samples(trained_model, testloader, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. TensorBoard Visualization Instructions\n",
    "\n",
    "View your training progress with TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close TensorBoard writer\n",
    "writer.close()\n",
    "\n",
    "# Display TensorBoard viewing instructions\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä TENSORBOARD VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Log directory: {log_dir}\")\n",
    "print(\"\\nüöÄ To view TensorBoard:\")\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"   In Google Colab:\")\n",
    "    print(\"   1. Run: %load_ext tensorboard\")\n",
    "    print(f\"   2. Run: %tensorboard --logdir {log_dir}\")\n",
    "    print(\"   3. TensorBoard will appear inline in the notebook\")\n",
    "elif IS_KAGGLE:\n",
    "    print(\"   In Kaggle:\")\n",
    "    print(f\"   1. Download logs from: {log_dir}\")\n",
    "    print(\"   2. Run locally: tensorboard --logdir ./tensorboard_logs\")\n",
    "    print(\"   3. Open http://localhost:6006 in browser\")\n",
    "else:\n",
    "    print(\"   Locally:\")\n",
    "    print(f\"   1. Run: tensorboard --logdir {log_dir}\")\n",
    "    print(\"   2. Open http://localhost:6006 in browser\")\n",
    "\n",
    "print(\"\\nüìà Available visualizations:\")\n",
    "print(\"   ‚Ä¢ Scalars: Loss, accuracy, learning rate over time\")\n",
    "print(\"   ‚Ä¢ Histograms: Model parameter distributions\")\n",
    "print(\"   ‚Ä¢ Graphs: Model architecture visualization\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key Learning Points: TensorFlow vs PyTorch\n",
    "\n",
    "Summary of key differences encountered in this \"Hello World\" example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "üéì KEY LEARNING POINTS: TensorFlow ‚Üí PyTorch Transition\n",
    "\n",
    "1. üèóÔ∏è MODEL DEFINITION:\n",
    "   TensorFlow: tf.keras.Sequential() or Functional API\n",
    "   PyTorch:    nn.Module subclass with __init__ and forward methods\n",
    "\n",
    "2. üîÑ TRAINING LOOPS:\n",
    "   TensorFlow: model.fit() handles everything automatically\n",
    "   PyTorch:    Manual loops with optimizer.zero_grad(), loss.backward(), optimizer.step()\n",
    "\n",
    "3. üìä DATA LOADING:\n",
    "   TensorFlow: tf.data.Dataset with built-in batching\n",
    "   PyTorch:    DataLoader with explicit dataset and transforms\n",
    "\n",
    "4. üéØ LOSS & OPTIMIZATION:\n",
    "   TensorFlow: model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "   PyTorch:    Explicit criterion = nn.CrossEntropyLoss() and optimizer = optim.Adam()\n",
    "\n",
    "5. üíæ DEVICE MANAGEMENT:\n",
    "   TensorFlow: Mostly automatic with tf.distribute.Strategy\n",
    "   PyTorch:    Explicit .to(device) calls for model and data\n",
    "\n",
    "6. üé≠ TRAINING/INFERENCE MODES:\n",
    "   TensorFlow: Implicit (training=True/False parameter)\n",
    "   PyTorch:    Explicit model.train() and model.eval() calls\n",
    "\n",
    "7. üìà MONITORING:\n",
    "   TensorFlow: Built-in callbacks and metrics\n",
    "   PyTorch:    Manual TensorBoard logging with SummaryWriter\n",
    "\n",
    "üöÄ ADVANTAGES OF PYTORCH:\n",
    "   ‚úÖ More explicit control over training process\n",
    "   ‚úÖ Dynamic computation graphs (easier debugging)\n",
    "   ‚úÖ Pythonic and intuitive API\n",
    "   ‚úÖ Better integration with Hugging Face transformers\n",
    "   ‚úÖ Immediate execution (no session.run())\n",
    "\n",
    "üéØ NEXT STEPS:\n",
    "   1. Practice with more complex architectures\n",
    "   2. Explore Hugging Face transformers\n",
    "   3. Learn custom dataset creation\n",
    "   4. Study advanced PyTorch features (autograd, hooks, etc.)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Saving (Optional)\n",
    "\n",
    "Save your trained model for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_save_path = './hello_pytorch_cifar10_model.pth'\n",
    "\n",
    "# PyTorch way: Save state dictionary\n",
    "torch.save(trained_model.state_dict(), model_save_path)\n",
    "print(f\"üíæ Model saved to: {model_save_path}\")\n",
    "\n",
    "# To load the model later:\n",
    "# model = SimpleCIFAR10CNN()\n",
    "# model.load_state_dict(torch.load(model_save_path))\n",
    "# model.eval()\n",
    "\n",
    "print(f\"\\nüéâ Hello PyTorch CIFAR-10 tutorial completed!\")\n",
    "print(f\"üìä Final test accuracy: {final_accuracy:.2f}%\")\n",
    "print(f\"üß† Model parameters: {model.get_model_info()['total_parameters']:,}\")\n",
    "print(f\"‚è±Ô∏è Training epochs: {num_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Congratulations!\n",
    "\n",
    "You've successfully completed your first PyTorch \"Hello World\" with CIFAR-10! You've learned:\n",
    "\n",
    "‚úÖ **PyTorch Fundamentals**: Tensors, autograd, and nn.Module  \n",
    "‚úÖ **Model Definition**: Creating CNN architectures with PyTorch  \n",
    "‚úÖ **Data Handling**: DataLoaders and transforms  \n",
    "‚úÖ **Training Loops**: Manual training vs TensorFlow's model.fit()  \n",
    "‚úÖ **Monitoring**: TensorBoard integration for PyTorch  \n",
    "‚úÖ **Model Evaluation**: Testing and visualization  \n",
    "\n",
    "### üöÄ Next Steps in Your PyTorch Journey:\n",
    "\n",
    "1. **Advanced CNN Architectures**: ResNet, VGG, EfficientNet\n",
    "2. **Transfer Learning**: Using pre-trained models\n",
    "3. **Hugging Face Integration**: Modern NLP with transformers\n",
    "4. **Custom Datasets**: Working with your own data\n",
    "5. **Advanced Training**: Mixed precision, distributed training\n",
    "\n",
    "**Happy learning with PyTorch!** üî•\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook demonstrates fundamental PyTorch concepts through a practical CIFAR-10 classification example. For more advanced tutorials and examples, explore the other notebooks in this repository.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}