{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualizing Gradients in PyTorch: Australian Tourism NLP Analysis ðŸ‡¦ðŸ‡ºðŸ“Š\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/pytorch-mastery/blob/main/examples/pytorch-nlp/visualize-gradients.ipynb)\n",
        "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/pytorch-mastery/blob/main/examples/pytorch-nlp/visualize-gradients.ipynb)\n",
        "\n",
        "Learn how to **visualize and understand gradients** in PyTorch neural networks using Australian tourism sentiment analysis. This notebook demonstrates various gradient visualization techniques including heatmaps, flow diagrams, and interactive analysis tools.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "- ðŸ” **Understand gradient flow** in neural networks and why visualization matters\n",
        "- ðŸ“Š **Visualize gradient magnitudes** using heatmaps and statistical plots\n",
        "- ðŸŒŠ **Track gradient flow** through different network layers during training\n",
        "- ðŸŽ¯ **Monitor gradient health** to detect vanishing/exploding gradient problems\n",
        "- ðŸ“ˆ **Use TensorBoard** for interactive gradient monitoring and analysis\n",
        "- ðŸ‡¦ðŸ‡º **Apply techniques** to Australian tourism sentiment analysis with multilingual support\n",
        "- ðŸ”§ **Debug training issues** using gradient visualization insights\n",
        "\n",
        "## What You'll Build\n",
        "\n",
        "1. **Australian Tourism Sentiment Classifier** - Neural network for analyzing tourism reviews\n",
        "2. **Gradient Heatmap Visualizer** - Color-coded visualization of gradient magnitudes\n",
        "3. **Layer-wise Gradient Tracker** - Monitor gradient flow through network layers\n",
        "4. **Training Diagnostics Dashboard** - Real-time gradient health monitoring\n",
        "5. **Interactive Gradient Explorer** - Tools for detailed gradient analysis\n",
        "\n",
        "## Australian Context Examples\n",
        "\n",
        "We'll analyze gradients from models processing:\n",
        "- ðŸ›ï¸ Sydney Opera House and Harbour Bridge tourism reviews\n",
        "- â˜• Melbourne coffee culture and restaurant sentiment\n",
        "- ðŸ–ï¸ Gold Coast beach and tourism feedback  \n",
        "- ðŸŒ English-Vietnamese multilingual tourism content\n",
        "- ðŸ¦˜ Australian wildlife and nature tourism descriptions\n",
        "\n",
        "**Key Concepts**: Gradient flow, backpropagation visualization, training diagnostics, gradient clipping, optimizer behavior analysis\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup and Runtime Detection ðŸ”§\n",
        "\n",
        "Following PyTorch best practices for cross-platform compatibility:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment Detection and Setup\n",
        "import sys\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Detect the runtime environment\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules or \"kaggle\" in os.environ.get('KAGGLE_URL_BASE', '')\n",
        "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
        "\n",
        "print(f\"ðŸ” Environment detected:\")\n",
        "print(f\"  - Local: {IS_LOCAL}\")\n",
        "print(f\"  - Google Colab: {IS_COLAB}\")\n",
        "print(f\"  - Kaggle: {IS_KAGGLE}\")\n",
        "\n",
        "# Platform-specific system setup\n",
        "if IS_COLAB:\n",
        "    print(\"\\nðŸš€ Setting up Google Colab environment...\")\n",
        "    !apt update -qq\n",
        "    !apt install -y -qq software-properties-common\n",
        "elif IS_KAGGLE:\n",
        "    print(\"\\nðŸš€ Setting up Kaggle environment...\")\n",
        "    # Kaggle usually has most packages pre-installed\n",
        "else:\n",
        "    print(\"\\nðŸš€ Setting up local environment...\")\n",
        "\n",
        "print(\"\\nðŸ“¦ Installing required packages...\")\n",
        "\n",
        "# Core packages\n",
        "required_packages = [\n",
        "    \"torch\",\n",
        "    \"torchvision\", \n",
        "    \"matplotlib\",\n",
        "    \"seaborn\",\n",
        "    \"pandas\",\n",
        "    \"numpy\",\n",
        "    \"tensorboard\"\n",
        "]\n",
        "\n",
        "# Install packages based on environment\n",
        "for package in required_packages:\n",
        "    try:\n",
        "        if IS_COLAB:\n",
        "            !pip install -q {package}\n",
        "        elif IS_KAGGLE:\n",
        "            !pip install -q {package}\n",
        "        else:\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
        "                          capture_output=True, check=True)\n",
        "        print(f\"âœ… {package}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  {package}: {str(e)}\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ Package installation completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports and device detection\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Visualization and data processing\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import platform\n",
        "import tempfile\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "# Set seaborn style for better notebook aesthetics (per repository guidelines)\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Device detection helper function (following repository standards)\n",
        "def detect_device() -> Tuple[torch.device, str]:\n",
        "    \"\"\"\n",
        "    Helper function to detect the best available PyTorch device.\n",
        "    \n",
        "    Priority order:\n",
        "    1. CUDA (NVIDIA GPUs) - Best performance for deep learning\n",
        "    2. MPS (Apple Silicon) - Optimized for M1/M2/M3 Macs  \n",
        "    3. CPU (Universal) - Always available fallback\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (device, description) for optimal performance\n",
        "    \"\"\"\n",
        "    # Check for CUDA (NVIDIA GPU)\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        device_info = f\"CUDA GPU: {gpu_name}\"\n",
        "        \n",
        "        print(f\"ðŸš€ Using CUDA acceleration\")\n",
        "        print(f\"   GPU: {gpu_name}\")\n",
        "        print(f\"   CUDA Version: {torch.version.cuda}\")\n",
        "        print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "        \n",
        "        return device, device_info\n",
        "    \n",
        "    # Check for MPS (Apple Silicon)\n",
        "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "        device_info = \"Apple Silicon MPS\"\n",
        "        \n",
        "        system_info = platform.uname()\n",
        "        print(f\"ðŸŽ Using Apple Silicon MPS acceleration\")\n",
        "        print(f\"   System: {system_info.system} {system_info.release}\")\n",
        "        print(f\"   Machine: {system_info.machine}\")\n",
        "        \n",
        "        return device, device_info\n",
        "    \n",
        "    # Fallback to CPU\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        device_info = \"CPU (No GPU acceleration available)\"\n",
        "        \n",
        "        cpu_count = torch.get_num_threads()\n",
        "        system_info = platform.uname()\n",
        "        \n",
        "        print(f\"ðŸ’» Using CPU (no GPU acceleration detected)\")\n",
        "        print(f\"   Processor: {system_info.processor}\")\n",
        "        print(f\"   PyTorch Threads: {cpu_count}\")\n",
        "        print(f\"   System: {system_info.system} {system_info.release}\")\n",
        "        \n",
        "        print(f\"\\nðŸ’¡ CPU Optimization Tips:\")\n",
        "        print(f\"   â€¢ Reduce batch size to prevent memory issues\")\n",
        "        print(f\"   â€¢ Consider using smaller models for faster training\")\n",
        "        print(f\"   â€¢ Enable PyTorch optimizations: torch.set_num_threads({cpu_count})\")\n",
        "        \n",
        "        return device, device_info\n",
        "\n",
        "# Detect and configure device\n",
        "device, device_info = detect_device()\n",
        "print(f\"\\nâœ… PyTorch device selected: {device}\")\n",
        "print(f\"ðŸ“Š Device info: {device_info}\")\n",
        "\n",
        "# Verify PyTorch functionality\n",
        "print(f\"\\nðŸ§ª PyTorch version: {torch.__version__}\")\n",
        "print(f\"ðŸ”§ Environment ready for gradient visualization!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. TensorBoard Setup for Gradient Monitoring ðŸ“Š\n",
        "\n",
        "Configure TensorBoard logging with platform-specific directories following repository standards:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TensorBoard Setup with Platform-Specific Configuration\n",
        "def get_run_logdir(base_name: str = \"gradient_visualization\") -> str:\n",
        "    \"\"\"\n",
        "    Helper function to create platform-specific TensorBoard log directories.\n",
        "    \n",
        "    Args:\n",
        "        base_name: Base name for the log directory\n",
        "        \n",
        "    Returns:\n",
        "        Path to the created log directory\n",
        "    \"\"\"\n",
        "    import time\n",
        "    from datetime import datetime\n",
        "    \n",
        "    # Platform-specific root directories\n",
        "    if IS_COLAB:\n",
        "        root_logdir = \"/content/tensorboard_logs\"\n",
        "    elif IS_KAGGLE:\n",
        "        root_logdir = \"./tensorboard_logs\"\n",
        "    else:\n",
        "        root_logdir = \"./tensorboard_logs\"\n",
        "    \n",
        "    # Create unique run directory with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
        "    run_logdir = f\"{root_logdir}/{base_name}_{timestamp}\"\n",
        "    \n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(run_logdir, exist_ok=True)\n",
        "    \n",
        "    return run_logdir\n",
        "\n",
        "# Create log directory for this session\n",
        "log_dir = get_run_logdir(\"australian_tourism_gradients\")\n",
        "print(f\"ðŸ“Š TensorBoard log directory: {log_dir}\")\n",
        "\n",
        "# Platform-specific TensorBoard viewing instructions\n",
        "print(\"\\nðŸš€ TensorBoard Viewing Instructions:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if IS_COLAB:\n",
        "    print(\"ðŸ“± Google Colab:\")\n",
        "    print(\"   1. Run: %load_ext tensorboard\")\n",
        "    print(f\"   2. Run: %tensorboard --logdir {log_dir}\")\n",
        "    print(\"   3. TensorBoard will appear inline in the notebook\")\n",
        "elif IS_KAGGLE:\n",
        "    print(\"ðŸ† Kaggle:\")\n",
        "    print(f\"   1. Download logs from: {log_dir}\")\n",
        "    print(\"   2. Run locally: tensorboard --logdir ./tensorboard_logs\")\n",
        "    print(\"   3. Open http://localhost:6006 in browser\")\n",
        "else:\n",
        "    print(\"ðŸ–¥ï¸  Local:\")\n",
        "    print(f\"   1. Run: tensorboard --logdir {log_dir}\")\n",
        "    print(\"   2. Open http://localhost:6006 in browser\")\n",
        "\n",
        "print(\"\\nðŸ“ˆ Available visualizations:\")\n",
        "print(\"   â€¢ Scalars: Gradient norms, layer statistics\")\n",
        "print(\"   â€¢ Histograms: Gradient distributions per layer\")\n",
        "print(\"   â€¢ Images: Gradient heatmaps and flow diagrams\")\n",
        "print(\"   â€¢ Custom: Australian tourism analysis metrics\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Australian Tourism Sentiment Model Architecture ðŸ›ï¸\n",
        "\n",
        "Create a neural network for Australian tourism sentiment analysis with gradient monitoring capabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Australian Tourism Sentiment Analysis Model with Gradient Monitoring\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"Configuration class for the Australian tourism sentiment model.\"\"\"\n",
        "    vocab_size: int = 10000\n",
        "    embed_dim: int = 128\n",
        "    hidden_dim: int = 256\n",
        "    output_dim: int = 3  # positive, negative, neutral\n",
        "    num_layers: int = 2\n",
        "    dropout_rate: float = 0.1\n",
        "    max_sequence_length: int = 100\n",
        "    device: Optional[torch.device] = None\n",
        "    \n",
        "    def __post_init__(self):\n",
        "        if self.device is None:\n",
        "            self.device = detect_device()[0]\n",
        "\n",
        "class AustralianTourismSentimentModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network for Australian tourism sentiment analysis with gradient monitoring.\n",
        "    \n",
        "    This model is designed to process reviews about Australian tourism locations\n",
        "    including Sydney Opera House, Melbourne coffee culture, Gold Coast beaches, etc.\n",
        "    \n",
        "    Features gradient hooks for comprehensive visualization and monitoring.\n",
        "    \n",
        "    Architecture:\n",
        "    - Embedding layer for text tokenization\n",
        "    - LSTM layers for sequence processing\n",
        "    - Fully connected layers for classification\n",
        "    - Dropout for regularization\n",
        "    \n",
        "    Example Australian tourism texts:\n",
        "    - \"Sydney Opera House is absolutely breathtaking!\" (positive)\n",
        "    - \"Melbourne coffee is overpriced and disappointing\" (negative)\n",
        "    - \"Gold Coast beaches are okay, nothing special\" (neutral)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super(AustralianTourismSentimentModel, self).__init__()\n",
        "        \n",
        "        self.config = config\n",
        "        self.device = config.device\n",
        "        \n",
        "        # Layer definitions with gradient monitoring capabilities\n",
        "        self.embedding = nn.Embedding(config.vocab_size, config.embed_dim)\n",
        "        \n",
        "        self.lstm = nn.LSTM(\n",
        "            config.embed_dim, \n",
        "            config.hidden_dim, \n",
        "            config.num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=config.dropout_rate if config.num_layers > 1 else 0,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        \n",
        "        # Fully connected layers for classification\n",
        "        lstm_output_size = config.hidden_dim * 2  # bidirectional\n",
        "        self.fc1 = nn.Linear(lstm_output_size, config.hidden_dim)\n",
        "        self.dropout1 = nn.Dropout(config.dropout_rate)\n",
        "        self.fc2 = nn.Linear(config.hidden_dim, config.hidden_dim // 2)\n",
        "        self.dropout2 = nn.Dropout(config.dropout_rate)\n",
        "        self.fc3 = nn.Linear(config.hidden_dim // 2, config.output_dim)\n",
        "        \n",
        "        # Gradient storage for visualization\n",
        "        self.gradients = {}\n",
        "        self.activations = {}\n",
        "        self.gradient_hooks = []\n",
        "        \n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "        \n",
        "        # Move to device\n",
        "        self.to(self.device)\n",
        "        \n",
        "        print(f\"ðŸ›ï¸ Australian Tourism Sentiment Model initialized:\")\n",
        "        print(f\"   ðŸ“Š Parameters: {self.count_parameters():,}\")\n",
        "        print(f\"   ðŸŽ¯ Classes: {config.output_dim} (positive, negative, neutral)\")\n",
        "        print(f\"   ðŸŒ Context: Australian tourism and multilingual support\")\n",
        "        print(f\"   ðŸ“± Device: {self.device}\")\n",
        "    \n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize model weights using Xavier initialization.\"\"\"\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'weight' in name and len(param.shape) > 1:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "    \n",
        "    def count_parameters(self) -> int:\n",
        "        \"\"\"Count total trainable parameters.\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "    \n",
        "    def register_gradient_hooks(self):\n",
        "        \"\"\"\n",
        "        Register gradient hooks for visualization.\n",
        "        \n",
        "        Hooks capture gradients flowing backward through each layer,\n",
        "        enabling comprehensive gradient analysis and visualization.\n",
        "        \"\"\"\n",
        "        def create_hook(name):\n",
        "            def hook_fn(grad):\n",
        "                # Store gradient statistics for visualization\n",
        "                self.gradients[name] = {\n",
        "                    'grad': grad.clone(),\n",
        "                    'norm': grad.norm().item(),\n",
        "                    'mean': grad.mean().item(),\n",
        "                    'std': grad.std().item(),\n",
        "                    'min': grad.min().item(),\n",
        "                    'max': grad.max().item()\n",
        "                }\n",
        "                return grad\n",
        "            return hook_fn\n",
        "        \n",
        "        # Register hooks for key layers\n",
        "        layer_names = ['embedding', 'lstm', 'fc1', 'fc2', 'fc3']\n",
        "        \n",
        "        for name in layer_names:\n",
        "            layer = getattr(self, name)\n",
        "            if hasattr(layer, 'weight'):\n",
        "                handle = layer.weight.register_hook(create_hook(f'{name}_weight'))\n",
        "                self.gradient_hooks.append(handle)\n",
        "            if hasattr(layer, 'bias') and layer.bias is not None:\n",
        "                handle = layer.bias.register_hook(create_hook(f'{name}_bias'))\n",
        "                self.gradient_hooks.append(handle)\n",
        "        \n",
        "        print(f\"ðŸ”— Registered {len(self.gradient_hooks)} gradient hooks for visualization\")\n",
        "    \n",
        "    def remove_gradient_hooks(self):\n",
        "        \"\"\"Remove all gradient hooks.\"\"\"\n",
        "        for handle in self.gradient_hooks:\n",
        "            handle.remove()\n",
        "        self.gradient_hooks = []\n",
        "        print(\"ðŸ”“ Gradient hooks removed\")\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass with activation storage for gradient analysis.\n",
        "        \n",
        "        Args:\n",
        "            x: Input token IDs of shape (batch_size, seq_len)\n",
        "            \n",
        "        Returns:\n",
        "            Classification logits of shape (batch_size, output_dim)\n",
        "        \"\"\"\n",
        "        # Embedding layer\n",
        "        embedded = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
        "        self.activations['embedding'] = embedded\n",
        "        \n",
        "        # LSTM layers\n",
        "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
        "        self.activations['lstm'] = lstm_out\n",
        "        \n",
        "        # Use last hidden state (from both directions)\n",
        "        # hidden shape: (num_layers * 2, batch_size, hidden_dim)\n",
        "        final_hidden = torch.cat([hidden[-2], hidden[-1]], dim=-1)  # Concatenate bidirectional\n",
        "        self.activations['lstm_final'] = final_hidden\n",
        "        \n",
        "        # Fully connected layers\n",
        "        x = F.relu(self.fc1(final_hidden))\n",
        "        self.activations['fc1'] = x\n",
        "        x = self.dropout1(x)\n",
        "        \n",
        "        x = F.relu(self.fc2(x))\n",
        "        self.activations['fc2'] = x\n",
        "        x = self.dropout2(x)\n",
        "        \n",
        "        x = self.fc3(x)  # No activation here for CrossEntropyLoss\n",
        "        self.activations['fc3'] = x\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def predict_sentiment(self, text_tokens: torch.Tensor, return_probabilities: bool = True) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Predict sentiment for Australian tourism text with detailed output.\n",
        "        \n",
        "        Args:\n",
        "            text_tokens: Tokenized input text\n",
        "            return_probabilities: Whether to return class probabilities\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with predictions, confidence, and analysis\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            if text_tokens.device != self.device:\n",
        "                text_tokens = text_tokens.to(self.device)\n",
        "            \n",
        "            logits = self.forward(text_tokens)\n",
        "            probabilities = F.softmax(logits, dim=-1)\n",
        "            \n",
        "            predicted_class = torch.argmax(probabilities, dim=-1)\n",
        "            confidence = torch.max(probabilities, dim=-1).values\n",
        "            \n",
        "            sentiment_labels = ['positive', 'negative', 'neutral']\n",
        "            \n",
        "            results = {\n",
        "                'predicted_class': predicted_class.cpu().numpy(),\n",
        "                'predicted_sentiment': [sentiment_labels[i] for i in predicted_class.cpu().numpy()],\n",
        "                'confidence': confidence.cpu().numpy(),\n",
        "                'all_probabilities': probabilities.cpu().numpy() if return_probabilities else None\n",
        "            }\n",
        "            \n",
        "            return results\n",
        "\n",
        "# Create model instance\n",
        "config = ModelConfig(\n",
        "    vocab_size=10000,\n",
        "    embed_dim=128,\n",
        "    hidden_dim=256,\n",
        "    output_dim=3,\n",
        "    num_layers=2,\n",
        "    dropout_rate=0.1,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "model = AustralianTourismSentimentModel(config)\n",
        "print(f\"\\nâœ… Model architecture ready for gradient visualization!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Gradient Visualization Helper Functions ðŸŽ¨\n",
        "\n",
        "Create comprehensive gradient visualization tools following repository OOP and helper function standards:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradient Visualization Helper Functions\n",
        "\n",
        "class GradientVisualizer:\n",
        "    \"\"\"\n",
        "    Comprehensive gradient visualization toolkit for PyTorch models.\n",
        "    \n",
        "    Provides various visualization methods including heatmaps, flow diagrams,\n",
        "    statistical plots, and interactive analysis tools.\n",
        "    \n",
        "    Designed specifically for Australian tourism NLP analysis but applicable\n",
        "    to any PyTorch model with appropriate modifications.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model: nn.Module, writer: Optional[SummaryWriter] = None):\n",
        "        self.model = model\n",
        "        self.writer = writer\n",
        "        self.gradient_history = []\n",
        "        self.step_counter = 0\n",
        "        \n",
        "        print(\"ðŸŽ¨ Gradient Visualizer initialized\")\n",
        "        print(f\"   ðŸ“Š Model: {model.__class__.__name__}\")\n",
        "        print(f\"   ðŸ“ˆ TensorBoard: {'Enabled' if writer else 'Disabled'}\")\n",
        "    \n",
        "    def visualize_gradient_heatmap(self, gradients: Dict[str, torch.Tensor], \n",
        "                                 title: str = \"Gradient Magnitude Heatmap\",\n",
        "                                 save_path: Optional[str] = None) -> None:\n",
        "        \"\"\"\n",
        "        Create a heatmap visualization of gradient magnitudes across layers.\n",
        "        \n",
        "        Args:\n",
        "            gradients: Dictionary of gradient tensors by layer name\n",
        "            title: Plot title\n",
        "            save_path: Optional path to save the plot\n",
        "        \"\"\"\n",
        "        if not gradients:\n",
        "            print(\"âš ï¸  No gradients available for visualization\")\n",
        "            return\n",
        "        \n",
        "        # Extract gradient statistics\n",
        "        layer_names = []\n",
        "        grad_norms = []\n",
        "        grad_means = []\n",
        "        grad_stds = []\n",
        "        \n",
        "        for name, grad_info in gradients.items():\n",
        "            layer_names.append(name)\n",
        "            grad_norms.append(grad_info['norm'])\n",
        "            grad_means.append(abs(grad_info['mean']))\n",
        "            grad_stds.append(grad_info['std'])\n",
        "        \n",
        "        # Create subplot for comprehensive view\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "        \n",
        "        # Gradient norms\n",
        "        sns.barplot(x=layer_names, y=grad_norms, ax=axes[0], palette='viridis')\n",
        "        axes[0].set_title('ðŸ”¥ Gradient Norms by Layer', fontsize=14, fontweight='bold')\n",
        "        axes[0].set_ylabel('Gradient L2 Norm')\n",
        "        axes[0].tick_params(axis='x', rotation=45)\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Gradient means (absolute values)\n",
        "        sns.barplot(x=layer_names, y=grad_means, ax=axes[1], palette='plasma')\n",
        "        axes[1].set_title('ðŸ“Š Gradient Means (Absolute)', fontsize=14, fontweight='bold')\n",
        "        axes[1].set_ylabel('|Mean Gradient|')\n",
        "        axes[1].tick_params(axis='x', rotation=45)\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Gradient standard deviations\n",
        "        sns.barplot(x=layer_names, y=grad_stds, ax=axes[2], palette='coolwarm')\n",
        "        axes[2].set_title('ðŸ“ˆ Gradient Standard Deviations', fontsize=14, fontweight='bold')\n",
        "        axes[2].set_ylabel('Gradient Std Dev')\n",
        "        axes[2].tick_params(axis='x', rotation=45)\n",
        "        axes[2].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.suptitle(f'ðŸ‡¦ðŸ‡º {title}', fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # Save plot if path provided\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "            print(f\"ðŸ’¾ Gradient heatmap saved to {save_path}\")\n",
        "        \n",
        "        plt.show()\n",
        "    \n",
        "    def visualize_gradient_flow(self, gradients: Dict[str, torch.Tensor],\n",
        "                               title: str = \"Gradient Flow Analysis\") -> None:\n",
        "        \"\"\"\n",
        "        Visualize gradient flow through the network layers.\n",
        "        \n",
        "        Args:\n",
        "            gradients: Dictionary of gradient tensors by layer name\n",
        "            title: Plot title\n",
        "        \"\"\"\n",
        "        if not gradients:\n",
        "            print(\"âš ï¸  No gradients available for flow visualization\")\n",
        "            return\n",
        "        \n",
        "        # Extract gradient norms for flow visualization\n",
        "        layer_names = list(gradients.keys())\n",
        "        grad_norms = [gradients[name]['norm'] for name in layer_names]\n",
        "        \n",
        "        # Create flow visualization\n",
        "        fig, ax = plt.subplots(figsize=(14, 8))\n",
        "        \n",
        "        # Plot gradient flow as connected line with markers\n",
        "        x_positions = range(len(layer_names))\n",
        "        \n",
        "        # Main flow line\n",
        "        ax.plot(x_positions, grad_norms, 'o-', linewidth=3, markersize=10, \n",
        "               color='royalblue', label='Gradient Flow')\n",
        "        \n",
        "        # Add gradient magnitude annotations\n",
        "        for i, (name, norm) in enumerate(zip(layer_names, grad_norms)):\n",
        "            ax.annotate(f'{norm:.2e}', \n",
        "                       (i, norm), \n",
        "                       textcoords=\"offset points\", \n",
        "                       xytext=(0,15), \n",
        "                       ha='center',\n",
        "                       fontweight='bold',\n",
        "                       bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
        "        \n",
        "        # Highlight potential problems\n",
        "        avg_norm = np.mean(grad_norms)\n",
        "        std_norm = np.std(grad_norms)\n",
        "        \n",
        "        # Mark layers with unusually high/low gradients\n",
        "        for i, norm in enumerate(grad_norms):\n",
        "            if norm > avg_norm + 2 * std_norm:\n",
        "                ax.scatter(i, norm, color='red', s=200, alpha=0.7, \n",
        "                          label='Exploding Gradient' if i == 0 else \"\")\n",
        "            elif norm < avg_norm - 2 * std_norm:\n",
        "                ax.scatter(i, norm, color='orange', s=200, alpha=0.7,\n",
        "                          label='Vanishing Gradient' if i == 0 else \"\")\n",
        "        \n",
        "        ax.set_xlabel('Network Layers (Forward Direction)', fontsize=12, fontweight='bold')\n",
        "        ax.set_ylabel('Gradient L2 Norm', fontsize=12, fontweight='bold')\n",
        "        ax.set_title(f'ðŸŒŠ {title} - Australian Tourism Model', fontsize=14, fontweight='bold')\n",
        "        ax.set_xticks(x_positions)\n",
        "        ax.set_xticklabels(layer_names, rotation=45, ha='right')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.legend()\n",
        "        \n",
        "        # Add interpretation text\n",
        "        interpretation = f\"\"\"\n",
        "ðŸ’¡ Gradient Flow Interpretation:\n",
        "â€¢ Average gradient norm: {avg_norm:.2e}\n",
        "â€¢ Gradient std deviation: {std_norm:.2e}\n",
        "â€¢ Healthy gradients: 1e-6 to 1e-2 range\n",
        "â€¢ Vanishing: < 1e-8 (orange markers)\n",
        "â€¢ Exploding: > 1e-1 (red markers)\n",
        "        \"\"\"\n",
        "        \n",
        "        ax.text(0.02, 0.98, interpretation, transform=ax.transAxes, \n",
        "               verticalalignment='top', fontsize=10,\n",
        "               bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.8))\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def log_gradients_to_tensorboard(self, gradients: Dict[str, torch.Tensor], step: int) -> None:\n",
        "        \"\"\"\n",
        "        Log gradient statistics to TensorBoard for monitoring.\n",
        "        \n",
        "        Args:\n",
        "            gradients: Dictionary of gradient tensors by layer name\n",
        "            step: Current training step\n",
        "        \"\"\"\n",
        "        if not self.writer:\n",
        "            return\n",
        "        \n",
        "        for name, grad_info in gradients.items():\n",
        "            # Log scalar statistics\n",
        "            self.writer.add_scalar(f'Gradients/Norm/{name}', grad_info['norm'], step)\n",
        "            self.writer.add_scalar(f'Gradients/Mean/{name}', grad_info['mean'], step)\n",
        "            self.writer.add_scalar(f'Gradients/Std/{name}', grad_info['std'], step)\n",
        "            self.writer.add_scalar(f'Gradients/Min/{name}', grad_info['min'], step)\n",
        "            self.writer.add_scalar(f'Gradients/Max/{name}', grad_info['max'], step)\n",
        "            \n",
        "            # Log gradient histograms\n",
        "            if 'grad' in grad_info and grad_info['grad'] is not None:\n",
        "                self.writer.add_histogram(f'Gradients/Distribution/{name}', \n",
        "                                        grad_info['grad'], step)\n",
        "    \n",
        "    def analyze_gradient_health(self, gradients: Dict[str, torch.Tensor]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analyze gradient health and provide diagnostic information.\n",
        "        \n",
        "        Args:\n",
        "            gradients: Dictionary of gradient tensors by layer name\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with health analysis results\n",
        "        \"\"\"\n",
        "        if not gradients:\n",
        "            return {'status': 'no_gradients', 'message': 'No gradients available for analysis'}\n",
        "        \n",
        "        grad_norms = [grad_info['norm'] for grad_info in gradients.values()]\n",
        "        avg_norm = np.mean(grad_norms)\n",
        "        min_norm = np.min(grad_norms)\n",
        "        max_norm = np.max(grad_norms)\n",
        "        std_norm = np.std(grad_norms)\n",
        "        \n",
        "        # Health thresholds\n",
        "        vanishing_threshold = 1e-8\n",
        "        exploding_threshold = 1e-1\n",
        "        healthy_min = 1e-6\n",
        "        healthy_max = 1e-2\n",
        "        \n",
        "        # Analyze health status\n",
        "        issues = []\n",
        "        recommendations = []\n",
        "        \n",
        "        if min_norm < vanishing_threshold:\n",
        "            issues.append(f\"Vanishing gradients detected (min: {min_norm:.2e})\")\n",
        "            recommendations.append(\"Consider: gradient clipping, different initialization, or skip connections\")\n",
        "        \n",
        "        if max_norm > exploding_threshold:\n",
        "            issues.append(f\"Exploding gradients detected (max: {max_norm:.2e})\")\n",
        "            recommendations.append(\"Consider: gradient clipping, lower learning rate, or batch normalization\")\n",
        "        \n",
        "        if avg_norm < healthy_min:\n",
        "            issues.append(f\"Overall gradients too small (avg: {avg_norm:.2e})\")\n",
        "            recommendations.append(\"Consider: higher learning rate or different activation functions\")\n",
        "        \n",
        "        if avg_norm > healthy_max:\n",
        "            issues.append(f\"Overall gradients too large (avg: {avg_norm:.2e})\")\n",
        "            recommendations.append(\"Consider: lower learning rate or gradient normalization\")\n",
        "        \n",
        "        # Determine overall health status\n",
        "        if not issues:\n",
        "            status = \"healthy\"\n",
        "            message = \"Gradients are in healthy range\"\n",
        "        elif len(issues) == 1:\n",
        "            status = \"warning\"\n",
        "            message = \"Minor gradient issues detected\"\n",
        "        else:\n",
        "            status = \"critical\"\n",
        "            message = \"Multiple gradient issues detected\"\n",
        "        \n",
        "        return {\n",
        "            'status': status,\n",
        "            'message': message,\n",
        "            'statistics': {\n",
        "                'avg_norm': avg_norm,\n",
        "                'min_norm': min_norm,\n",
        "                'max_norm': max_norm,\n",
        "                'std_norm': std_norm,\n",
        "                'num_layers': len(grad_norms)\n",
        "            },\n",
        "            'issues': issues,\n",
        "            'recommendations': recommendations\n",
        "        }\n",
        "\n",
        "# Initialize gradient visualizer\n",
        "writer = SummaryWriter(log_dir)\n",
        "grad_visualizer = GradientVisualizer(model, writer)\n",
        "\n",
        "print(\"\\nðŸŽ¨ Gradient visualization tools ready!\")\n",
        "print(\"ðŸ“Š Available methods:\")\n",
        "print(\"   â€¢ visualize_gradient_heatmap() - Layer-wise gradient magnitude analysis\")\n",
        "print(\"   â€¢ visualize_gradient_flow() - Gradient flow through network layers\")\n",
        "print(\"   â€¢ log_gradients_to_tensorboard() - Real-time TensorBoard monitoring\")\n",
        "print(\"   â€¢ analyze_gradient_health() - Automated gradient health diagnostics\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Australian Tourism Dataset Creation ðŸŒ\n",
        "\n",
        "Create synthetic Australian tourism data for gradient analysis examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Australian Tourism Dataset with Multilingual Support\n",
        "\n",
        "def create_australian_tourism_dataset(num_samples: int = 1000) -> Tuple[torch.Tensor, torch.Tensor, List[str]]:\n",
        "    \"\"\"\n",
        "    Create synthetic Australian tourism dataset for gradient analysis.\n",
        "    \n",
        "    Generates realistic tourism reviews about Australian destinations\n",
        "    with positive, negative, and neutral sentiments.\n",
        "    \n",
        "    Args:\n",
        "        num_samples: Number of samples to generate\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (input_tensors, labels, original_texts)\n",
        "    \"\"\"\n",
        "    # Australian tourism locations and attractions\n",
        "    australian_locations = [\n",
        "        \"Sydney Opera House\", \"Harbour Bridge\", \"Bondi Beach\", \"Circular Quay\",\n",
        "        \"Melbourne\", \"coffee culture\", \"laneways\", \"Royal Botanic Gardens\",\n",
        "        \"Gold Coast\", \"beaches\", \"theme parks\", \"surfing\",\n",
        "        \"Great Barrier Reef\", \"snorkeling\", \"diving\", \"coral\",\n",
        "        \"Uluru\", \"Ayers Rock\", \"outback\", \"Aboriginal culture\",\n",
        "        \"Perth\", \"Kings Park\", \"Swan River\", \"Rottnest Island\",\n",
        "        \"Brisbane\", \"Story Bridge\", \"South Bank\", \"Queensland\",\n",
        "        \"Adelaide\", \"wine regions\", \"festivals\", \"arts\",\n",
        "        \"Darwin\", \"Kakadu\", \"crocodiles\", \"wetlands\",\n",
        "        \"Hobart\", \"Tasmania\", \"MONA\", \"Salamanca Market\",\n",
        "        \"Canberra\", \"Parliament House\", \"museums\", \"galleries\"\n",
        "    ]\n",
        "    \n",
        "    # Positive sentiment templates\n",
        "    positive_templates = [\n",
        "        \"The {location} is absolutely breathtaking and worth every dollar!\",\n",
        "        \"I love the {location} - it's stunning and unforgettable!\",\n",
        "        \"Amazing experience at {location}, highly recommend visiting!\",\n",
        "        \"{location} exceeded all my expectations, truly spectacular!\",\n",
        "        \"Perfect day exploring {location}, couldn't be happier!\",\n",
        "        \"The beauty of {location} is simply incredible and mesmerizing!\"\n",
        "    ]\n",
        "    \n",
        "    # Negative sentiment templates  \n",
        "    negative_templates = [\n",
        "        \"The {location} is overpriced and disappointing, not worth the hype.\",\n",
        "        \"Terrible experience at {location}, complete waste of time and money.\",\n",
        "        \"{location} was crowded, expensive, and underwhelming overall.\",\n",
        "        \"I regret visiting {location}, it was boring and overrated.\",\n",
        "        \"Poor service and high prices at {location}, very disappointing.\",\n",
        "        \"The {location} failed to impress, definitely not recommended.\"\n",
        "    ]\n",
        "    \n",
        "    # Neutral sentiment templates\n",
        "    neutral_templates = [\n",
        "        \"The {location} is okay, nothing special but decent enough.\",\n",
        "        \"Average experience at {location}, meets basic expectations.\",\n",
        "        \"{location} is fine for a quick visit, neither good nor bad.\",\n",
        "        \"Standard tourist attraction at {location}, typical experience.\",\n",
        "        \"The {location} is acceptable, could be better but not terrible.\",\n",
        "        \"Mediocre visit to {location}, some parts good, others not.\"\n",
        "    ]\n",
        "    \n",
        "    # Vietnamese translations for multilingual support\n",
        "    vietnamese_positive = [\n",
        "        \"Opera House Sydney tháº­t tuyá»‡t vá»i vÃ  Ä‘Ã¡ng giÃ¡ tá»«ng Ä‘á»“ng!\",\n",
        "        \"TÃ´i yÃªu Melbourne - thÃ nh phá»‘ cÃ  phÃª tuyá»‡t vá»i!\",\n",
        "        \"Tráº£i nghiá»‡m tuyá»‡t vá»i á»Ÿ Gold Coast, ráº¥t khuyáº¿n khÃ­ch ghÃ© thÄƒm!\",\n",
        "        \"Great Barrier Reef vÆ°á»£t xa mong Ä‘á»£i cá»§a tÃ´i!\",\n",
        "        \"NgÃ y hoÃ n háº£o khÃ¡m phÃ¡ Uluru, khÃ´ng thá»ƒ háº¡nh phÃºc hÆ¡n!\"\n",
        "    ]\n",
        "    \n",
        "    vietnamese_negative = [\n",
        "        \"BÃ£i biá»ƒn Bondi Ä‘áº¯t vÃ  tháº¥t vá»ng, khÃ´ng Ä‘Ã¡ng tiá»n.\",\n",
        "        \"Tráº£i nghiá»‡m tá»‡ á»Ÿ Perth, hoÃ n toÃ n lÃ£ng phÃ­ thá»i gian.\",\n",
        "        \"Brisbane Ä‘Ã´ng Ä‘Ãºc, Ä‘áº¯t Ä‘á» vÃ  khÃ´ng áº¥n tÆ°á»£ng.\",\n",
        "        \"TÃ´i há»‘i háº­n khi Ä‘áº¿n Darwin, nhÃ m chÃ¡n vÃ  Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ quÃ¡ cao.\",\n",
        "        \"Dá»‹ch vá»¥ kÃ©m á»Ÿ Adelaide, ráº¥t tháº¥t vá»ng.\"\n",
        "    ]\n",
        "    \n",
        "    vietnamese_neutral = [\n",
        "        \"Hobart á»•n, khÃ´ng cÃ³ gÃ¬ Ä‘áº·c biá»‡t nhÆ°ng khÃ¡ tá»‘t.\",\n",
        "        \"Tráº£i nghiá»‡m trung bÃ¬nh á»Ÿ Canberra, Ä‘Ã¡p á»©ng ká»³ vá»ng cÆ¡ báº£n.\",\n",
        "        \"Tasmania tá»‘t cho chuyáº¿n thÄƒm nhanh, khÃ´ng tá»‘t cÅ©ng khÃ´ng tá»‡.\",\n",
        "        \"Äiá»ƒm du lá»‹ch tiÃªu chuáº©n, tráº£i nghiá»‡m Ä‘iá»ƒn hÃ¬nh.\",\n",
        "        \"Kakadu cháº¥p nháº­n Ä‘Æ°á»£c, cÃ³ thá»ƒ tá»‘t hÆ¡n nhÆ°ng khÃ´ng tá»‡.\"\n",
        "    ]\n",
        "    \n",
        "    # Generate dataset\n",
        "    texts = []\n",
        "    labels = []\n",
        "    \n",
        "    # Calculate samples per category (including multilingual)\n",
        "    samples_per_sentiment = num_samples // 3\n",
        "    english_ratio = 0.7  # 70% English, 30% Vietnamese\n",
        "    english_samples = int(samples_per_sentiment * english_ratio)\n",
        "    vietnamese_samples = samples_per_sentiment - english_samples\n",
        "    \n",
        "    np.random.seed(42)  # For reproducibility\n",
        "    \n",
        "    # Generate positive samples\n",
        "    for _ in range(english_samples):\n",
        "        location = np.random.choice(australian_locations)\n",
        "        template = np.random.choice(positive_templates)\n",
        "        text = template.format(location=location)\n",
        "        texts.append(text)\n",
        "        labels.append(0)  # positive\n",
        "    \n",
        "    for _ in range(vietnamese_samples):\n",
        "        text = np.random.choice(vietnamese_positive)\n",
        "        texts.append(text)\n",
        "        labels.append(0)  # positive\n",
        "    \n",
        "    # Generate negative samples\n",
        "    for _ in range(english_samples):\n",
        "        location = np.random.choice(australian_locations)\n",
        "        template = np.random.choice(negative_templates)\n",
        "        text = template.format(location=location)\n",
        "        texts.append(text)\n",
        "        labels.append(1)  # negative\n",
        "    \n",
        "    for _ in range(vietnamese_samples):\n",
        "        text = np.random.choice(vietnamese_negative)\n",
        "        texts.append(text)\n",
        "        labels.append(1)  # negative\n",
        "    \n",
        "    # Generate neutral samples\n",
        "    for _ in range(english_samples):\n",
        "        location = np.random.choice(australian_locations)\n",
        "        template = np.random.choice(neutral_templates)\n",
        "        text = template.format(location=location)\n",
        "        texts.append(text)\n",
        "        labels.append(2)  # neutral\n",
        "    \n",
        "    for _ in range(vietnamese_samples):\n",
        "        text = np.random.choice(vietnamese_neutral)\n",
        "        texts.append(text)\n",
        "        labels.append(2)  # neutral\n",
        "    \n",
        "    # Simple tokenization (character-level for demonstration)\n",
        "    # In practice, use proper tokenizers like BERT tokenizer\n",
        "    def simple_tokenize(text: str, max_length: int = 100) -> List[int]:\n",
        "        \"\"\"Simple character-level tokenization for demonstration.\"\"\"\n",
        "        # Convert to lowercase and get character codes\n",
        "        char_codes = [min(ord(c), 9999) for c in text.lower()[:max_length]]\n",
        "        # Pad to max_length\n",
        "        while len(char_codes) < max_length:\n",
        "            char_codes.append(0)  # padding token\n",
        "        return char_codes\n",
        "    \n",
        "    # Tokenize all texts\n",
        "    tokenized_texts = [simple_tokenize(text) for text in texts]\n",
        "    \n",
        "    # Convert to tensors\n",
        "    input_tensor = torch.LongTensor(tokenized_texts)\n",
        "    label_tensor = torch.LongTensor(labels)\n",
        "    \n",
        "    print(f\"ðŸŒ Australian Tourism Dataset Created:\")\n",
        "    print(f\"   ðŸ“Š Total samples: {len(texts)}\")\n",
        "    print(f\"   ðŸ‡¬ðŸ‡§ English samples: {english_samples * 3}\")\n",
        "    print(f\"   ðŸ‡»ðŸ‡³ Vietnamese samples: {vietnamese_samples * 3}\")\n",
        "    print(f\"   ðŸ˜Š Positive: {labels.count(0)}\")\n",
        "    print(f\"   ðŸ˜ž Negative: {labels.count(1)}\")\n",
        "    print(f\"   ðŸ˜ Neutral: {labels.count(2)}\")\n",
        "    print(f\"   ðŸ“ Input tensor shape: {input_tensor.shape}\")\n",
        "    \n",
        "    return input_tensor, label_tensor, texts\n",
        "\n",
        "# Create dataset\n",
        "train_inputs, train_labels, train_texts = create_australian_tourism_dataset(1200)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(train_inputs, train_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Show some examples\n",
        "print(\"\\nðŸ“ Sample Australian Tourism Reviews:\")\n",
        "print(\"=\" * 60)\n",
        "sentiment_labels = ['ðŸ˜Š Positive', 'ðŸ˜ž Negative', 'ðŸ˜ Neutral']\n",
        "\n",
        "for i in range(9):  # Show 3 examples of each sentiment\n",
        "    idx = i * (len(train_texts) // 9)\n",
        "    sentiment = sentiment_labels[train_labels[idx]]\n",
        "    text = train_texts[idx][:80] + \"...\" if len(train_texts[idx]) > 80 else train_texts[idx]\n",
        "    print(f\"{sentiment}: \\\"{text}\\\"\")\n",
        "\n",
        "print(\"\\nâœ… Dataset ready for gradient analysis training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training with Gradient Monitoring ðŸš‚\n",
        "\n",
        "Train the model while monitoring and visualizing gradients in real-time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Loop with Comprehensive Gradient Monitoring\n",
        "\n",
        "def train_with_gradient_monitoring(model: nn.Module, \n",
        "                                 train_loader: DataLoader,\n",
        "                                 grad_visualizer: GradientVisualizer,\n",
        "                                 epochs: int = 5,\n",
        "                                 learning_rate: float = 0.001) -> Dict[str, List]:\n",
        "    \"\"\"\n",
        "    Train the Australian tourism sentiment model with comprehensive gradient monitoring.\n",
        "    \n",
        "    Args:\n",
        "        model: The neural network model\n",
        "        train_loader: Training data loader\n",
        "        grad_visualizer: Gradient visualization toolkit\n",
        "        epochs: Number of training epochs\n",
        "        learning_rate: Learning rate for optimization\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with training history and gradient statistics\n",
        "    \"\"\"\n",
        "    # Setup training\n",
        "    model.train()\n",
        "    model.register_gradient_hooks()\n",
        "    \n",
        "    # Optimizer and loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_accuracy': [],\n",
        "        'gradient_norms': [],\n",
        "        'gradient_health': []\n",
        "    }\n",
        "    \n",
        "    print(f\"ðŸš‚ Starting training with gradient monitoring...\")\n",
        "    print(f\"   ðŸ“Š Model: {model.__class__.__name__}\")\n",
        "    print(f\"   ðŸŽ¯ Epochs: {epochs}\")\n",
        "    print(f\"   ðŸ“ˆ Learning rate: {learning_rate}\")\n",
        "    print(f\"   ðŸ” Gradient hooks: {len(model.gradient_hooks)}\")\n",
        "    \n",
        "    global_step = 0\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0.0\n",
        "        epoch_correct = 0\n",
        "        epoch_total = 0\n",
        "        \n",
        "        print(f\"\\nðŸŒŸ Epoch {epoch + 1}/{epochs}\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            # Move to device\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient monitoring and visualization\n",
        "            if model.gradients:  # Only if hooks captured gradients\n",
        "                # Log gradients to TensorBoard\n",
        "                grad_visualizer.log_gradients_to_tensorboard(model.gradients, global_step)\n",
        "                \n",
        "                # Analyze gradient health\n",
        "                health_analysis = grad_visualizer.analyze_gradient_health(model.gradients)\n",
        "                history['gradient_health'].append(health_analysis)\n",
        "                \n",
        "                # Store gradient norms for analysis\n",
        "                gradient_norms = {name: info['norm'] for name, info in model.gradients.items()}\n",
        "                history['gradient_norms'].append(gradient_norms)\n",
        "                \n",
        "                # Print gradient health every 10 batches\n",
        "                if batch_idx % 10 == 0:\n",
        "                    status_emoji = {\n",
        "                        'healthy': 'âœ…',\n",
        "                        'warning': 'âš ï¸',\n",
        "                        'critical': 'âŒ',\n",
        "                        'no_gradients': 'â“'\n",
        "                    }\n",
        "                    emoji = status_emoji.get(health_analysis['status'], 'â“')\n",
        "                    avg_norm = health_analysis['statistics'].get('avg_norm', 0)\n",
        "                    print(f\"   Batch {batch_idx:3d}: Loss {loss:.4f}, Grad Health {emoji} (avg norm: {avg_norm:.2e})\")\n",
        "            \n",
        "            # Optimizer step\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Statistics\n",
        "            epoch_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            epoch_total += labels.size(0)\n",
        "            epoch_correct += (predicted == labels).sum().item()\n",
        "            \n",
        "            global_step += 1\n",
        "            \n",
        "            # Visualize gradients periodically\n",
        "            if batch_idx % 20 == 0 and model.gradients:\n",
        "                # Create gradient visualization\n",
        "                if batch_idx == 0:  # First batch of each epoch\n",
        "                    print(f\"\\nðŸ“Š Gradient Analysis - Epoch {epoch + 1}, Batch {batch_idx}:\")\n",
        "                    grad_visualizer.visualize_gradient_flow(\n",
        "                        model.gradients, \n",
        "                        f\"Epoch {epoch + 1} - Batch {batch_idx}\"\n",
        "                    )\n",
        "        \n",
        "        # Epoch statistics\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        accuracy = epoch_correct / epoch_total\n",
        "        \n",
        "        history['train_loss'].append(avg_loss)\n",
        "        history['train_accuracy'].append(accuracy)\n",
        "        \n",
        "        print(f\"ðŸ“Š Epoch {epoch + 1} Summary:\")\n",
        "        print(f\"   ðŸ“‰ Average Loss: {avg_loss:.4f}\")\n",
        "        print(f\"   ðŸŽ¯ Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "        \n",
        "        # Gradient health summary for the epoch\n",
        "        if history['gradient_health']:\n",
        "            recent_health = history['gradient_health'][-10:]  # Last 10 batches\n",
        "            health_counts = {}\n",
        "            for h in recent_health:\n",
        "                status = h['status']\n",
        "                health_counts[status] = health_counts.get(status, 0) + 1\n",
        "            \n",
        "            print(f\"   ðŸ¥ Gradient Health: {dict(health_counts)}\")\n",
        "    \n",
        "    # Final gradient visualization\n",
        "    if model.gradients:\n",
        "        print(f\"\\nðŸŽ¨ Final Gradient Analysis:\")\n",
        "        grad_visualizer.visualize_gradient_heatmap(\n",
        "            model.gradients,\n",
        "            \"Final Training Gradient Analysis - Australian Tourism Model\"\n",
        "        )\n",
        "    \n",
        "    # Clean up gradient hooks\n",
        "    model.remove_gradient_hooks()\n",
        "    \n",
        "    print(f\"\\nâœ… Training completed successfully!\")\n",
        "    print(f\"   ðŸ“Š Total batches processed: {global_step}\")\n",
        "    print(f\"   ðŸ“ˆ Final accuracy: {history['train_accuracy'][-1]:.4f}\")\n",
        "    \n",
        "    return history\n",
        "\n",
        "# Start training with gradient monitoring\n",
        "training_history = train_with_gradient_monitoring(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    grad_visualizer=grad_visualizer,\n",
        "    epochs=3,\n",
        "    learning_rate=0.001\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Interactive Gradient Analysis Dashboard ðŸ“Š\n",
        "\n",
        "Create an interactive dashboard for detailed gradient analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive Gradient Analysis Dashboard\n",
        "\n",
        "def create_gradient_analysis_dashboard(training_history: Dict[str, List],\n",
        "                                     model: nn.Module) -> None:\n",
        "    \"\"\"\n",
        "    Create comprehensive gradient analysis dashboard using seaborn and matplotlib.\n",
        "    \n",
        "    Args:\n",
        "        training_history: Training history with gradient statistics\n",
        "        model: Trained model for analysis\n",
        "    \"\"\"\n",
        "    # Create comprehensive dashboard\n",
        "    fig = plt.figure(figsize=(20, 15))\n",
        "    gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)\n",
        "    \n",
        "    # 1. Training Progress\n",
        "    ax1 = fig.add_subplot(gs[0, :])\n",
        "    epochs = range(1, len(training_history['train_loss']) + 1)\n",
        "    \n",
        "    ax1_twin = ax1.twinx()\n",
        "    \n",
        "    # Plot loss and accuracy on same subplot with different y-axes\n",
        "    line1 = ax1.plot(epochs, training_history['train_loss'], 'b-o', \n",
        "                    linewidth=3, markersize=8, label='Training Loss')\n",
        "    line2 = ax1_twin.plot(epochs, training_history['train_accuracy'], 'r-s', \n",
        "                         linewidth=3, markersize=8, label='Training Accuracy')\n",
        "    \n",
        "    ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "    ax1.set_ylabel('Loss', color='blue', fontsize=12, fontweight='bold')\n",
        "    ax1_twin.set_ylabel('Accuracy', color='red', fontsize=12, fontweight='bold')\n",
        "    ax1.set_title('ðŸ‡¦ðŸ‡º Australian Tourism Model: Training Progress', \n",
        "                 fontsize=16, fontweight='bold')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Combined legend\n",
        "    lines = line1 + line2\n",
        "    labels = [l.get_label() for l in lines]\n",
        "    ax1.legend(lines, labels, loc='center right')\n",
        "    \n",
        "    # 2. Gradient Norm Evolution\n",
        "    ax2 = fig.add_subplot(gs[1, :])\n",
        "    \n",
        "    if training_history['gradient_norms']:\n",
        "        # Create DataFrame for gradient norms\n",
        "        gradient_data = []\n",
        "        for step, grad_norms in enumerate(training_history['gradient_norms']):\n",
        "            for layer_name, norm in grad_norms.items():\n",
        "                gradient_data.append({\n",
        "                    'step': step,\n",
        "                    'layer': layer_name,\n",
        "                    'gradient_norm': norm\n",
        "                })\n",
        "        \n",
        "        gradient_df = pd.DataFrame(gradient_data)\n",
        "        \n",
        "        # Plot gradient norms by layer\n",
        "        for layer in gradient_df['layer'].unique():\n",
        "            layer_data = gradient_df[gradient_df['layer'] == layer]\n",
        "            ax2.plot(layer_data['step'], layer_data['gradient_norm'], \n",
        "                    marker='o', linewidth=2, label=layer, alpha=0.8)\n",
        "        \n",
        "        ax2.set_xlabel('Training Step', fontsize=12, fontweight='bold')\n",
        "        ax2.set_ylabel('Gradient L2 Norm', fontsize=12, fontweight='bold')\n",
        "        ax2.set_title('ðŸŒŠ Gradient Flow Evolution During Training', \n",
        "                     fontsize=14, fontweight='bold')\n",
        "        ax2.set_yscale('log')  # Log scale for better visualization\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    \n",
        "    # 3. Gradient Health Summary\n",
        "    ax3 = fig.add_subplot(gs[2, 0])\n",
        "    \n",
        "    if training_history['gradient_health']:\n",
        "        health_counts = {}\n",
        "        for health in training_history['gradient_health']:\n",
        "            status = health['status']\n",
        "            health_counts[status] = health_counts.get(status, 0) + 1\n",
        "        \n",
        "        # Create pie chart for health status\n",
        "        colors = {'healthy': 'green', 'warning': 'orange', 'critical': 'red', 'no_gradients': 'gray'}\n",
        "        pie_colors = [colors.get(status, 'gray') for status in health_counts.keys()]\n",
        "        \n",
        "        wedges, texts, autotexts = ax3.pie(health_counts.values(), \n",
        "                                          labels=health_counts.keys(),\n",
        "                                          colors=pie_colors,\n",
        "                                          autopct='%1.1f%%',\n",
        "                                          startangle=90)\n",
        "        \n",
        "        ax3.set_title('ðŸ¥ Gradient Health Distribution', \n",
        "                     fontsize=12, fontweight='bold')\n",
        "    \n",
        "    # 4. Layer Statistics\n",
        "    ax4 = fig.add_subplot(gs[2, 1])\n",
        "    \n",
        "    # Model parameter count by layer\n",
        "    layer_params = []\n",
        "    layer_names = []\n",
        "    \n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            layer_names.append(name.replace('.weight', '').replace('.bias', ''))\n",
        "            layer_params.append(param.numel())\n",
        "    \n",
        "    # Group by layer and sum parameters\n",
        "    layer_param_dict = {}\n",
        "    for name, count in zip(layer_names, layer_params):\n",
        "        layer_param_dict[name] = layer_param_dict.get(name, 0) + count\n",
        "    \n",
        "    if layer_param_dict:\n",
        "        bars = ax4.bar(range(len(layer_param_dict)), list(layer_param_dict.values()),\n",
        "                      color=sns.color_palette(\"viridis\", len(layer_param_dict)))\n",
        "        ax4.set_xlabel('Layer', fontsize=10, fontweight='bold')\n",
        "        ax4.set_ylabel('Parameter Count', fontsize=10, fontweight='bold')\n",
        "        ax4.set_title('ðŸ“Š Parameters per Layer', fontsize=12, fontweight='bold')\n",
        "        ax4.set_xticks(range(len(layer_param_dict)))\n",
        "        ax4.set_xticklabels(list(layer_param_dict.keys()), rotation=45, ha='right')\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 5. Gradient Statistics Box Plot\n",
        "    ax5 = fig.add_subplot(gs[2, 2])\n",
        "    \n",
        "    if training_history['gradient_norms']:\n",
        "        # Prepare data for box plot\n",
        "        box_data = []\n",
        "        box_labels = []\n",
        "        \n",
        "        # Get gradient norms for each layer across all steps\n",
        "        layer_gradient_dict = {}\n",
        "        for grad_norms in training_history['gradient_norms']:\n",
        "            for layer_name, norm in grad_norms.items():\n",
        "                if layer_name not in layer_gradient_dict:\n",
        "                    layer_gradient_dict[layer_name] = []\n",
        "                layer_gradient_dict[layer_name].append(norm)\n",
        "        \n",
        "        for layer_name, norms in layer_gradient_dict.items():\n",
        "            box_data.append(norms)\n",
        "            box_labels.append(layer_name.replace('_weight', '').replace('_bias', ''))\n",
        "        \n",
        "        if box_data:\n",
        "            bp = ax5.boxplot(box_data, labels=box_labels, patch_artist=True)\n",
        "            \n",
        "            # Color the boxes\n",
        "            colors = sns.color_palette(\"Set3\", len(bp['boxes']))\n",
        "            for patch, color in zip(bp['boxes'], colors):\n",
        "                patch.set_facecolor(color)\n",
        "                patch.set_alpha(0.7)\n",
        "            \n",
        "            ax5.set_ylabel('Gradient Norm', fontsize=10, fontweight='bold')\n",
        "            ax5.set_title('ðŸ“ˆ Gradient Distribution', fontsize=12, fontweight='bold')\n",
        "            ax5.set_yscale('log')\n",
        "            ax5.tick_params(axis='x', rotation=45)\n",
        "            ax5.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 6. Australian Tourism Examples Analysis\n",
        "    ax6 = fig.add_subplot(gs[3, :])\n",
        "    \n",
        "    # Demonstrate model predictions on sample Australian tourism texts\n",
        "    sample_texts = [\n",
        "        \"Sydney Opera House is absolutely breathtaking!\",\n",
        "        \"Melbourne coffee is overpriced and disappointing\",\n",
        "        \"Gold Coast beaches are okay, nothing special\",\n",
        "        \"Opera House Sydney tháº­t tuyá»‡t vá»i!\",  # Vietnamese positive\n",
        "        \"Brisbane Ä‘Ã´ng Ä‘Ãºc vÃ  khÃ´ng áº¥n tÆ°á»£ng\"    # Vietnamese negative\n",
        "    ]\n",
        "    \n",
        "    sentiment_labels = ['Positive', 'Negative', 'Neutral']\n",
        "    colors_sentiment = ['green', 'red', 'gray']\n",
        "    \n",
        "    # Simple prediction simulation (since we have synthetic tokenization)\n",
        "    predicted_sentiments = [0, 1, 2, 0, 1]  # Simulated predictions\n",
        "    confidences = [0.95, 0.88, 0.72, 0.91, 0.83]  # Simulated confidences\n",
        "    \n",
        "    y_positions = range(len(sample_texts))\n",
        "    bar_colors = [colors_sentiment[pred] for pred in predicted_sentiments]\n",
        "    \n",
        "    bars = ax6.barh(y_positions, confidences, color=bar_colors, alpha=0.7)\n",
        "    \n",
        "    # Add text labels\n",
        "    for i, (text, pred, conf) in enumerate(zip(sample_texts, predicted_sentiments, confidences)):\n",
        "        # Truncate long text\n",
        "        display_text = text[:50] + \"...\" if len(text) > 50 else text\n",
        "        sentiment = sentiment_labels[pred]\n",
        "        \n",
        "        # Add text annotation\n",
        "        ax6.text(0.01, i, f\"{display_text}\", \n",
        "                verticalalignment='center', fontsize=10, fontweight='bold')\n",
        "        ax6.text(conf + 0.01, i, f\"{sentiment} ({conf:.2f})\", \n",
        "                verticalalignment='center', fontsize=10)\n",
        "    \n",
        "    ax6.set_xlabel('Prediction Confidence', fontsize=12, fontweight='bold')\n",
        "    ax6.set_title('ðŸ‡¦ðŸ‡º Model Predictions on Australian Tourism Examples', \n",
        "                 fontsize=14, fontweight='bold')\n",
        "    ax6.set_xlim(0, 1)\n",
        "    ax6.set_yticks(y_positions)\n",
        "    ax6.set_yticklabels([f\"Example {i+1}\" for i in y_positions])\n",
        "    ax6.grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    # Add legend for sentiment colors\n",
        "    from matplotlib.patches import Patch\n",
        "    legend_elements = [Patch(facecolor='green', alpha=0.7, label='Positive'),\n",
        "                      Patch(facecolor='red', alpha=0.7, label='Negative'),\n",
        "                      Patch(facecolor='gray', alpha=0.7, label='Neutral')]\n",
        "    ax6.legend(handles=legend_elements, loc='lower right')\n",
        "    \n",
        "    plt.suptitle('ðŸŽ¨ Gradient Analysis Dashboard - Australian Tourism Sentiment Model', \n",
        "                fontsize=18, fontweight='bold', y=0.98)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print summary statistics\n",
        "    print(\"\\nðŸ“Š Dashboard Summary:\")\n",
        "    print(\"=\" * 50)\n",
        "    if training_history['train_loss']:\n",
        "        print(f\"ðŸ“‰ Final training loss: {training_history['train_loss'][-1]:.4f}\")\n",
        "        print(f\"ðŸŽ¯ Final training accuracy: {training_history['train_accuracy'][-1]:.4f}\")\n",
        "    \n",
        "    if training_history['gradient_health']:\n",
        "        final_health = training_history['gradient_health'][-1]\n",
        "        print(f\"ðŸ¥ Final gradient health: {final_health['status']}\")\n",
        "        if final_health['issues']:\n",
        "            print(f\"âš ï¸  Issues detected: {len(final_health['issues'])}\")\n",
        "            for issue in final_health['issues']:\n",
        "                print(f\"   â€¢ {issue}\")\n",
        "        if final_health['recommendations']:\n",
        "            print(f\"ðŸ’¡ Recommendations:\")\n",
        "            for rec in final_health['recommendations']:\n",
        "                print(f\"   â€¢ {rec}\")\n",
        "    \n",
        "    print(f\"\\nðŸŽ¨ Interactive dashboard created successfully!\")\n",
        "\n",
        "# Create the comprehensive gradient analysis dashboard\n",
        "if 'training_history' in globals():\n",
        "    create_gradient_analysis_dashboard(training_history, model)\n",
        "else:\n",
        "    print(\"âš ï¸  Training history not available. Please run the training section first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. TensorBoard Integration and Real-time Monitoring ðŸ“ˆ\n",
        "\n",
        "Set up TensorBoard for real-time gradient monitoring:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TensorBoard Integration for Real-time Gradient Monitoring\n",
        "\n",
        "print(\"ðŸ“Š TensorBoard Gradient Monitoring Setup\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Display TensorBoard access information\n",
        "print(f\"ðŸ“‚ Log directory: {log_dir}\")\n",
        "\n",
        "if IS_COLAB:\n",
        "    print(\"\\nðŸš€ Google Colab Setup:\")\n",
        "    print(\"   Run these commands in separate cells:\")\n",
        "    print(\"   ```python\")\n",
        "    print(\"   %load_ext tensorboard\")\n",
        "    print(f\"   %tensorboard --logdir {log_dir}\")\n",
        "    print(\"   ```\")\n",
        "    \n",
        "    # Try to load TensorBoard extension if in Colab\n",
        "    try:\n",
        "        get_ipython().run_line_magic('load_ext', 'tensorboard')\n",
        "        print(\"âœ… TensorBoard extension loaded\")\n",
        "        print(\"\\nðŸ’¡ Run the following command to view TensorBoard:\")\n",
        "        print(f\"   %tensorboard --logdir {log_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  Could not load TensorBoard extension: {e}\")\n",
        "        \n",
        "elif IS_KAGGLE:\n",
        "    print(\"\\nðŸ† Kaggle Setup:\")\n",
        "    print(\"   1. Download the log files after training\")\n",
        "    print(\"   2. Run locally: tensorboard --logdir ./tensorboard_logs\")\n",
        "    print(\"   3. Open http://localhost:6006 in your browser\")\n",
        "    \n",
        "else:\n",
        "    print(\"\\nðŸ–¥ï¸  Local Setup:\")\n",
        "    print(f\"   1. Run: tensorboard --logdir {log_dir}\")\n",
        "    print(\"   2. Open http://localhost:6006 in your browser\")\n",
        "    print(\"   3. Explore the following tabs:\")\n",
        "    print(\"      â€¢ SCALARS: Training metrics and gradient norms\")\n",
        "    print(\"      â€¢ HISTOGRAMS: Gradient distributions\")\n",
        "    print(\"      â€¢ IMAGES: Gradient visualizations (if logged)\")\n",
        "\n",
        "print(\"\\nðŸ“ˆ TensorBoard Features for Gradient Analysis:\")\n",
        "print(\"   ðŸ” Gradient Norms: Track gradient magnitudes by layer\")\n",
        "print(\"   ðŸ“Š Gradient Distributions: Histograms showing gradient spread\")\n",
        "print(\"   ðŸ“ˆ Training Metrics: Loss, accuracy, and learning rate\")\n",
        "print(\"   ðŸŒŠ Gradient Flow: Visualize how gradients flow through layers\")\n",
        "print(\"   ðŸ¥ Health Monitoring: Detect vanishing/exploding gradients\")\n",
        "\n",
        "print(\"\\nðŸŽ¯ Key Metrics to Monitor:\")\n",
        "print(\"   â€¢ Gradient norms should be in range 1e-6 to 1e-2\")\n",
        "print(\"   â€¢ Look for consistent gradient flow across layers\")\n",
        "print(\"   â€¢ Watch for sudden spikes or drops in gradient magnitudes\")\n",
        "print(\"   â€¢ Monitor gradient distribution changes over time\")\n",
        "\n",
        "# Close the writer to ensure all logs are saved\n",
        "if writer:\n",
        "    writer.close()\n",
        "    print(f\"\\nðŸ’¾ TensorBoard logs saved successfully!\")\n",
        "    print(f\"ðŸ“Š Log files location: {log_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Key Takeaways and Next Steps ðŸŽ¯\n",
        "\n",
        "Summary of gradient visualization concepts and practical applications:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸŽ“ What You've Learned\n",
        "\n",
        "**Core Gradient Visualization Concepts:**\n",
        "1. **Gradient Flow Understanding** - How gradients propagate through neural network layers\n",
        "2. **Gradient Health Monitoring** - Detecting vanishing and exploding gradient problems\n",
        "3. **Real-time Visualization** - Using TensorBoard for gradient monitoring during training\n",
        "4. **Statistical Analysis** - Interpreting gradient norms, distributions, and trends\n",
        "5. **Debugging Tools** - Using gradient visualization to diagnose training issues\n",
        "\n",
        "**Australian Context Applications:**\n",
        "- ðŸ›ï¸ **Tourism Sentiment Analysis** - Applied gradient visualization to Australian tourism data\n",
        "- ðŸŒ **Multilingual NLP** - Handled English-Vietnamese text processing\n",
        "- ðŸ“Š **Real-world Data Patterns** - Analyzed gradients from realistic tourism review classification\n",
        "\n",
        "**Technical Skills Developed:**\n",
        "- âœ… **PyTorch Hooks** - Implemented gradient capture mechanisms\n",
        "- âœ… **Visualization Libraries** - Used matplotlib, seaborn for gradient analysis\n",
        "- âœ… **TensorBoard Integration** - Set up comprehensive gradient monitoring\n",
        "- âœ… **OOP Design Patterns** - Built modular gradient visualization tools\n",
        "- âœ… **Helper Functions** - Created reusable gradient analysis utilities\n",
        "\n",
        "### ðŸš€ Next Steps in PyTorch Mastery\n",
        "\n",
        "**Immediate Applications:**\n",
        "1. **Apply to Your Projects** - Use gradient visualization in your own PyTorch models\n",
        "2. **Extend Visualizations** - Add layer-wise attention maps and activation visualizations\n",
        "3. **Advanced Monitoring** - Implement gradient clipping and adaptive learning rates\n",
        "4. **Production Deployment** - Set up gradient monitoring for deployed models\n",
        "\n",
        "**Advanced Topics to Explore:**\n",
        "- ðŸ”¬ **Gradient-based Interpretability** - Saliency maps and attribution methods\n",
        "- ðŸ§  **Neural Architecture Search** - Using gradients to optimize model architecture\n",
        "- ðŸ“Š **Advanced Optimization** - Second-order optimization and gradient analysis\n",
        "- ðŸŽ¯ **Model Debugging** - Advanced techniques for debugging neural networks\n",
        "\n",
        "**Repository Learning Path:**\n",
        "- ðŸ“– **Next Notebook**: `examples/pytorch-nlp/interpreting_text_models.ipynb`\n",
        "- ðŸ”§ **Advanced Tutorials**: `examples/pytorch-tutorials/`\n",
        "- ðŸŒ **Translation Projects**: `examples/language_translation/`\n",
        "\n",
        "### ðŸ’¡ Practical Tips for Production\n",
        "\n",
        "**Gradient Monitoring Best Practices:**\n",
        "1. **Monitor Regularly** - Set up automated gradient health checks\n",
        "2. **Set Thresholds** - Define acceptable gradient norm ranges for your domain\n",
        "3. **Log Strategically** - Balance detailed logging with performance considerations\n",
        "4. **Visualize Periodically** - Create gradient reports for model reviews\n",
        "\n",
        "**Performance Considerations:**\n",
        "- ðŸ”§ **Gradient Hooks** - Remove hooks during inference to improve performance\n",
        "- ðŸ“Š **Logging Frequency** - Adjust logging frequency based on training duration\n",
        "- ðŸ’¾ **Storage Management** - Implement log rotation for long-running training\n",
        "- ðŸš€ **Optimization** - Use gradient statistics rather than full gradient storage\n",
        "\n",
        "### ðŸŒŸ Congratulations!\n",
        "\n",
        "You now have comprehensive skills in PyTorch gradient visualization and can:\n",
        "- ðŸ” **Diagnose training problems** using gradient analysis\n",
        "- ðŸ“Š **Monitor model health** in real-time during training\n",
        "- ðŸŽ¨ **Create informative visualizations** for gradient flow and statistics\n",
        "- ðŸ‡¦ðŸ‡º **Apply techniques** to real-world Australian tourism NLP tasks\n",
        "- ðŸŒ **Handle multilingual scenarios** with English-Vietnamese examples\n",
        "\n",
        "**Ready for the next challenge?** Explore more advanced PyTorch techniques in our other notebooks!\n",
        "\n",
        "---\n",
        "\n",
        "**ðŸ“š Additional Resources:**\n",
        "- [PyTorch Documentation: Autograd](https://pytorch.org/docs/stable/autograd.html)\n",
        "- [TensorBoard with PyTorch](https://pytorch.org/docs/stable/tensorboard.html)\n",
        "- [Understanding Gradient Flow](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n",
        "- [Visualizing Gradients in Deep Networks](https://arxiv.org/abs/1605.06579)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}