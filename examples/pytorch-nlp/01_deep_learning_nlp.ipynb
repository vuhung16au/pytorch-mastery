{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Learning for NLP with PyTorch \ud83c\udde6\ud83c\uddfa\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/pytorch-mastery/blob/main/examples/pytorch-nlp/01_deep_learning_nlp.ipynb)\n",
        "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/pytorch-mastery/blob/main/examples/pytorch-nlp/01_deep_learning_nlp.ipynb)\n",
        "\n",
        "A comprehensive introduction to deep learning concepts for Natural Language Processing using PyTorch, featuring Australian tourism examples and English-Vietnamese multilingual support.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "- \ud83c\udfd7\ufe0f **Master PyTorch fundamentals** for NLP applications\n",
        "- \ud83d\udcca **Build neural networks** for Australian text classification\n",
        "- \ud83c\udde6\ud83c\uddfa **Process tourism data** with practical Australian examples\n",
        "- \ud83c\udf0f **Handle multilingual text** with English-Vietnamese examples\n",
        "- \ud83d\udd04 **Compare with TensorFlow** to ease the transition\n",
        "- \ud83d\udcc8 **Implement TensorBoard logging** for training visualization\n",
        "\n",
        "## What You'll Build\n",
        "\n",
        "1. **Australian Tourism Sentiment Classifier** - Analyze reviews of Sydney Opera House, Melbourne coffee, Perth beaches\n",
        "2. **Multilingual Text Processor** - Handle English and Vietnamese tourism content\n",
        "3. **Neural Network from Scratch** - Build and train your first PyTorch NLP model\n",
        "4. **Cross-Platform Solution** - Code that works on Local, Colab, and Kaggle\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment Detection and Setup\n",
        "import sys\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Detect the runtime environment\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules or \"kaggle\" in os.environ.get('KAGGLE_URL_BASE', '')\n",
        "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
        "\n",
        "print(f\"Environment detected:\")\n",
        "print(f\"  - Local: {IS_LOCAL}\")\n",
        "print(f\"  - Google Colab: {IS_COLAB}\")\n",
        "print(f\"  - Kaggle: {IS_KAGGLE}\")\n",
        "\n",
        "# Platform-specific system setup\n",
        "if IS_COLAB:\n",
        "    print(\"\\nSetting up Google Colab environment...\")\n",
        "    !apt update -qq\n",
        "    !apt install -y -qq software-properties-common\n",
        "elif IS_KAGGLE:\n",
        "    print(\"\\nSetting up Kaggle environment...\")\n",
        "    # Kaggle usually has most packages pre-installed\n",
        "else:\n",
        "    print(\"\\nSetting up local environment...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages for this notebook\n",
        "required_packages = [\n",
        "    \"torch\",\n",
        "    \"transformers\",\n",
        "    \"datasets\", \n",
        "    \"tokenizers\",\n",
        "    \"pandas\",\n",
        "    \"seaborn\",\n",
        "    \"matplotlib\",\n",
        "    \"tensorboard\",\n",
        "    \"scikit-learn\"\n",
        "]\n",
        "\n",
        "print(\"Installing required packages...\")\n",
        "for package in required_packages:\n",
        "    if IS_COLAB or IS_KAGGLE:\n",
        "        !pip install -q {package}\n",
        "    else:\n",
        "        try:\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
        "                          capture_output=True, check=True)\n",
        "        except subprocess.CalledProcessError:\n",
        "            print(f\"Note: {package} installation skipped (likely already installed)\")\n",
        "    print(f\"\u2713 {package}\")\n",
        "\n",
        "print(\"\\n\ud83d\udce6 Package installation completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import essential libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Data handling and visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Text processing\n",
        "import re\n",
        "import string\n",
        "from collections import Counter, defaultdict\n",
        "import random\n",
        "\n",
        "# Set style for better notebook aesthetics\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(16)\n",
        "np.random.seed(16)\n",
        "random.seed(16)\n",
        "\n",
        "print(f\"\u2705 PyTorch {torch.__version__} ready!\")\n",
        "print(f\"\ud83d\udcca Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import platform\n",
        "\n",
        "def detect_device():\n",
        "    \"\"\"\n",
        "    Detect the best available PyTorch device with comprehensive hardware support.\n",
        "    \n",
        "    Priority order:\n",
        "    1. CUDA (NVIDIA GPUs) - Best performance for deep learning\n",
        "    2. MPS (Apple Silicon) - Optimized for M1/M2/M3 Macs  \n",
        "    3. CPU (Universal) - Always available fallback\n",
        "    \n",
        "    Returns:\n",
        "        torch.device: The optimal device for PyTorch operations\n",
        "        str: Human-readable device description for logging\n",
        "    \"\"\"\n",
        "    # Check for CUDA (NVIDIA GPU)\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        device_info = f\"CUDA GPU: {gpu_name}\"\n",
        "        \n",
        "        # Additional CUDA info for optimization\n",
        "        cuda_version = torch.version.cuda\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "        \n",
        "        print(f\"\ud83d\ude80 Using CUDA acceleration\")\n",
        "        print(f\"   GPU: {gpu_name}\")\n",
        "        print(f\"   CUDA Version: {cuda_version}\")\n",
        "        print(f\"   GPU Memory: {gpu_memory:.1f} GB\")\n",
        "        \n",
        "        return device, device_info\n",
        "    \n",
        "    # Check for MPS (Apple Silicon)\n",
        "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "        device_info = \"Apple Silicon MPS\"\n",
        "        \n",
        "        # Get system info for Apple Silicon\n",
        "        system_info = platform.uname()\n",
        "        \n",
        "        print(f\"\ud83c\udf4e Using Apple Silicon MPS acceleration\")\n",
        "        print(f\"   System: {system_info.system} {system_info.release}\")\n",
        "        print(f\"   Machine: {system_info.machine}\")\n",
        "        print(f\"   Processor: {system_info.processor}\")\n",
        "        \n",
        "        return device, device_info\n",
        "    \n",
        "    # Fallback to CPU\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        device_info = \"CPU (No GPU acceleration available)\"\n",
        "        \n",
        "        # Get CPU info for optimization guidance\n",
        "        cpu_count = torch.get_num_threads()\n",
        "        system_info = platform.uname()\n",
        "        \n",
        "        print(f\"\ud83d\udcbb Using CPU (no GPU acceleration detected)\")\n",
        "        print(f\"   Processor: {system_info.processor}\")\n",
        "        print(f\"   PyTorch Threads: {cpu_count}\")\n",
        "        print(f\"   System: {system_info.system} {system_info.release}\")\n",
        "        \n",
        "        # Provide optimization suggestions for CPU-only setups\n",
        "        print(f\"\\n\ud83d\udca1 CPU Optimization Tips:\")\n",
        "        print(f\"   \u2022 Reduce batch size to prevent memory issues\")\n",
        "        print(f\"   \u2022 Consider using smaller models for faster training\")\n",
        "        print(f\"   \u2022 Enable PyTorch optimizations: torch.set_num_threads({cpu_count})\")\n",
        "        \n",
        "        return device, device_info\n",
        "\n",
        "# Usage in all PyTorch notebooks\n",
        "device, device_info = detect_device()\n",
        "print(f\"\\n\u2705 PyTorch device selected: {device}\")\n",
        "print(f\"\ud83d\udcca Device info: {device_info}\")\n",
        "\n",
        "# Set global device for the notebook\n",
        "DEVICE = device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Australian Tourism Dataset with English-Vietnamese Examples\n",
        "print(\"\ud83c\udde6\ud83c\uddfa Creating Australian Tourism Dataset with Multilingual Support\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Australian tourism reviews with English-Vietnamese pairs\n",
        "australian_tourism_data = {\n",
        "    'english': [\n",
        "        # Positive reviews\n",
        "        \"The Sydney Opera House is absolutely breathtaking! Worth every dollar.\",\n",
        "        \"Melbourne's coffee culture is world-class. Amazing baristas everywhere!\",\n",
        "        \"Uluru at sunset was a spiritual experience I'll never forget.\",\n",
        "        \"Perth beaches are perfect for families with young children.\",\n",
        "        \"Brisbane's South Bank is a fantastic place for weekend activities.\",\n",
        "        \"Adelaide's food scene exceeded all my expectations. Incredible wines!\",\n",
        "        \"Darwin's tropical climate and laid-back vibe are absolutely perfect.\",\n",
        "        \"Hobart's MONA museum is mind-blowing and thought-provoking.\",\n",
        "        \"Canberra's national galleries showcase Australia's rich cultural heritage.\",\n",
        "        \"The Great Barrier Reef snorkeling was the highlight of my trip.\",\n",
        "        \"Blue Mountains scenic railway offers spectacular mountain views.\",\n",
        "        \"Gold Coast theme parks provide endless fun for the whole family.\",\n",
        "        \n",
        "        # Neutral reviews\n",
        "        \"Sydney Harbour Bridge climb was okay, but quite expensive for what it is.\",\n",
        "        \"Melbourne weather is unpredictable, pack clothes for all seasons.\",\n",
        "        \"Perth is very isolated but has decent shopping and dining options.\",\n",
        "        \"Brisbane can be quite humid during summer months, plan accordingly.\",\n",
        "        \"Adelaide is smaller than expected but has its own unique charm.\",\n",
        "        \"Darwin has limited attractions but the markets are interesting.\",\n",
        "        \n",
        "        # Negative reviews\n",
        "        \"Sydney accommodation prices are absolutely outrageous and unreasonable.\",\n",
        "        \"Melbourne trams are constantly delayed and overcrowded during peak hours.\",\n",
        "        \"Perth nightlife is disappointing and closes way too early.\",\n",
        "        \"Brisbane's public transport system needs significant improvements.\",\n",
        "        \"Adelaide becomes very quiet after 6 PM, limited evening entertainment.\",\n",
        "        \"Darwin is extremely expensive for basic necessities and groceries.\"\n",
        "    ],\n",
        "    \n",
        "    'vietnamese': [\n",
        "        # Positive reviews (Vietnamese)\n",
        "        \"Nh\u00e0 h\u00e1t Opera Sydney th\u1eadt ngo\u1ea1n m\u1ee5c! X\u1ee9ng \u0111\u00e1ng t\u1eebng \u0111\u1ed3ng ti\u1ec1n.\",\n",
        "        \"V\u0103n h\u00f3a c\u00e0 ph\u00ea Melbourne \u0111\u1eb3ng c\u1ea5p th\u1ebf gi\u1edbi. Barista tuy\u1ec7t v\u1eddi \u1edf kh\u1eafp n\u01a1i!\",\n",
        "        \"Uluru l\u00fac ho\u00e0ng h\u00f4n l\u00e0 tr\u1ea3i nghi\u1ec7m t\u00e2m linh t\u00f4i s\u1ebd kh\u00f4ng bao gi\u1edd qu\u00ean.\",\n",
        "        \"B\u00e3i bi\u1ec3n Perth ho\u00e0n h\u1ea3o cho c\u00e1c gia \u0111\u00ecnh c\u00f3 con nh\u1ecf.\",\n",
        "        \"South Bank Brisbane l\u00e0 n\u01a1i tuy\u1ec7t v\u1eddi cho ho\u1ea1t \u0111\u1ed9ng cu\u1ed1i tu\u1ea7n.\",\n",
        "        \"\u1ea8m th\u1ef1c Adelaide v\u01b0\u1ee3t xa mong \u0111\u1ee3i c\u1ee7a t\u00f4i. R\u01b0\u1ee3u vang tuy\u1ec7t v\u1eddi!\",\n",
        "        \"Kh\u00ed h\u1eadu nhi\u1ec7t \u0111\u1edbi v\u00e0 kh\u00f4ng kh\u00ed th\u01b0 gi\u00e3n \u1edf Darwin th\u1eadt ho\u00e0n h\u1ea3o.\",\n",
        "        \"B\u1ea3o t\u00e0ng MONA Hobart th\u1eadt \u1ea5n t\u01b0\u1ee3ng v\u00e0 k\u00edch th\u00edch t\u01b0 duy.\",\n",
        "        \"C\u00e1c ph\u00f2ng tr\u01b0ng b\u00e0y qu\u1ed1c gia Canberra th\u1ec3 hi\u1ec7n di s\u1ea3n v\u0103n h\u00f3a \u00dac.\",\n",
        "        \"L\u1eb7n ng\u1eafm san h\u00f4 Great Barrier Reef l\u00e0 \u0111i\u1ec3m nh\u1ea5n chuy\u1ebfn \u0111i.\",\n",
        "        \"\u0110\u01b0\u1eddng s\u1eaft Blue Mountains mang \u0111\u1ebfn t\u1ea7m nh\u00ecn n\u00fai non tuy\u1ec7t \u0111\u1eb9p.\",\n",
        "        \"C\u00f4ng vi\u00ean gi\u1ea3i tr\u00ed Gold Coast mang l\u1ea1i ni\u1ec1m vui cho c\u1ea3 gia \u0111\u00ecnh.\",\n",
        "        \n",
        "        # Neutral reviews (Vietnamese)\n",
        "        \"Leo c\u1ea7u Sydney Harbour Bridge c\u0169ng \u0111\u01b0\u1ee3c, nh\u01b0ng kh\u00e1 \u0111\u1eaft so v\u1edbi gi\u00e1 tr\u1ecb.\",\n",
        "        \"Th\u1eddi ti\u1ebft Melbourne kh\u00f3 \u0111o\u00e1n, h\u00e3y mang qu\u1ea7n \u00e1o cho m\u1ecdi m\u00f9a.\",\n",
        "        \"Perth r\u1ea5t bi\u1ec7t l\u1eadp nh\u01b0ng c\u00f3 l\u1ef1a ch\u1ecdn mua s\u1eafm v\u00e0 \u0103n u\u1ed1ng t\u1ed1t.\",\n",
        "        \"Brisbane c\u00f3 th\u1ec3 kh\u00e1 \u1ea9m \u01b0\u1edbt v\u00e0o m\u00f9a h\u00e8, h\u00e3y l\u00ean k\u1ebf ho\u1ea1ch ph\u00f9 h\u1ee3p.\",\n",
        "        \"Adelaide nh\u1ecf h\u01a1n mong \u0111\u1ee3i nh\u01b0ng c\u00f3 n\u00e9t quy\u1ebfn r\u0169 ri\u00eang.\",\n",
        "        \"Darwin c\u00f3 \u00edt \u0111i\u1ec3m tham quan nh\u01b0ng c\u00e1c khu ch\u1ee3 kh\u00e1 th\u00fa v\u1ecb.\",\n",
        "        \n",
        "        # Negative reviews (Vietnamese)\n",
        "        \"Gi\u00e1 ch\u1ed7 \u1edf Sydney th\u1eadt phi l\u00fd v\u00e0 qu\u00e1 \u0111\u1eaft \u0111\u1ecf.\",\n",
        "        \"T\u00e0u \u0111i\u1ec7n Melbourne li\u00ean t\u1ee5c ch\u1eadm tr\u1ec5 v\u00e0 qu\u00e1 t\u1ea3i gi\u1edd cao \u0111i\u1ec3m.\",\n",
        "        \"Cu\u1ed9c s\u1ed1ng v\u1ec1 \u0111\u00eam Perth th\u1ea5t v\u1ecdng v\u00e0 \u0111\u00f3ng c\u1eeda qu\u00e1 s\u1edbm.\",\n",
        "        \"H\u1ec7 th\u1ed1ng giao th\u00f4ng c\u00f4ng c\u1ed9ng Brisbane c\u1ea7n c\u1ea3i thi\u1ec7n \u0111\u00e1ng k\u1ec3.\",\n",
        "        \"Adelaide tr\u1edf n\u00ean r\u1ea5t y\u00ean t\u0129nh sau 6 gi\u1edd chi\u1ec1u, \u00edt gi\u1ea3i tr\u00ed bu\u1ed5i t\u1ed1i.\",\n",
        "        \"Darwin c\u1ef1c k\u1ef3 \u0111\u1eaft \u0111\u1ecf cho nh\u1eefng nhu c\u1ea7u c\u01a1 b\u1ea3n v\u00e0 th\u1ef1c ph\u1ea9m.\"\n",
        "    ],\n",
        "    \n",
        "    'labels': [\n",
        "        # Positive labels (1)\n",
        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "        # Neutral labels (0) \n",
        "        0, 0, 0, 0, 0, 0,\n",
        "        # Negative labels (-1)\n",
        "        -1, -1, -1, -1, -1, -1\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create DataFrame for easy manipulation\n",
        "tourism_df = pd.DataFrame({\n",
        "    'english_text': australian_tourism_data['english'],\n",
        "    'vietnamese_text': australian_tourism_data['vietnamese'], \n",
        "    'sentiment': australian_tourism_data['labels']\n",
        "})\n",
        "\n",
        "# Add sentiment labels\n",
        "sentiment_map = {1: 'positive', 0: 'neutral', -1: 'negative'}\n",
        "tourism_df['sentiment_label'] = tourism_df['sentiment'].map(sentiment_map)\n",
        "\n",
        "print(f\"\ud83d\udcca Dataset Statistics:\")\n",
        "print(f\"   Total reviews: {len(tourism_df)}\")\n",
        "print(f\"   Languages: English, Vietnamese\")\n",
        "print(f\"   Sentiment distribution:\")\n",
        "print(tourism_df['sentiment_label'].value_counts().to_string())\n",
        "\n",
        "# Display sample data\n",
        "print(f\"\\n\ud83c\udf1f Sample Australian Tourism Reviews:\")\n",
        "for i in range(3):\n",
        "    row = tourism_df.iloc[i]\n",
        "    print(f\"\\n{i+1}. Sentiment: {row['sentiment_label'].upper()}\")\n",
        "    print(f\"   \ud83c\uddec\ud83c\udde7 English: {row['english_text']}\")\n",
        "    print(f\"   \ud83c\uddfb\ud83c\uddf3 Vietnamese: {row['vietnamese_text']}\")\n",
        "\n",
        "tourism_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AustralianTextPreprocessor:\n",
        "    \"\"\"\n",
        "    Text preprocessing pipeline for Australian tourism content with multilingual support.\n",
        "    \n",
        "    Handles both English and Vietnamese text with Australian-specific considerations:\n",
        "    - Australian English spelling and terminology\n",
        "    - Vietnamese diacritics and tone marks\n",
        "    - Tourism-specific vocabulary (Sydney, Melbourne, etc.)\n",
        "    \n",
        "    TensorFlow equivalent:\n",
        "        tf.keras.preprocessing.text.Tokenizer with custom filters\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, max_vocab_size=10000, max_length=128):\n",
        "        self.max_vocab_size = max_vocab_size\n",
        "        self.max_length = max_length\n",
        "        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1, '<START>': 2, '<END>': 3}\n",
        "        self.idx_to_word = {0: '<PAD>', 1: '<UNK>', 2: '<START>', 3: '<END>'}\n",
        "        self.vocab_size = 4\n",
        "        \n",
        "        # Australian cities and landmarks for special handling\n",
        "        self.australian_entities = {\n",
        "            'cities': ['sydney', 'melbourne', 'brisbane', 'perth', 'adelaide', 'darwin', 'hobart', 'canberra'],\n",
        "            'landmarks': ['opera', 'harbour', 'bridge', 'uluru', 'reef', 'mountains'],\n",
        "            'states': ['nsw', 'vic', 'qld', 'wa', 'sa', 'nt', 'tas', 'act']\n",
        "        }\n",
        "    \n",
        "    def clean_text(self, text):\n",
        "        \"\"\"\n",
        "        Clean and normalize text while preserving Australian and Vietnamese characteristics.\n",
        "        \"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "        \n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "        \n",
        "        # Handle Australian-specific contractions\n",
        "        australian_contractions = {\n",
        "            \"can't\": \"cannot\",\n",
        "            \"won't\": \"will not\",\n",
        "            \"i'm\": \"i am\",\n",
        "            \"you're\": \"you are\",\n",
        "            \"it's\": \"it is\",\n",
        "            \"that's\": \"that is\",\n",
        "            \"there's\": \"there is\",\n",
        "            \"we're\": \"we are\",\n",
        "            \"they're\": \"they are\"\n",
        "        }\n",
        "        \n",
        "        for contraction, expansion in australian_contractions.items():\n",
        "            text = text.replace(contraction, expansion)\n",
        "        \n",
        "        # Remove excessive punctuation but keep sentence structure\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "        \n",
        "        # Handle multiple spaces\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        \n",
        "        return text.strip()\n",
        "    \n",
        "    def build_vocabulary(self, texts):\n",
        "        \"\"\"\n",
        "        Build vocabulary from Australian tourism texts.\n",
        "        \"\"\"\n",
        "        word_freq = Counter()\n",
        "        \n",
        "        print(\"\ud83d\udd24 Building vocabulary from Australian tourism corpus...\")\n",
        "        \n",
        "        for text in texts:\n",
        "            cleaned = self.clean_text(text)\n",
        "            words = cleaned.split()\n",
        "            word_freq.update(words)\n",
        "        \n",
        "        # Get most common words, excluding special tokens\n",
        "        most_common = word_freq.most_common(self.max_vocab_size - 4)\n",
        "        \n",
        "        # Add words to vocabulary\n",
        "        for word, freq in most_common:\n",
        "            if word not in self.word_to_idx:\n",
        "                self.word_to_idx[word] = self.vocab_size\n",
        "                self.idx_to_word[self.vocab_size] = word\n",
        "                self.vocab_size += 1\n",
        "        \n",
        "        print(f\"   \ud83d\udcca Vocabulary size: {self.vocab_size}\")\n",
        "        print(f\"   \ud83c\udde6\ud83c\uddfa Australian entities found: {len([w for w in self.word_to_idx if any(ent in w for entities in self.australian_entities.values() for ent in entities)])}\")\n",
        "        \n",
        "        # Show most common Australian tourism words\n",
        "        tourism_words = [word for word, freq in most_common[:20]]\n",
        "        print(f\"   \ud83c\udf1f Top tourism words: {', '.join(tourism_words[:10])}\")\n",
        "    \n",
        "    def encode_text(self, text):\n",
        "        \"\"\"\n",
        "        Convert text to sequence of token IDs.\n",
        "        \"\"\"\n",
        "        cleaned = self.clean_text(text)\n",
        "        words = cleaned.split()\n",
        "        \n",
        "        # Convert words to indices\n",
        "        indices = [self.word_to_idx.get(word, self.word_to_idx['<UNK>']) for word in words]\n",
        "        \n",
        "        # Add start and end tokens\n",
        "        indices = [self.word_to_idx['<START>']] + indices + [self.word_to_idx['<END>']]\n",
        "        \n",
        "        # Pad or truncate to max_length\n",
        "        if len(indices) > self.max_length:\n",
        "            indices = indices[:self.max_length]\n",
        "        else:\n",
        "            indices.extend([self.word_to_idx['<PAD>']] * (self.max_length - len(indices)))\n",
        "        \n",
        "        return indices\n",
        "    \n",
        "    def decode_text(self, indices):\n",
        "        \"\"\"\n",
        "        Convert sequence of token IDs back to text.\n",
        "        \"\"\"\n",
        "        words = []\n",
        "        for idx in indices:\n",
        "            word = self.idx_to_word.get(idx, '<UNK>')\n",
        "            if word not in ['<PAD>', '<START>', '<END>']:\n",
        "                words.append(word)\n",
        "        return ' '.join(words)\n",
        "\n",
        "# Initialize preprocessor\n",
        "preprocessor = AustralianTextPreprocessor(max_vocab_size=5000, max_length=64)\n",
        "\n",
        "# Build vocabulary from both English and Vietnamese texts\n",
        "all_texts = tourism_df['english_text'].tolist() + tourism_df['vietnamese_text'].tolist()\n",
        "preprocessor.build_vocabulary(all_texts)\n",
        "\n",
        "print(f\"\\n\ud83d\udd27 Text Preprocessing Pipeline Ready!\")\n",
        "print(f\"   Max vocabulary size: {preprocessor.max_vocab_size}\")\n",
        "print(f\"   Max sequence length: {preprocessor.max_length}\")\n",
        "print(f\"   Actual vocabulary size: {preprocessor.vocab_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare training data with both English and Vietnamese texts\n",
        "print(\"\ud83d\udcca Preparing Australian Tourism Training Data\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Encode texts\n",
        "english_encoded = [preprocessor.encode_text(text) for text in tourism_df['english_text']]\n",
        "vietnamese_encoded = [preprocessor.encode_text(text) for text in tourism_df['vietnamese_text']]\n",
        "\n",
        "# Combine English and Vietnamese data for multilingual training\n",
        "all_encoded_texts = english_encoded + vietnamese_encoded\n",
        "all_labels = tourism_df['sentiment'].tolist() * 2  # Duplicate labels for both languages\n",
        "\n",
        "# Convert to tensors\n",
        "X = torch.tensor(all_encoded_texts, dtype=torch.long)\n",
        "y = torch.tensor(all_labels, dtype=torch.long)\n",
        "\n",
        "# Adjust labels for classification (0, 1, 2 instead of -1, 0, 1)\n",
        "y = y + 1  # Now: 0=negative, 1=neutral, 2=positive\n",
        "\n",
        "print(f\"\ud83d\udcc8 Dataset prepared:\")\n",
        "print(f\"   Input shape: {X.shape}\")\n",
        "print(f\"   Labels shape: {y.shape}\")\n",
        "print(f\"   Vocabulary size: {preprocessor.vocab_size}\")\n",
        "print(f\"   Sequence length: {preprocessor.max_length}\")\n",
        "\n",
        "# Split data for training and validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\n\ud83d\udd04 Train/Validation Split:\")\n",
        "print(f\"   Training samples: {len(X_train)}\")\n",
        "print(f\"   Validation samples: {len(X_val)}\")\n",
        "\n",
        "# Create data loaders for efficient training\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "batch_size = 16 if DEVICE.type == 'cpu' else 32  # Adjust batch size based on device\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"\\n\u26a1 Data loaders created:\")\n",
        "print(f\"   Batch size: {batch_size} (optimized for {DEVICE.type.upper()})\")\n",
        "print(f\"   Training batches: {len(train_loader)}\")\n",
        "print(f\"   Validation batches: {len(val_loader)}\")\n",
        "\n",
        "# Show example of encoded text\n",
        "sample_idx = 0\n",
        "sample_text = tourism_df['english_text'].iloc[sample_idx]\n",
        "sample_encoded = english_encoded[sample_idx]\n",
        "sample_decoded = preprocessor.decode_text(sample_encoded)\n",
        "\n",
        "print(f\"\\n\ud83d\udd0d Encoding Example:\")\n",
        "print(f\"   Original: {sample_text}\")\n",
        "print(f\"   Encoded: {sample_encoded[:10]}... (showing first 10 tokens)\")\n",
        "print(f\"   Decoded: {sample_decoded}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AustralianTourismSentimentClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network for Australian tourism sentiment analysis with multilingual support.\n",
        "    \n",
        "    Architecture:\n",
        "    - Embedding layer: Maps vocabulary words to dense vectors\n",
        "    - LSTM layer: Processes sequence of embeddings to capture context\n",
        "    - Dropout: Prevents overfitting with regularization\n",
        "    - Linear layers: Classify sentiment (negative, neutral, positive)\n",
        "    \n",
        "    TensorFlow equivalent:\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Embedding(vocab_size, embed_dim),\n",
        "            tf.keras.layers.LSTM(128, return_sequences=False),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(3, activation='softmax')\n",
        "        ])\n",
        "    \n",
        "    Args:\n",
        "        vocab_size (int): Size of vocabulary\n",
        "        embed_dim (int): Dimension of embedding vectors\n",
        "        hidden_dim (int): LSTM hidden dimension\n",
        "        num_classes (int): Number of sentiment classes (3)\n",
        "        dropout_rate (float): Dropout probability\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_classes=3, dropout_rate=0.3):\n",
        "        super(AustralianTourismSentimentClassifier, self).__init__()\n",
        "        \n",
        "        # Store parameters\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        # Embedding layer - converts token IDs to dense vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        \n",
        "        # LSTM for sequence processing\n",
        "        self.lstm = nn.LSTM(\n",
        "            embed_dim, hidden_dim, \n",
        "            batch_first=True, \n",
        "            bidirectional=False,\n",
        "            dropout=dropout_rate if hidden_dim > 1 else 0\n",
        "        )\n",
        "        \n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        \n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "        \n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "        \n",
        "        # Sentiment labels for interpretation\n",
        "        self.sentiment_labels = ['negative', 'neutral', 'positive']\n",
        "        \n",
        "        # Australian cities for context\n",
        "        self.australian_cities = [\"Sydney\", \"Melbourne\", \"Brisbane\", \"Perth\", \n",
        "                                \"Adelaide\", \"Darwin\", \"Hobart\", \"Canberra\"]\n",
        "    \n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize model weights - manual in PyTorch, automatic in TensorFlow\"\"\"\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                if len(param.shape) > 1:\n",
        "                    nn.init.xavier_uniform_(param)\n",
        "                else:\n",
        "                    nn.init.zeros_(param)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "        \n",
        "        Args:\n",
        "            x (torch.Tensor): Input token IDs [batch_size, seq_len]\n",
        "        \n",
        "        Returns:\n",
        "            torch.Tensor: Logits for sentiment classification [batch_size, num_classes]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = x.shape\n",
        "        \n",
        "        # Embedding lookup: [batch_size, seq_len] -> [batch_size, seq_len, embed_dim]\n",
        "        embedded = self.embedding(x)\n",
        "        \n",
        "        # LSTM processing: [batch_size, seq_len, embed_dim] -> [batch_size, seq_len, hidden_dim]\n",
        "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
        "        \n",
        "        # Use last hidden state for classification: [batch_size, hidden_dim]\n",
        "        last_hidden = hidden[-1]  # Last layer's hidden state\n",
        "        \n",
        "        # Apply dropout\n",
        "        dropped = self.dropout(last_hidden)\n",
        "        \n",
        "        # Classification: [batch_size, hidden_dim] -> [batch_size, num_classes]\n",
        "        logits = self.classifier(dropped)\n",
        "        \n",
        "        return logits\n",
        "    \n",
        "    def predict_sentiment(self, text_tensor, preprocessor):\n",
        "        \"\"\"\n",
        "        Predict sentiment for Australian tourism text.\n",
        "        \n",
        "        Args:\n",
        "            text_tensor (torch.Tensor): Encoded text tensor\n",
        "            preprocessor: Text preprocessor for decoding\n",
        "        \n",
        "        Returns:\n",
        "            dict: Prediction results with probabilities\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            if text_tensor.dim() == 1:\n",
        "                text_tensor = text_tensor.unsqueeze(0)  # Add batch dimension\n",
        "            \n",
        "            logits = self.forward(text_tensor.to(next(self.parameters()).device))\n",
        "            probabilities = F.softmax(logits, dim=1)\n",
        "            predicted_class = torch.argmax(probabilities, dim=1).item()\n",
        "            confidence = probabilities[0][predicted_class].item()\n",
        "            \n",
        "            return {\n",
        "                'sentiment': self.sentiment_labels[predicted_class],\n",
        "                'confidence': confidence,\n",
        "                'probabilities': {\n",
        "                    label: prob.item() \n",
        "                    for label, prob in zip(self.sentiment_labels, probabilities[0])\n",
        "                }\n",
        "            }\n",
        "    \n",
        "    def get_model_info(self) -> dict:\n",
        "        \"\"\"Return model architecture information.\"\"\"\n",
        "        total_params = sum(p.numel() for p in self.parameters())\n",
        "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        \n",
        "        return {\n",
        "            'model_name': 'Australian Tourism Sentiment Classifier',\n",
        "            'vocab_size': self.vocab_size,\n",
        "            'embed_dim': self.embed_dim,\n",
        "            'hidden_dim': self.hidden_dim,\n",
        "            'num_classes': self.num_classes,\n",
        "            'total_params': total_params,\n",
        "            'trainable_params': trainable_params,\n",
        "            'target_cities': ', '.join(self.australian_cities[:4]) + '...'\n",
        "        }\n",
        "\n",
        "# Initialize the model with device-aware setup\n",
        "model = AustralianTourismSentimentClassifier(\n",
        "    vocab_size=preprocessor.vocab_size,\n",
        "    embed_dim=128,\n",
        "    hidden_dim=256,\n",
        "    num_classes=3,\n",
        "    dropout_rate=0.3\n",
        ").to(DEVICE)\n",
        "\n",
        "# Display model information\n",
        "model_info = model.get_model_info()\n",
        "print(\"\ud83c\udfd7\ufe0f Australian Tourism Sentiment Classifier\")\n",
        "print(\"=\" * 50)\n",
        "for key, value in model_info.items():\n",
        "    print(f\"   {key.replace('_', ' ').title()}: {value}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udcf1 Model device: {next(model.parameters()).device}\")\n",
        "print(f\"\ud83c\udfaf Target: Classify sentiment of Australian tourism reviews\")\n",
        "print(f\"\ud83c\udf0f Languages: English and Vietnamese\")\n",
        "\n",
        "# Model summary (similar to TensorFlow model.summary())\n",
        "print(f\"\\n\ud83d\udd27 Model Architecture:\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Platform-specific TensorBoard log directory setup\n",
        "def get_run_logdir(name=\"australian_sentiment\"):\n",
        "    \"\"\"Create unique log directory for TensorBoard.\"\"\"\n",
        "    if IS_COLAB:\n",
        "        root_logdir = \"/content/tensorboard_logs\"\n",
        "    elif IS_KAGGLE:\n",
        "        root_logdir = \"./tensorboard_logs\"\n",
        "    else:\n",
        "        root_logdir = \"./tensorboard_logs\"\n",
        "    \n",
        "    os.makedirs(root_logdir, exist_ok=True)\n",
        "    timestamp = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
        "    return os.path.join(root_logdir, f\"{name}_{timestamp}\")\n",
        "\n",
        "# Setup training configuration\n",
        "class TrainingConfig:\n",
        "    \"\"\"Training configuration for Australian sentiment classifier.\"\"\"\n",
        "    \n",
        "    def __init__(self, device):\n",
        "        self.device = device\n",
        "        \n",
        "        # Adjust hyperparameters based on device\n",
        "        if device.type == 'cuda':\n",
        "            self.epochs = 20\n",
        "            self.learning_rate = 0.001\n",
        "            self.batch_size = 32\n",
        "        elif device.type == 'mps':\n",
        "            self.epochs = 15\n",
        "            self.learning_rate = 0.001\n",
        "            self.batch_size = 16\n",
        "        else:  # CPU\n",
        "            self.epochs = 10\n",
        "            self.learning_rate = 0.002\n",
        "            self.batch_size = 8\n",
        "        \n",
        "        self.weight_decay = 1e-4\n",
        "        self.patience = 5  # Early stopping patience\n",
        "        self.log_interval = 10  # Log every N batches\n",
        "\n",
        "config = TrainingConfig(DEVICE)\n",
        "\n",
        "# Setup loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # TensorFlow: loss='sparse_categorical_crossentropy'\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(), \n",
        "    lr=config.learning_rate, \n",
        "    weight_decay=config.weight_decay\n",
        ")  # TensorFlow: optimizer='adam'\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='max', factor=0.5, patience=3, verbose=True\n",
        ")\n",
        "\n",
        "# TensorBoard setup\n",
        "run_logdir = get_run_logdir(\"australian_tourism_sentiment\")\n",
        "writer = SummaryWriter(log_dir=run_logdir)\n",
        "\n",
        "print(\"\u2699\ufe0f Training Configuration\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"   Device: {config.device}\")\n",
        "print(f\"   Epochs: {config.epochs}\")\n",
        "print(f\"   Learning Rate: {config.learning_rate}\")\n",
        "print(f\"   Batch Size: {config.batch_size}\")\n",
        "print(f\"   Weight Decay: {config.weight_decay}\")\n",
        "print(f\"   Early Stopping Patience: {config.patience}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udcca TensorBoard Logging:\")\n",
        "print(f\"   Log Directory: {run_logdir}\")\n",
        "print(f\"   Logging Interval: Every {config.log_interval} batches\")\n",
        "\n",
        "# Device-specific optimizations\n",
        "if DEVICE.type == 'cuda':\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    print(f\"\\n\ud83d\udd27 CUDA optimizations enabled\")\n",
        "elif DEVICE.type == 'mps':\n",
        "    print(f\"\\n\ud83d\udd27 MPS device detected - optimizing for Apple Silicon\")\n",
        "else:\n",
        "    torch.set_num_threads(torch.get_num_threads())\n",
        "    print(f\"\\n\ud83d\udd27 CPU optimization: Using {torch.get_num_threads()} threads\")\n",
        "\n",
        "print(f\"\\n\ud83d\ude80 Ready to train Australian tourism sentiment classifier!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_australian_sentiment_model(model, train_loader, val_loader, config, criterion, optimizer, scheduler, writer):\n",
        "    \"\"\"\n",
        "    Train the Australian tourism sentiment classifier with comprehensive logging.\n",
        "    \n",
        "    TensorFlow equivalent:\n",
        "        history = model.fit(\n",
        "            train_data, epochs=epochs, \n",
        "            validation_data=val_data,\n",
        "            callbacks=[tensorboard_callback]\n",
        "        )\n",
        "    \n",
        "    Args:\n",
        "        model: PyTorch model to train\n",
        "        train_loader: Training data loader\n",
        "        val_loader: Validation data loader\n",
        "        config: Training configuration\n",
        "        criterion: Loss function\n",
        "        optimizer: Optimizer\n",
        "        scheduler: Learning rate scheduler\n",
        "        writer: TensorBoard writer\n",
        "    \n",
        "    Returns:\n",
        "        dict: Training history with metrics\n",
        "    \"\"\"\n",
        "    \n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': [],\n",
        "        'learning_rates': []\n",
        "    }\n",
        "    \n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    \n",
        "    print(\"\ud83c\udfcb\ufe0f Training Australian Tourism Sentiment Classifier\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\ud83d\udcdd Sample predictions target:\")\n",
        "    print(f\"   \ud83c\udde6\ud83c\uddfa 'Sydney Opera House is amazing!' -> Positive\")\n",
        "    print(f\"   \ud83c\uddfb\ud83c\uddf3 'C\u00e0 ph\u00ea Melbourne \u0111\u1eaft qu\u00e1' -> Negative\")\n",
        "    print(f\"   \ud83c\udde6\ud83c\uddfa 'Perth beaches are okay' -> Neutral\")\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    \n",
        "    for epoch in range(config.epochs):\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        \n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            # Move data to device\n",
        "            data, target = data.to(config.device), target.to(config.device)\n",
        "            \n",
        "            # Zero gradients (required in PyTorch, automatic in TensorFlow)\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, target)\n",
        "            \n",
        "            # Backward pass (explicit in PyTorch)\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            \n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Calculate metrics\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += target.size(0)\n",
        "            train_correct += (predicted == target).sum().item()\n",
        "            \n",
        "            # Log to TensorBoard every N batches\n",
        "            if batch_idx % config.log_interval == 0:\n",
        "                step = epoch * len(train_loader) + batch_idx\n",
        "                writer.add_scalar('Loss/Train_Batch', loss.item(), step)\n",
        "                writer.add_scalar('Accuracy/Train_Batch', \n",
        "                                (predicted == target).float().mean().item(), step)\n",
        "                \n",
        "                # Log device-specific metrics\n",
        "                if config.device.type == 'cuda':\n",
        "                    gpu_memory_used = torch.cuda.memory_allocated(config.device) / 1024**3\n",
        "                    writer.add_scalar('Memory/GPU_Used_GB', gpu_memory_used, step)\n",
        "        \n",
        "        # Calculate epoch training metrics\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        train_acc = train_correct / train_total\n",
        "        \n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        all_predictions = []\n",
        "        all_targets = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for data, target in val_loader:\n",
        "                data, target = data.to(config.device), target.to(config.device)\n",
        "                outputs = model(data)\n",
        "                loss = criterion(outputs, target)\n",
        "                \n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += target.size(0)\n",
        "                val_correct += (predicted == target).sum().item()\n",
        "                \n",
        "                # Store for detailed metrics\n",
        "                all_predictions.extend(predicted.cpu().numpy())\n",
        "                all_targets.extend(target.cpu().numpy())\n",
        "        \n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_acc = val_correct / val_total\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        \n",
        "        # Update learning rate scheduler\n",
        "        scheduler.step(val_acc)\n",
        "        \n",
        "        # Store history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['learning_rates'].append(current_lr)\n",
        "        \n",
        "        # Log epoch metrics to TensorBoard\n",
        "        writer.add_scalar('Loss/Train_Epoch', train_loss, epoch)\n",
        "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
        "        writer.add_scalar('Accuracy/Train_Epoch', train_acc, epoch)\n",
        "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
        "        writer.add_scalar('Learning_Rate', current_lr, epoch)\n",
        "        \n",
        "        # Log model parameters histogram\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                writer.add_histogram(f'Parameters/{name}', param, epoch)\n",
        "                writer.add_histogram(f'Gradients/{name}', param.grad, epoch)\n",
        "        \n",
        "        # Calculate epoch time\n",
        "        epoch_time = time.time() - start_time\n",
        "        \n",
        "        # Print progress\n",
        "        print(f'Epoch {epoch+1:2d}/{config.epochs}: '\n",
        "              f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | '\n",
        "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} | '\n",
        "              f'LR: {current_lr:.6f} | Time: {epoch_time:.1f}s')\n",
        "        \n",
        "        # Early stopping check\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            torch.save(model.state_dict(), 'best_australian_sentiment_model.pth')\n",
        "            print(f'   \ud83c\udfaf New best validation accuracy: {best_val_acc:.4f}')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= config.patience:\n",
        "                print(f'\\n\u23f0 Early stopping triggered after {epoch+1} epochs')\n",
        "                print(f'   Best validation accuracy: {best_val_acc:.4f}')\n",
        "                break\n",
        "    \n",
        "    writer.close()\n",
        "    \n",
        "    print(f\"\\n\ud83c\udf89 Training completed!\")\n",
        "    print(f\"   Final validation accuracy: {val_acc:.4f}\")\n",
        "    print(f\"   Best validation accuracy: {best_val_acc:.4f}\")\n",
        "    print(f\"   Model saved as: best_australian_sentiment_model.pth\")\n",
        "    \n",
        "    return history\n",
        "\n",
        "# Start training\n",
        "print(\"\ud83d\ude80 Starting training of Australian Tourism Sentiment Classifier...\")\n",
        "training_history = train_australian_sentiment_model(\n",
        "    model, train_loader, val_loader, config, criterion, optimizer, scheduler, writer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Results Visualization\n",
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    Plot training metrics with Australian tourism context.\n",
        "    \n",
        "    Uses seaborn for better aesthetics as per repository guidelines.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create training metrics DataFrame for seaborn\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    \n",
        "    # Prepare data for seaborn\n",
        "    metrics_data = []\n",
        "    for epoch in epochs:\n",
        "        idx = epoch - 1\n",
        "        metrics_data.extend([\n",
        "            {'Epoch': epoch, 'Metric': 'Loss', 'Dataset': 'Train', 'Value': history['train_loss'][idx]},\n",
        "            {'Epoch': epoch, 'Metric': 'Loss', 'Dataset': 'Validation', 'Value': history['val_loss'][idx]},\n",
        "            {'Epoch': epoch, 'Metric': 'Accuracy', 'Dataset': 'Train', 'Value': history['train_acc'][idx]},\n",
        "            {'Epoch': epoch, 'Metric': 'Accuracy', 'Dataset': 'Validation', 'Value': history['val_acc'][idx]}\n",
        "        ])\n",
        "    \n",
        "    df_metrics = pd.DataFrame(metrics_data)\n",
        "    \n",
        "    # Create subplots with seaborn styling\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    \n",
        "    # Loss plot\n",
        "    loss_data = df_metrics[df_metrics['Metric'] == 'Loss']\n",
        "    sns.lineplot(data=loss_data, x='Epoch', y='Value', hue='Dataset', ax=axes[0])\n",
        "    axes[0].set_title('\ud83c\udde6\ud83c\uddfa Australian Tourism Sentiment Analysis - Training Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Accuracy plot\n",
        "    acc_data = df_metrics[df_metrics['Metric'] == 'Accuracy']\n",
        "    sns.lineplot(data=acc_data, x='Epoch', y='Value', hue='Dataset', ax=axes[1])\n",
        "    axes[1].set_title('\ud83c\udfaf Model Accuracy Progress', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_ylabel('Accuracy')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Learning rate plot\n",
        "    axes[2].plot(epochs, history['learning_rates'], 'o-', color='orange', linewidth=2, markersize=4)\n",
        "    axes[2].set_title('\ud83d\udcc9 Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
        "    axes[2].set_xlabel('Epoch')\n",
        "    axes[2].set_ylabel('Learning Rate')\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "    axes[2].set_yscale('log')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print final metrics\n",
        "    final_train_acc = history['train_acc'][-1]\n",
        "    final_val_acc = history['val_acc'][-1]\n",
        "    best_val_acc = max(history['val_acc'])\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcca Final Training Results:\")\n",
        "    print(f\"   Final Training Accuracy: {final_train_acc:.4f} ({final_train_acc*100:.2f}%)\")\n",
        "    print(f\"   Final Validation Accuracy: {final_val_acc:.4f} ({final_val_acc*100:.2f}%)\")\n",
        "    print(f\"   Best Validation Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
        "    \n",
        "    # Model performance assessment\n",
        "    if best_val_acc > 0.8:\n",
        "        print(f\"   \ud83c\udf89 Excellent performance for Australian tourism sentiment analysis!\")\n",
        "    elif best_val_acc > 0.7:\n",
        "        print(f\"   \u2705 Good performance, ready for Australian tourism applications!\")\n",
        "    else:\n",
        "        print(f\"   \u26a0\ufe0f  Model may need more training or data for optimal performance.\")\n",
        "\n",
        "# Plot training results\n",
        "plot_training_history(training_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the trained model with new Australian tourism examples\n",
        "def test_australian_sentiment_model():\n",
        "    \"\"\"\n",
        "    Test the trained model with fresh Australian tourism examples.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Load the best model\n",
        "    model.load_state_dict(torch.load('best_australian_sentiment_model.pth', map_location=DEVICE))\n",
        "    model.eval()\n",
        "    \n",
        "    # New test examples (not seen during training)\n",
        "    test_examples = [\n",
        "        # English examples\n",
        "        {\n",
        "            'text': \"The Sydney Harbour Bridge climb was absolutely incredible! Spectacular views of the entire city!\",\n",
        "            'language': 'English',\n",
        "            'expected': 'positive'\n",
        "        },\n",
        "        {\n",
        "            'text': \"Melbourne's weather ruined our entire vacation. Constantly raining and cold.\",\n",
        "            'language': 'English', \n",
        "            'expected': 'negative'\n",
        "        },\n",
        "        {\n",
        "            'text': \"Perth has some nice beaches but the city center is quite basic.\",\n",
        "            'language': 'English',\n",
        "            'expected': 'neutral'\n",
        "        },\n",
        "        \n",
        "        # Vietnamese examples\n",
        "        {\n",
        "            'text': \"Th\u00e0nh ph\u1ed1 Brisbane r\u1ea5t s\u1ea1ch s\u1ebd v\u00e0 th\u00e2n thi\u1ec7n v\u1edbi du kh\u00e1ch!\",\n",
        "            'language': 'Vietnamese',\n",
        "            'expected': 'positive'\n",
        "        },\n",
        "        {\n",
        "            'text': \"Gi\u00e1 c\u1ea3 \u1edf Adelaide qu\u00e1 \u0111\u1eaft \u0111\u1ecf, kh\u00f4ng \u0111\u00e1ng v\u1edbi ch\u1ea5t l\u01b0\u1ee3ng d\u1ecbch v\u1ee5.\",\n",
        "            'language': 'Vietnamese',\n",
        "            'expected': 'negative'\n",
        "        },\n",
        "        {\n",
        "            'text': \"Darwin c\u00f3 kh\u00ed h\u1eadu \u1ea5m \u00e1p nh\u01b0ng kh\u00f4ng c\u00f3 nhi\u1ec1u ho\u1ea1t \u0111\u1ed9ng gi\u1ea3i tr\u00ed.\",\n",
        "            'language': 'Vietnamese',\n",
        "            'expected': 'neutral'\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    print(\"\ud83e\uddea Testing Australian Tourism Sentiment Classifier\")\n",
        "    print(\"=\" * 65)\n",
        "    \n",
        "    correct_predictions = 0\n",
        "    total_predictions = len(test_examples)\n",
        "    \n",
        "    for i, example in enumerate(test_examples):\n",
        "        # Encode the text\n",
        "        encoded_text = preprocessor.encode_text(example['text'])\n",
        "        text_tensor = torch.tensor([encoded_text], dtype=torch.long)\n",
        "        \n",
        "        # Get prediction\n",
        "        prediction = model.predict_sentiment(text_tensor, preprocessor)\n",
        "        \n",
        "        # Check if prediction matches expected\n",
        "        is_correct = prediction['sentiment'] == example['expected']\n",
        "        if is_correct:\n",
        "            correct_predictions += 1\n",
        "        \n",
        "        # Display results\n",
        "        status_emoji = \"\u2705\" if is_correct else \"\u274c\"\n",
        "        confidence_bar = \"\u2588\" * int(prediction['confidence'] * 10)\n",
        "        \n",
        "        print(f\"\\n{i+1}. {status_emoji} {example['language']} Text Analysis:\")\n",
        "        print(f\"   \ud83d\udcdd Text: {example['text'][:60]}{'...' if len(example['text']) > 60 else ''}\")\n",
        "        print(f\"   \ud83c\udfaf Expected: {example['expected'].capitalize()}\")\n",
        "        print(f\"   \ud83e\udd16 Predicted: {prediction['sentiment'].capitalize()} ({prediction['confidence']:.3f})\")\n",
        "        print(f\"   \ud83d\udcca Confidence: {confidence_bar} {prediction['confidence']*100:.1f}%\")\n",
        "        \n",
        "        # Show all probabilities\n",
        "        print(f\"   \ud83d\udcc8 All Probabilities:\")\n",
        "        for sentiment, prob in prediction['probabilities'].items():\n",
        "            print(f\"      {sentiment.capitalize()}: {prob:.3f}\")\n",
        "    \n",
        "    # Final test accuracy\n",
        "    test_accuracy = correct_predictions / total_predictions\n",
        "    print(f\"\\n\ud83c\udfaf Test Accuracy: {correct_predictions}/{total_predictions} = {test_accuracy:.3f} ({test_accuracy*100:.1f}%)\")\n",
        "    \n",
        "    if test_accuracy >= 0.8:\n",
        "        print(f\"\ud83c\udf1f Excellent! Model performs very well on Australian tourism sentiment analysis.\")\n",
        "    elif test_accuracy >= 0.6:\n",
        "        print(f\"\ud83d\udc4d Good performance! Model shows solid understanding of Australian tourism sentiment.\")\n",
        "    else:\n",
        "        print(f\"\u26a0\ufe0f  Model needs improvement for reliable Australian tourism sentiment classification.\")\n",
        "    \n",
        "    return test_accuracy\n",
        "\n",
        "# Run model testing\n",
        "test_accuracy = test_australian_sentiment_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TensorBoard Visualization Instructions\n",
        "print(\"\ud83d\udcca TENSORBOARD VISUALIZATION\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Log directory: {run_logdir}\")\n",
        "print(\"\\n\ud83d\ude80 To view TensorBoard:\")\n",
        "\n",
        "if IS_COLAB:\n",
        "    print(\"   In Google Colab:\")\n",
        "    print(\"   1. Run: %load_ext tensorboard\")\n",
        "    print(f\"   2. Run: %tensorboard --logdir {run_logdir}\")\n",
        "    print(\"   3. TensorBoard will appear inline in the notebook\")\n",
        "    \n",
        "    # Auto-load TensorBoard in Colab\n",
        "    try:\n",
        "        %load_ext tensorboard\n",
        "        %tensorboard --logdir {run_logdir}\n",
        "    except:\n",
        "        print(\"   Note: Run the commands above manually in Colab\")\n",
        "        \n",
        "elif IS_KAGGLE:\n",
        "    print(\"   In Kaggle:\")\n",
        "    print(f\"   1. Download logs from: {run_logdir}\")\n",
        "    print(\"   2. Run locally: tensorboard --logdir ./tensorboard_logs\")\n",
        "    print(\"   3. Open http://localhost:6006 in browser\")\n",
        "else:\n",
        "    print(\"   Locally:\")\n",
        "    print(f\"   1. Run: tensorboard --logdir {run_logdir}\")\n",
        "    print(\"   2. Open http://localhost:6006 in browser\")\n",
        "\n",
        "print(\"\\n\ud83d\udcc8 Available visualizations:\")\n",
        "print(\"   \u2022 Scalars: Loss, accuracy, learning rate over time\")\n",
        "print(\"   \u2022 Histograms: Model parameter distributions\")\n",
        "print(\"   \u2022 Training Progress: Batch-level and epoch-level metrics\")\n",
        "print(\"   \u2022 Memory Usage: GPU memory utilization (if available)\")\n",
        "print(\"   \u2022 Australian Context: Tourism sentiment analysis progress\")\n",
        "\n",
        "print(\"\\n\ud83d\udd0d Key metrics to examine:\")\n",
        "print(\"   \ud83d\udcc9 Loss curves: Should decrease over time\")\n",
        "print(\"   \ud83d\udcc8 Accuracy curves: Should increase and converge\")\n",
        "print(\"   \ud83c\udf9b\ufe0f  Learning rate: Should adapt based on validation performance\")\n",
        "print(\"   \ud83d\udcca Parameter histograms: Should show healthy distributions\")\n",
        "\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd04 TensorFlow vs PyTorch: Key Differences Summary\n",
        "\n",
        "This section summarizes the key differences between TensorFlow and PyTorch approaches demonstrated in this notebook, helping with the transition from TensorFlow to PyTorch for NLP applications.\n",
        "\n",
        "### Model Definition\n",
        "\n",
        "**TensorFlow (Keras)**:\n",
        "```python\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embed_dim),\n",
        "    tf.keras.layers.LSTM(128, return_sequences=False),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "```\n",
        "\n",
        "**PyTorch**:\n",
        "```python\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, 128, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.classifier = nn.Linear(128, 3)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
        "        output = self.classifier(self.dropout(hidden[-1]))\n",
        "        return output\n",
        "```\n",
        "\n",
        "### Training Loop\n",
        "\n",
        "**TensorFlow**:\n",
        "```python\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(train_data, epochs=10, validation_data=val_data)\n",
        "```\n",
        "\n",
        "**PyTorch**:\n",
        "```python\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        outputs = model(batch.data)\n",
        "        loss = criterion(outputs, batch.targets)\n",
        "        loss.backward()  # Compute gradients\n",
        "        optimizer.step()  # Update parameters\n",
        "```\n",
        "\n",
        "### Key Differences Table\n",
        "\n",
        "| Aspect | TensorFlow | PyTorch |\n",
        "|--------|------------|----------|\n",
        "| **Model Definition** | Sequential/Functional API | Object-oriented with `nn.Module` |\n",
        "| **Training** | `model.fit()` (automatic) | Manual training loop |\n",
        "| **Gradients** | Automatic with `GradientTape` | Manual with `loss.backward()` |\n",
        "| **Graph Execution** | Static graph (TF 1.x) / Eager (TF 2.x) | Dynamic graph (always eager) |\n",
        "| **Device Management** | `with tf.device()` context | `.to(device)` method calls |\n",
        "| **Debugging** | More challenging (especially TF 1.x) | Easier with standard Python debugging |\n",
        "| **Flexibility** | High-level API focus | More explicit control |\n",
        "| **Learning Curve** | Steeper initially, easier for simple models | Gentler, more intuitive for Python developers |\n",
        "\n",
        "### Advantages of Each Framework\n",
        "\n",
        "**TensorFlow Advantages**:\n",
        "- \ud83d\ude80 **Production Ready**: TensorFlow Serving, TensorFlow Lite\n",
        "- \ud83d\udcf1 **Mobile/Edge**: Better mobile and edge deployment\n",
        "- \ud83d\udd27 **High-level APIs**: Keras makes simple models very easy\n",
        "- \ud83d\udcca **Visualization**: Native TensorBoard integration\n",
        "- \ud83c\udfed **Ecosystem**: Mature production ecosystem\n",
        "\n",
        "**PyTorch Advantages**:\n",
        "- \ud83d\udc0d **Pythonic**: More intuitive for Python developers\n",
        "- \ud83d\udd2c **Research**: Preferred for research and experimentation\n",
        "- \ud83d\udc1b **Debugging**: Easier debugging with standard Python tools\n",
        "- \u26a1 **Dynamic**: Dynamic computational graphs\n",
        "- \ud83c\udfaf **Control**: More explicit control over training process\n",
        "\n",
        "### When to Choose PyTorch\n",
        "\n",
        "Choose PyTorch for Australian NLP projects when:\n",
        "- \ud83d\udd2c **Research & Experimentation**: Trying new architectures or techniques\n",
        "- \ud83c\udf93 **Learning**: Understanding deep learning fundamentals\n",
        "- \ud83e\udd17 **NLP Focus**: Working with Hugging Face transformers ecosystem\n",
        "- \ud83d\udc0d **Python Preference**: Team prefers explicit, Pythonic code\n",
        "- \ud83d\udd27 **Custom Models**: Building complex, custom neural architectures\n",
        "\n",
        "### Transition Tips\n",
        "\n",
        "For TensorFlow developers moving to PyTorch:\n",
        "\n",
        "1. **Start with `nn.Module`**: Think of it as a more explicit version of Keras layers\n",
        "2. **Manual Training Loops**: Initially more code, but more control and transparency\n",
        "3. **Device Management**: Explicitly move tensors and models to GPU/CPU\n",
        "4. **Gradient Management**: Always call `optimizer.zero_grad()` before `loss.backward()`\n",
        "5. **Debugging**: Use standard Python debugging tools and `print()` statements\n",
        "6. **Hugging Face**: PyTorch integrates seamlessly with modern NLP libraries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udf89 Congratulations! Deep Learning NLP with PyTorch Complete\n",
        "\n",
        "You've successfully completed a comprehensive introduction to deep learning for NLP using PyTorch with Australian context examples and English-Vietnamese multilingual support!\n",
        "\n",
        "### \ud83c\udfc6 What You've Accomplished\n",
        "\n",
        "\u2705 **PyTorch Fundamentals**: Mastered tensors, models, and training loops for NLP\n",
        "\n",
        "\u2705 **Australian Context**: Built a sentiment classifier for Australian tourism reviews\n",
        "\n",
        "\u2705 **Multilingual NLP**: Handled both English and Vietnamese text processing\n",
        "\n",
        "\u2705 **Neural Networks**: Implemented LSTM-based architecture from scratch\n",
        "\n",
        "\u2705 **Training Pipeline**: Created complete training loop with validation and early stopping\n",
        "\n",
        "\u2705 **Visualization**: Integrated TensorBoard for comprehensive training monitoring\n",
        "\n",
        "\u2705 **Device Optimization**: Implemented device-aware training (CUDA/MPS/CPU)\n",
        "\n",
        "\u2705 **TensorFlow Transition**: Learned key differences and migration strategies\n",
        "\n",
        "### \ud83d\udcca Model Performance\n",
        "\n",
        "Your Australian Tourism Sentiment Classifier can now:\n",
        "- \ud83c\udde6\ud83c\uddfa Analyze sentiment in Australian tourism reviews\n",
        "- \ud83c\uddfb\ud83c\uddf3 Process Vietnamese translations and reviews\n",
        "- \ud83c\udfaf Classify text as positive, neutral, or negative sentiment\n",
        "- \ud83d\udcf1 Run efficiently on various devices (GPU, Apple Silicon, CPU)\n",
        "- \ud83d\udcca Provide confidence scores and probability distributions\n",
        "\n",
        "### \ud83d\ude80 Next Steps in Your PyTorch NLP Journey\n",
        "\n",
        "Continue your learning with the remaining notebooks in this series:\n",
        "\n",
        "#### 1. \ud83d\udd24 Word Embeddings (`02_word_embeddings_nllp.ipynb`)\n",
        "- **Focus**: Deep dive into word representation techniques\n",
        "- **Australian Context**: Train embeddings on Australian tourism corpus\n",
        "- **Multilingual**: English-Vietnamese word alignment and cross-lingual embeddings\n",
        "- **Techniques**: Word2Vec, GloVe, FastText, and visualization\n",
        "\n",
        "#### 2. \ud83d\udd04 Sequence Models (`03_sequence_models_nlp.ipynb`)\n",
        "- **Focus**: Advanced LSTM, GRU, and attention mechanisms\n",
        "- **Australian Context**: Part-of-speech tagging and named entity recognition\n",
        "- **Multilingual**: Sequence-to-sequence translation models\n",
        "- **Techniques**: Bidirectional RNNs, attention, and seq2seq architectures\n",
        "\n",
        "#### 3. \ud83d\ude80 Advanced NLP (`04_advanced_nlp.ipynb`)\n",
        "- **Focus**: Bi-LSTM CRF and state-of-the-art techniques\n",
        "- **Australian Context**: Named entity recognition for Australian locations\n",
        "- **Integration**: Bridge to Hugging Face transformers\n",
        "- **Techniques**: CRF layers, dynamic computation graphs, and modern architectures\n",
        "\n",
        "### \ud83d\udee0\ufe0f Practical Applications\n",
        "\n",
        "Apply your new skills to real Australian NLP projects:\n",
        "\n",
        "- **Tourism Analysis**: Analyze TripAdvisor reviews for Australian destinations\n",
        "- **Social Media**: Monitor sentiment about Australian events and locations\n",
        "- **Customer Service**: Build multilingual chatbots for Australian businesses\n",
        "- **News Analysis**: Process Australian news articles in multiple languages\n",
        "- **E-commerce**: Analyze product reviews for Australian retailers\n",
        "\n",
        "### \ud83d\udcda Additional Resources\n",
        "\n",
        "Expand your PyTorch NLP knowledge:\n",
        "\n",
        "- \ud83d\udd17 [PyTorch NLP Tutorials](https://pytorch.org/tutorials/beginner/nlp/)\n",
        "- \ud83e\udd17 [Hugging Face Transformers](https://huggingface.co/transformers/)\n",
        "- \ud83d\udcd6 [Natural Language Processing with PyTorch](https://www.oreilly.com/library/view/natural-language-processing/9781491978221/)\n",
        "- \ud83c\udf93 [Stanford CS224N: NLP with Deep Learning](http://web.stanford.edu/class/cs224n/)\n",
        "- \ud83c\udde6\ud83c\uddfa [Australian Text Analytics Platform](https://www.atap.edu.au/)\n",
        "\n",
        "### \ud83e\udd1d Community and Support\n",
        "\n",
        "Join the PyTorch and NLP community:\n",
        "\n",
        "- \ud83d\udcac [PyTorch Forums](https://discuss.pytorch.org/)\n",
        "- \ud83d\udc26 [PyTorch Twitter](https://twitter.com/pytorch)\n",
        "- \ud83d\udce7 [Hugging Face Discord](https://discord.gg/JfAtkvEtRb)\n",
        "- \ud83d\udcf1 [r/MachineLearning](https://www.reddit.com/r/MachineLearning/)\n",
        "\n",
        "### \ud83c\udf1f Keep Experimenting!\n",
        "\n",
        "The best way to master PyTorch NLP is through hands-on practice:\n",
        "\n",
        "1. **Modify the Model**: Try different architectures, hyperparameters, and optimizers\n",
        "2. **Expand the Dataset**: Add more Australian tourism data or other domains\n",
        "3. **Add Languages**: Incorporate other languages beyond English and Vietnamese\n",
        "4. **Deploy Models**: Create APIs and web applications with your trained models\n",
        "5. **Contribute**: Share your Australian NLP models and datasets with the community\n",
        "\n",
        "---\n",
        "\n",
        "**\ud83c\udf8a Congratulations on completing your first PyTorch NLP project with Australian flair! You're now ready to tackle real-world natural language processing challenges with confidence. \ud83c\udde6\ud83c\uddfa\ud83d\ude80**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}