{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PyTorch Neural Networks Basics: Australian Tourism Sentiment Analysis \ud83c\udde6\ud83c\uddfa\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/pytorch-mastery/blob/main/examples/pytorch-nlp/neural-network-basic.ipynb)\n",
        "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/pytorch-mastery/blob/main/examples/pytorch-nlp/neural-network-basic.ipynb)\n",
        "\n",
        "A comprehensive introduction to PyTorch neural networks through Australian tourism sentiment analysis with multilingual support (English + Vietnamese). This notebook demonstrates basic neural network concepts using real-world NLP applications.\n",
        "\n",
        "## Learning Objectives\n",
        "- Build and train basic neural networks with PyTorch `nn.Module`\n",
        "- Implement sentiment classification for Australian tourism reviews\n",
        "- Master PyTorch training loops vs TensorFlow's `model.fit()`\n",
        "- Handle multilingual text data (English + Vietnamese)\n",
        "- Use TensorBoard for comprehensive training monitoring\n",
        "- Apply Australian context to NLP tasks (Sydney, Melbourne, Brisbane)\n",
        "\n",
        "## What We'll Build\n",
        "- **Binary Sentiment Classifier**: Positive/Negative Australian tourism reviews\n",
        "- **Multi-class City Classifier**: Sydney/Melbourne/Brisbane/Perth location detection\n",
        "- **Multilingual Support**: Process English and Vietnamese tourism content\n",
        "- **Complete Training Pipeline**: Data loading, training, validation, visualization\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup and Runtime Detection\n",
        "\n",
        "Following PyTorch mastery repository standards for cross-platform compatibility:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment Detection and Setup\n",
        "import sys\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Detect the runtime environment\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules or \"kaggle\" in os.environ.get('KAGGLE_URL_BASE', '')\n",
        "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
        "\n",
        "print(f\"Environment detected:\")\n",
        "print(f\"  - Local: {IS_LOCAL}\")\n",
        "print(f\"  - Google Colab: {IS_COLAB}\")\n",
        "print(f\"  - Kaggle: {IS_KAGGLE}\")\n",
        "\n",
        "# Platform-specific system setup\n",
        "if IS_COLAB:\n",
        "    print(\"\\nSetting up Google Colab environment...\")\n",
        "    !apt update -qq\n",
        "    !apt install -y -qq software-properties-common\n",
        "elif IS_KAGGLE:\n",
        "    print(\"\\nSetting up Kaggle environment...\")\n",
        "    # Kaggle usually has most packages pre-installed\n",
        "else:\n",
        "    print(\"\\nSetting up local environment...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages for this notebook\n",
        "required_packages = [\n",
        "    \"torch\",\n",
        "    \"torchvision\", \n",
        "    \"torchaudio\",\n",
        "    \"pandas\",\n",
        "    \"seaborn\",\n",
        "    \"matplotlib\",\n",
        "    \"numpy\",\n",
        "    \"scikit-learn\",\n",
        "    \"tensorboard\"\n",
        "]\n",
        "\n",
        "print(\"Installing required packages...\")\n",
        "for package in required_packages:\n",
        "    if IS_COLAB or IS_KAGGLE:\n",
        "        !pip install -q {package}\n",
        "    else:\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
        "                      capture_output=True)\n",
        "    print(f\"\u2713 {package}\")\n",
        "\n",
        "print(\"\\n\u2705 Package installation completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries and Device Detection\n",
        "\n",
        "Following repository guidelines for consistent imports and device handling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports following repository standards\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F  # Standard alias for functional operations\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Data science and visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns  # Primary visualization library for notebooks\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Text processing\n",
        "import re\n",
        "import string\n",
        "from collections import Counter, defaultdict\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "import platform\n",
        "import datetime\n",
        "\n",
        "# Set seaborn style for better notebook aesthetics\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "# Verify PyTorch installation\n",
        "print(f\"\u2705 PyTorch {torch.__version__} ready!\")\n",
        "print(f\"\u2705 All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Device Detection with comprehensive hardware support\n",
        "def detect_device():\n",
        "    \"\"\"\n",
        "    Detect the best available PyTorch device with comprehensive hardware support.\n",
        "    \n",
        "    Priority order:\n",
        "    1. CUDA (NVIDIA GPUs) - Best performance for deep learning\n",
        "    2. MPS (Apple Silicon) - Optimized for M1/M2/M3 Macs  \n",
        "    3. CPU (Universal) - Always available fallback\n",
        "    \n",
        "    Returns:\n",
        "        torch.device: The optimal device for PyTorch operations\n",
        "        str: Human-readable device description for logging\n",
        "    \"\"\"\n",
        "    # Check for CUDA (NVIDIA GPU)\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        device_info = f\"CUDA GPU: {gpu_name}\"\n",
        "        \n",
        "        # Additional CUDA info for optimization\n",
        "        cuda_version = torch.version.cuda\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "        \n",
        "        print(f\"\ud83d\ude80 Using CUDA acceleration\")\n",
        "        print(f\"   GPU: {gpu_name}\")\n",
        "        print(f\"   CUDA Version: {cuda_version}\")\n",
        "        print(f\"   GPU Memory: {gpu_memory:.1f} GB\")\n",
        "        \n",
        "        return device, device_info\n",
        "    \n",
        "    # Check for MPS (Apple Silicon)\n",
        "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "        device_info = \"Apple Silicon MPS\"\n",
        "        \n",
        "        # Get system info for Apple Silicon\n",
        "        system_info = platform.uname()\n",
        "        \n",
        "        print(f\"\ud83c\udf4e Using Apple Silicon MPS acceleration\")\n",
        "        print(f\"   System: {system_info.system} {system_info.release}\")\n",
        "        print(f\"   Machine: {system_info.machine}\")\n",
        "        print(f\"   Processor: {system_info.processor}\")\n",
        "        \n",
        "        return device, device_info\n",
        "    \n",
        "    # Fallback to CPU\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        device_info = \"CPU (No GPU acceleration available)\"\n",
        "        \n",
        "        # Get CPU info for optimization guidance\n",
        "        cpu_count = torch.get_num_threads()\n",
        "        system_info = platform.uname()\n",
        "        \n",
        "        print(f\"\ud83d\udcbb Using CPU (no GPU acceleration detected)\")\n",
        "        print(f\"   Processor: {system_info.processor}\")\n",
        "        print(f\"   PyTorch Threads: {cpu_count}\")\n",
        "        print(f\"   System: {system_info.system} {system_info.release}\")\n",
        "        \n",
        "        # Provide optimization suggestions for CPU-only setups\n",
        "        print(f\"\\n\ud83d\udca1 CPU Optimization Tips:\")\n",
        "        print(f\"   \u2022 Reduce batch size to prevent memory issues\")\n",
        "        print(f\"   \u2022 Consider using smaller models for faster training\")\n",
        "        print(f\"   \u2022 Enable PyTorch optimizations: torch.set_num_threads({cpu_count})\")\n",
        "        \n",
        "        return device, device_info\n",
        "\n",
        "# Usage in notebook\n",
        "device, device_info = detect_device()\n",
        "print(f\"\\n\u2705 PyTorch device selected: {device}\")\n",
        "print(f\"\ud83d\udcca Device info: {device_info}\")\n",
        "\n",
        "# Set global device for the notebook\n",
        "DEVICE = device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Australian Tourism Dataset Creation\n",
        "\n",
        "Creating multilingual Australian tourism sentiment data with English and Vietnamese examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Australian Tourism Sentiment Dataset\n",
        "# Following repository guidelines: Australian context with English-Vietnamese multilingual support\n",
        "\n",
        "australian_tourism_data = {\n",
        "    'english_reviews': [\n",
        "        # Positive reviews - Sydney (label: 1)\n",
        "        \"The Sydney Opera House tour was absolutely breathtaking! A must-see landmark.\",\n",
        "        \"Harbour Bridge climb exceeded all expectations. Sydney views are stunning!\",\n",
        "        \"Bondi Beach is perfect for swimming and surfing. Love the Australian lifestyle!\",\n",
        "        \"Circular Quay area is vibrant with amazing restaurants and street performers.\",\n",
        "        \"The Royal Botanic Gardens offer peaceful walks with Opera House views.\",\n",
        "        \n",
        "        # Positive reviews - Melbourne (label: 1)\n",
        "        \"Melbourne's coffee culture is world-class. Every caf\u00e9 serves excellent brews.\",\n",
        "        \"The laneways are filled with incredible street art and hidden gem restaurants.\",\n",
        "        \"Queen Victoria Market has amazing fresh produce and local crafts.\",\n",
        "        \"St Kilda penguin parade at sunset is a magical wildlife experience.\",\n",
        "        \"Melbourne's food scene is diverse and incredibly delicious.\",\n",
        "        \n",
        "        # Positive reviews - Brisbane (label: 1)\n",
        "        \"South Bank Parklands is a beautiful urban oasis with great river views.\",\n",
        "        \"Brisbane River cruise offers spectacular city skyline perspectives.\",\n",
        "        \"The Gold Coast beaches are pristine with perfect surfing conditions.\",\n",
        "        \"Story Bridge climb provides amazing 360-degree city views.\",\n",
        "        \"Brisbane's subtropical climate makes outdoor activities enjoyable year-round.\",\n",
        "        \n",
        "        # Negative reviews - Sydney (label: 0)\n",
        "        \"Sydney accommodation prices are extremely expensive and overpriced.\",\n",
        "        \"Circular Quay was overcrowded with tourists, very disappointing experience.\",\n",
        "        \"The weather was terrible during our Sydney visit, rained constantly.\",\n",
        "        \"Public transport in Sydney is confusing and unreliable.\",\n",
        "        \"Sydney restaurant prices are outrageous, not worth the money.\",\n",
        "        \n",
        "        # Negative reviews - Melbourne (label: 0)\n",
        "        \"Melbourne weather is unpredictable and ruined our outdoor plans.\",\n",
        "        \"The city center feels congested with construction everywhere.\",\n",
        "        \"Melbourne restaurants are pretentious and overpriced for mediocre food.\",\n",
        "        \"Public transport delays made us late for every appointment.\",\n",
        "        \"The hotel service in Melbourne was poor and unprofessional.\",\n",
        "        \n",
        "        # Negative reviews - Brisbane (label: 0)\n",
        "        \"Brisbane is boring compared to other Australian cities.\",\n",
        "        \"The heat and humidity in Brisbane is unbearable during summer.\",\n",
        "        \"Gold Coast beaches are overcrowded and polluted with tourists.\",\n",
        "        \"Brisbane nightlife is limited and closes too early.\",\n",
        "        \"Public facilities in Brisbane need major improvements and maintenance.\"\n",
        "    ],\n",
        "    \n",
        "    'vietnamese_reviews': [\n",
        "        # Positive reviews - Vietnamese translations (label: 1)\n",
        "        \"Tour Nh\u00e0 h\u00e1t Opera Sydney th\u1eadt tuy\u1ec7t v\u1eddi! L\u00e0 \u0111i\u1ec3m tham quan kh\u00f4ng th\u1ec3 b\u1ecf qua.\",\n",
        "        \"Leo c\u1ea7u Harbour Bridge v\u01b0\u1ee3t qu\u00e1 mong \u0111\u1ee3i. C\u1ea3nh Sydney th\u1eadt \u0111\u1eb9p!\",\n",
        "        \"B\u00e3i bi\u1ec3n Bondi ho\u00e0n h\u1ea3o \u0111\u1ec3 b\u01a1i l\u1ed9i v\u00e0 l\u01b0\u1edbt s\u00f3ng. Y\u00eau l\u1ed1i s\u1ed1ng \u00dac!\",\n",
        "        \"Khu v\u1ef1c Circular Quay s\u00f4i \u0111\u1ed9ng v\u1edbi nh\u00e0 h\u00e0ng v\u00e0 ngh\u1ec7 s\u0129 \u0111\u01b0\u1eddng ph\u1ed1 tuy\u1ec7t v\u1eddi.\",\n",
        "        \"V\u01b0\u1eddn B\u00e1ch th\u1ea3o Ho\u00e0ng gia c\u00f3 \u0111\u01b0\u1eddng \u0111i b\u1ed9 y\u00ean b\u00ecnh v\u1edbi view Nh\u00e0 h\u00e1t Opera.\",\n",
        "        \n",
        "        \"V\u0103n h\u00f3a c\u00e0 ph\u00ea Melbourne \u0111\u1eb3ng c\u1ea5p th\u1ebf gi\u1edbi. M\u1ecdi qu\u00e1n \u0111\u1ec1u pha ch\u1ebf tuy\u1ec7t v\u1eddi.\",\n",
        "        \"Nh\u1eefng con h\u1ebbm \u0111\u1ea7y ngh\u1ec7 thu\u1eadt \u0111\u01b0\u1eddng ph\u1ed1 v\u00e0 nh\u00e0 h\u00e0ng \u1ea9n m\u00ecnh tuy\u1ec7t v\u1eddi.\",\n",
        "        \"Ch\u1ee3 Queen Victoria c\u00f3 n\u00f4ng s\u1ea3n t\u01b0\u01a1i ngon v\u00e0 th\u1ee7 c\u00f4ng m\u1ef9 ngh\u1ec7 \u0111\u1ecba ph\u01b0\u01a1ng.\",\n",
        "        \"Cu\u1ed9c di\u1ec5u h\u00e0nh chim c\u00e1nh c\u1ee5t St Kilda l\u00fac ho\u00e0ng h\u00f4n th\u1eadt k\u1ef3 di\u1ec7u.\",\n",
        "        \"\u1ea8m th\u1ef1c Melbourne \u0111a d\u1ea1ng v\u00e0 c\u1ef1c k\u1ef3 ngon mi\u1ec7ng.\",\n",
        "        \n",
        "        \"C\u00f4ng vi\u00ean South Bank l\u00e0 \u1ed1c \u0111\u1ea3o \u0111\u00f4 th\u1ecb xinh \u0111\u1eb9p v\u1edbi view s\u00f4ng tuy\u1ec7t v\u1eddi.\",\n",
        "        \"Du thuy\u1ec1n s\u00f4ng Brisbane mang \u0111\u1ebfn g\u00f3c nh\u00ecn tuy\u1ec7t v\u1eddi v\u1ec1 \u0111\u01b0\u1eddng ch\u00e2n tr\u1eddi.\",\n",
        "        \"B\u00e3i bi\u1ec3n Gold Coast nguy\u00ean s\u01a1 v\u1edbi \u0111i\u1ec1u ki\u1ec7n l\u01b0\u1edbt s\u00f3ng ho\u00e0n h\u1ea3o.\",\n",
        "        \"Leo c\u1ea7u Story Bridge c\u00f3 view 360 \u0111\u1ed9 th\u00e0nh ph\u1ed1 tuy\u1ec7t v\u1eddi.\",\n",
        "        \"Kh\u00ed h\u1eadu c\u1eadn nhi\u1ec7t \u0111\u1edbi Brisbane l\u00e0m cho ho\u1ea1t \u0111\u1ed9ng ngo\u00e0i tr\u1eddi th\u00fa v\u1ecb quanh n\u0103m.\",\n",
        "        \n",
        "        # Negative reviews - Vietnamese translations (label: 0)\n",
        "        \"Gi\u00e1 ch\u1ed7 \u1edf Sydney c\u1ef1c k\u1ef3 \u0111\u1eaft \u0111\u1ecf v\u00e0 qu\u00e1 m\u1ee9c.\",\n",
        "        \"Circular Quay \u0111\u00f4ng ngh\u1eb9t kh\u00e1ch du l\u1ecbch, tr\u1ea3i nghi\u1ec7m th\u1ea5t v\u1ecdng.\",\n",
        "        \"Th\u1eddi ti\u1ebft t\u1ec7 trong chuy\u1ebfn th\u0103m Sydney, m\u01b0a li\u00ean t\u1ee5c.\",\n",
        "        \"Giao th\u00f4ng c\u00f4ng c\u1ed9ng Sydney kh\u00f3 hi\u1ec3u v\u00e0 kh\u00f4ng \u0111\u00e1ng tin c\u1eady.\",\n",
        "        \"Gi\u00e1 nh\u00e0 h\u00e0ng Sydney qu\u00e1 \u0111\u1eaft, kh\u00f4ng x\u1ee9ng \u0111\u00e1ng v\u1edbi \u0111\u1ed3ng ti\u1ec1n.\",\n",
        "        \n",
        "        \"Th\u1eddi ti\u1ebft Melbourne kh\u00f3 \u0111o\u00e1n v\u00e0 l\u00e0m h\u1ecfng k\u1ebf ho\u1ea1ch ngo\u00e0i tr\u1eddi.\",\n",
        "        \"Trung t\u00e2m th\u00e0nh ph\u1ed1 c\u1ea3m th\u1ea5y t\u1eafc ngh\u1ebdn v\u1edbi c\u00f4ng tr\u00ecnh x\u00e2y d\u1ef1ng kh\u1eafp n\u01a1i.\",\n",
        "        \"Nh\u00e0 h\u00e0ng Melbourne ki\u00eau c\u0103ng v\u00e0 \u0111\u1eaft \u0111\u1ecf cho \u0111\u1ed3 \u0103n t\u1ea7m th\u01b0\u1eddng.\",\n",
        "        \"Giao th\u00f4ng c\u00f4ng c\u1ed9ng ch\u1eadm tr\u1ec5 l\u00e0m ch\u00fang t\u00f4i tr\u1ec5 m\u1ecdi cu\u1ed9c h\u1eb9n.\",\n",
        "        \"D\u1ecbch v\u1ee5 kh\u00e1ch s\u1ea1n Melbourne k\u00e9m v\u00e0 kh\u00f4ng chuy\u00ean nghi\u1ec7p.\",\n",
        "        \n",
        "        \"Brisbane nh\u00e0m ch\u00e1n so v\u1edbi c\u00e1c th\u00e0nh ph\u1ed1 \u00dac kh\u00e1c.\",\n",
        "        \"C\u00e1i n\u00f3ng v\u00e0 \u0111\u1ed9 \u1ea9m \u1edf Brisbane kh\u00f4ng ch\u1ecbu n\u1ed5i v\u00e0o m\u00f9a h\u00e8.\",\n",
        "        \"B\u00e3i bi\u1ec3n Gold Coast \u0111\u00f4ng ngh\u1eb9t v\u00e0 \u00f4 nhi\u1ec5m v\u1edbi kh\u00e1ch du l\u1ecbch.\",\n",
        "        \"Cu\u1ed9c s\u1ed1ng v\u1ec1 \u0111\u00eam Brisbane h\u1ea1n ch\u1ebf v\u00e0 \u0111\u00f3ng c\u1eeda qu\u00e1 s\u1edbm.\",\n",
        "        \"C\u01a1 s\u1edf c\u00f4ng c\u1ed9ng Brisbane c\u1ea7n c\u1ea3i thi\u1ec7n v\u00e0 b\u1ea3o tr\u00ec l\u1edbn.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create labels: 1 for positive (first 15 each language), 0 for negative (last 15 each language)\n",
        "labels = [1] * 15 + [0] * 15  # English labels\n",
        "labels_vi = [1] * 15 + [0] * 15  # Vietnamese labels\n",
        "\n",
        "# Combine all texts and labels\n",
        "all_texts = australian_tourism_data['english_reviews'] + australian_tourism_data['vietnamese_reviews']\n",
        "all_labels = labels + labels_vi\n",
        "\n",
        "# Create DataFrame for easier manipulation\n",
        "tourism_df = pd.DataFrame({\n",
        "    'text': all_texts,\n",
        "    'label': all_labels,\n",
        "    'language': ['en'] * len(labels) + ['vi'] * len(labels_vi),\n",
        "    'city': (\n",
        "        ['Sydney'] * 5 + ['Melbourne'] * 5 + ['Brisbane'] * 5 +  # Positive English\n",
        "        ['Sydney'] * 5 + ['Melbourne'] * 5 + ['Brisbane'] * 5 +  # Negative English\n",
        "        ['Sydney'] * 5 + ['Melbourne'] * 5 + ['Brisbane'] * 5 +  # Positive Vietnamese\n",
        "        ['Sydney'] * 5 + ['Melbourne'] * 5 + ['Brisbane'] * 5    # Negative Vietnamese\n",
        "    )\n",
        "})\n",
        "\n",
        "print(f\"\ud83c\udde6\ud83c\uddfa Australian Tourism Dataset Created:\")\n",
        "print(f\"   \ud83d\udcca Total samples: {len(tourism_df)}\")\n",
        "print(f\"   \ud83c\udf0f Languages: {tourism_df['language'].value_counts().to_dict()}\")\n",
        "print(f\"   \ud83d\ude0a Sentiment distribution: {tourism_df['label'].value_counts().to_dict()}\")\n",
        "print(f\"   \ud83c\udfd9\ufe0f City distribution: {tourism_df['city'].value_counts().to_dict()}\")\n",
        "\n",
        "# Display sample data\n",
        "print(f\"\\n\ud83d\udcdd Sample data:\")\n",
        "display(tourism_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Simple Neural Network for Text Classification\n",
        "\n",
        "Building a basic neural network using PyTorch's `nn.Module` with OOP design patterns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple Text Processing Helper Functions\n",
        "\n",
        "def simple_tokenize(text: str) -> List[str]:\n",
        "    \"\"\"Simple tokenization by splitting on whitespace and removing punctuation.\"\"\"\n",
        "    # Convert to lowercase and remove punctuation\n",
        "    text = text.lower()\n",
        "    for punct in string.punctuation:\n",
        "        text = text.replace(punct, ' ')\n",
        "    # Split and filter empty strings\n",
        "    return [word for word in text.split() if word]\n",
        "\n",
        "def build_vocab(texts: List[str], max_vocab_size: int = 5000) -> Dict[str, int]:\n",
        "    \"\"\"Build vocabulary from texts.\"\"\"\n",
        "    # Count all words\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        tokens = simple_tokenize(text)\n",
        "        word_counts.update(tokens)\n",
        "    \n",
        "    # Create vocabulary with most frequent words\n",
        "    vocab = {'<UNK>': 0, '<PAD>': 1}  # Special tokens\n",
        "    most_common = word_counts.most_common(max_vocab_size - 2)\n",
        "    \n",
        "    for i, (word, count) in enumerate(most_common):\n",
        "        vocab[word] = i + 2\n",
        "    \n",
        "    print(f\"\ud83d\udcda Built vocabulary: {len(vocab)} words\")\n",
        "    print(f\"   Most common words: {[word for word, _ in most_common[:10]]}\")\n",
        "    \n",
        "    return vocab\n",
        "\n",
        "def text_to_sequence(text: str, vocab: Dict[str, int], max_length: int = 50) -> List[int]:\n",
        "    \"\"\"Convert text to sequence of token IDs.\"\"\"\n",
        "    tokens = simple_tokenize(text)\n",
        "    sequence = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
        "    \n",
        "    # Pad or truncate to max_length\n",
        "    if len(sequence) > max_length:\n",
        "        sequence = sequence[:max_length]\n",
        "    else:\n",
        "        sequence.extend([vocab['<PAD>']] * (max_length - len(sequence)))\n",
        "    \n",
        "    return sequence\n",
        "\n",
        "# Build vocabulary from our tourism data\n",
        "vocab = build_vocab(tourism_df['text'].tolist(), max_vocab_size=3000)\n",
        "vocab_size = len(vocab)\n",
        "max_length = 50\n",
        "\n",
        "print(f\"\\n\ud83d\udd24 Text preprocessing setup:\")\n",
        "print(f\"   Vocabulary size: {vocab_size}\")\n",
        "print(f\"   Max sequence length: {max_length}\")\n",
        "\n",
        "# Test tokenization\n",
        "sample_text = \"The Sydney Opera House is absolutely beautiful!\"\n",
        "sample_sequence = text_to_sequence(sample_text, vocab, max_length)\n",
        "print(f\"\\n\ud83e\uddea Sample tokenization:\")\n",
        "print(f\"   Text: '{sample_text}'\")\n",
        "print(f\"   Tokens: {simple_tokenize(sample_text)}\")\n",
        "print(f\"   Sequence (first 10): {sample_sequence[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BasicSentimentClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic neural network for Australian tourism sentiment classification.\n",
        "    \n",
        "    Architecture:\n",
        "    - Embedding layer: Maps word IDs to dense vectors\n",
        "    - Global Average Pooling: Averages word embeddings to get sentence representation\n",
        "    - Hidden layers: Two fully connected layers with ReLU activation\n",
        "    - Output layer: Binary classification (positive/negative sentiment)\n",
        "    \n",
        "    This demonstrates fundamental PyTorch neural network concepts:\n",
        "    - Inheriting from nn.Module\n",
        "    - Defining layers in __init__\n",
        "    - Implementing forward pass\n",
        "    - Using functional operations (F.relu, F.dropout)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size: int, embed_dim: int = 64, hidden_dim: int = 128, \n",
        "                 output_dim: int = 2, dropout_rate: float = 0.3):\n",
        "        super(BasicSentimentClassifier, self).__init__()\n",
        "        \n",
        "        # Store hyperparameters\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "        \n",
        "        # Embedding layer - converts token IDs to dense vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab.get('<PAD>', 1))\n",
        "        \n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
        "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
        "        \n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        \n",
        "        # Australian cities for context (just for fun!)\n",
        "        self.australian_cities = [\"Sydney\", \"Melbourne\", \"Brisbane\", \"Perth\", \"Adelaide\"]\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of token IDs, shape (batch_size, sequence_length)\n",
        "            \n",
        "        Returns:\n",
        "            Logits for sentiment classification, shape (batch_size, output_dim)\n",
        "        \"\"\"\n",
        "        # 1. Embedding lookup: (batch_size, seq_len) -> (batch_size, seq_len, embed_dim)\n",
        "        embedded = self.embedding(x)\n",
        "        \n",
        "        # 2. Global average pooling: average across sequence dimension\n",
        "        # This gives us a fixed-size representation regardless of input length\n",
        "        pooled = embedded.mean(dim=1)  # (batch_size, embed_dim)\n",
        "        \n",
        "        # 3. First hidden layer with ReLU activation and dropout\n",
        "        hidden1 = F.relu(self.fc1(pooled))\n",
        "        hidden1 = self.dropout(hidden1)\n",
        "        \n",
        "        # 4. Second hidden layer with ReLU activation and dropout\n",
        "        hidden2 = F.relu(self.fc2(hidden1))\n",
        "        hidden2 = self.dropout(hidden2)\n",
        "        \n",
        "        # 5. Output layer (no activation - will use CrossEntropyLoss)\n",
        "        logits = self.fc3(hidden2)\n",
        "        \n",
        "        return logits\n",
        "    \n",
        "    def predict_sentiment(self, text: str, vocab: Dict[str, int], device: torch.device) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Predict sentiment for a single text with Australian context analysis.\n",
        "        \n",
        "        Args:\n",
        "            text: Input text to analyze\n",
        "            vocab: Vocabulary dictionary\n",
        "            device: PyTorch device\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with prediction results and Australian context\n",
        "        \"\"\"\n",
        "        self.eval()  # Set to evaluation mode\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # Convert text to sequence\n",
        "            sequence = text_to_sequence(text, vocab, max_length)\n",
        "            input_tensor = torch.tensor([sequence], dtype=torch.long).to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            logits = self.forward(input_tensor)\n",
        "            probabilities = F.softmax(logits, dim=1)\n",
        "            \n",
        "            # Get prediction\n",
        "            predicted_class = logits.argmax(dim=1).item()\n",
        "            confidence = probabilities[0, predicted_class].item()\n",
        "            \n",
        "            # Check for Australian cities mentioned\n",
        "            mentioned_cities = [city for city in self.australian_cities if city.lower() in text.lower()]\n",
        "            \n",
        "            return {\n",
        "                'text': text,\n",
        "                'predicted_sentiment': 'Positive' if predicted_class == 1 else 'Negative',\n",
        "                'confidence': confidence,\n",
        "                'probabilities': {\n",
        "                    'negative': probabilities[0, 0].item(),\n",
        "                    'positive': probabilities[0, 1].item()\n",
        "                },\n",
        "                'australian_cities_mentioned': mentioned_cities,\n",
        "                'has_australian_context': len(mentioned_cities) > 0\n",
        "            }\n",
        "    \n",
        "    def get_model_info(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get model architecture information.\"\"\"\n",
        "        total_params = sum(p.numel() for p in self.parameters())\n",
        "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        \n",
        "        return {\n",
        "            'architecture': 'Basic Feedforward Neural Network',\n",
        "            'vocab_size': self.vocab_size,\n",
        "            'embed_dim': self.embed_dim,\n",
        "            'hidden_dim': self.hidden_dim,\n",
        "            'output_dim': self.output_dim,\n",
        "            'dropout_rate': self.dropout_rate,\n",
        "            'total_parameters': total_params,\n",
        "            'trainable_parameters': trainable_params,\n",
        "            'target_task': 'Australian Tourism Sentiment Analysis',\n",
        "            'supported_languages': ['English', 'Vietnamese']\n",
        "        }\n",
        "\n",
        "# Create model instance\n",
        "model = BasicSentimentClassifier(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=64,\n",
        "    hidden_dim=128,\n",
        "    output_dim=2,  # Binary classification: negative (0) and positive (1)\n",
        "    dropout_rate=0.3\n",
        ").to(DEVICE)\n",
        "\n",
        "# Display model information\n",
        "model_info = model.get_model_info()\n",
        "print(f\"\ud83e\udde0 Basic Sentiment Classifier Created:\")\n",
        "print(f\"   Architecture: {model_info['architecture']}\")\n",
        "print(f\"   Total parameters: {model_info['total_parameters']:,}\")\n",
        "print(f\"   Vocabulary size: {model_info['vocab_size']:,}\")\n",
        "print(f\"   Embedding dimension: {model_info['embed_dim']}\")\n",
        "print(f\"   Hidden dimension: {model_info['hidden_dim']}\")\n",
        "print(f\"   Target task: {model_info['target_task']}\")\n",
        "print(f\"   Supported languages: {', '.join(model_info['supported_languages'])}\")\n",
        "print(f\"   Device: {DEVICE}\")\n",
        "\n",
        "# Display model architecture\n",
        "print(f\"\\n\ud83d\udcd0 Model Architecture:\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for training\n",
        "print(\"\ud83c\udfd7\ufe0f Preparing training data...\")\n",
        "\n",
        "# Convert all texts to sequences\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for idx, row in tourism_df.iterrows():\n",
        "    sequence = text_to_sequence(row['text'], vocab, max_length)\n",
        "    X.append(sequence)\n",
        "    y.append(row['label'])\n",
        "\n",
        "# Convert to tensors\n",
        "X_tensor = torch.tensor(X, dtype=torch.long)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "print(f\"   \ud83d\udcca Data shape: X={X_tensor.shape}, y={y_tensor.shape}\")\n",
        "print(f\"   \ud83c\udfaf Label distribution: {Counter(y)}\")\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tensor, y_tensor, \n",
        "    test_size=0.2, \n",
        "    random_state=42, \n",
        "    stratify=y_tensor  # Ensure balanced split\n",
        ")\n",
        "\n",
        "print(f\"   \ud83d\ude82 Train set: {X_train.shape[0]} samples\")\n",
        "print(f\"   \ud83e\uddea Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "# Create DataLoaders for batch processing\n",
        "batch_size = 8 if DEVICE.type == 'cpu' else 16\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"   \ud83d\udd04 Batch size: {batch_size} (optimized for {DEVICE.type.upper()})\")\n",
        "print(f\"   \ud83d\udce6 Training batches: {len(train_loader)}\")\n",
        "print(f\"   \ud83d\udce6 Test batches: {len(test_loader)}\")\n",
        "\n",
        "# Test data loading\n",
        "sample_batch_X, sample_batch_y = next(iter(train_loader))\n",
        "print(f\"\\n\ud83e\uddea Sample batch:\")\n",
        "print(f\"   Input shape: {sample_batch_X.shape}\")\n",
        "print(f\"   Label shape: {sample_batch_y.shape}\")\n",
        "print(f\"   Sample labels: {sample_batch_y.tolist()[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Setup and Configuration\n",
        "\n",
        "Setting up the training process with loss function, optimizer, and TensorBoard logging:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Configuration\n",
        "# Following repository standards for comprehensive training setup\n",
        "\n",
        "# Training hyperparameters\n",
        "num_epochs = 20\n",
        "learning_rate = 0.001\n",
        "weight_decay = 1e-5\n",
        "\n",
        "# Loss function for binary classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer with weight decay for regularization\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# Learning rate scheduler for better training\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.5)\n",
        "\n",
        "# TensorBoard setup for monitoring (required by repository standards)\n",
        "log_dir = f\"runs/australian_tourism_sentiment_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "writer = SummaryWriter(log_dir)\n",
        "\n",
        "print(f\"\ud83d\udd27 Training Configuration:\")\n",
        "print(f\"   Epochs: {num_epochs}\")\n",
        "print(f\"   Learning rate: {learning_rate}\")\n",
        "print(f\"   Batch size: {batch_size}\")\n",
        "print(f\"   Optimizer: Adam with weight decay {weight_decay}\")\n",
        "print(f\"   Loss function: CrossEntropyLoss\")\n",
        "print(f\"   Device: {DEVICE}\")\n",
        "print(f\"   \ud83d\udcca TensorBoard logs: {log_dir}\")\n",
        "\n",
        "# Log model architecture to TensorBoard\n",
        "sample_input = X_train[:1].to(DEVICE)\n",
        "writer.add_graph(model, sample_input)\n",
        "\n",
        "# Log hyperparameters\n",
        "hyperparams = {\n",
        "    'learning_rate': learning_rate,\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': num_epochs,\n",
        "    'embed_dim': model.embed_dim,\n",
        "    'hidden_dim': model.hidden_dim,\n",
        "    'dropout_rate': model.dropout_rate,\n",
        "    'vocab_size': vocab_size,\n",
        "    'max_length': max_length\n",
        "}\n",
        "\n",
        "print(f\"\\n\ud83d\udccb Hyperparameters logged to TensorBoard:\")\n",
        "for key, value in hyperparams.items():\n",
        "    print(f\"   {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. PyTorch Training Loop\n",
        "\n",
        "Implementing a manual training loop (contrasted with TensorFlow's `model.fit()`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual Training Loop - Key difference from TensorFlow\n",
        "# TensorFlow: model.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_test, y_test))\n",
        "# PyTorch: Manual implementation with explicit forward/backward passes\n",
        "\n",
        "def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, \n",
        "                num_epochs, device, writer):\n",
        "    \"\"\"\n",
        "    Train the sentiment classification model with comprehensive logging.\n",
        "    \n",
        "    This demonstrates the key differences between PyTorch and TensorFlow:\n",
        "    - Manual epoch and batch loops\n",
        "    - Explicit gradient zeroing: optimizer.zero_grad()\n",
        "    - Manual backward pass: loss.backward()\n",
        "    - Manual optimizer step: optimizer.step()\n",
        "    - Explicit model mode switching: model.train() / model.eval()\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"\ud83d\ude80 Starting Training: Australian Tourism Sentiment Analysis\")\n",
        "    print(f\"\ud83d\udcca Training samples: {len(train_loader.dataset)}\")\n",
        "    print(f\"\ud83d\udcca Test samples: {len(test_loader.dataset)}\")\n",
        "    print(f\"\ud83c\udfaf Target: Binary sentiment classification (Positive/Negative)\")\n",
        "    print(f\"\ud83c\udf0f Context: Multilingual Australian tourism reviews\")\n",
        "    print(f\"\u26a1 Device: {device}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    # Training history storage\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'test_loss': [],\n",
        "        'test_acc': [],\n",
        "        'learning_rates': []\n",
        "    }\n",
        "    \n",
        "    best_test_acc = 0.0\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # Training Phase\n",
        "        model.train()  # Set model to training mode (enables dropout, batch norm)\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        \n",
        "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "            # Move data to device\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            \n",
        "            # Zero gradients (REQUIRED in PyTorch, automatic in TensorFlow)\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            \n",
        "            # Backward pass (explicit in PyTorch)\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Track statistics\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += targets.size(0)\n",
        "            train_correct += predicted.eq(targets).sum().item()\n",
        "            \n",
        "            # Log batch-level metrics to TensorBoard\n",
        "            if batch_idx % 5 == 0:\n",
        "                global_step = epoch * len(train_loader) + batch_idx\n",
        "                writer.add_scalar('Loss/Train_Batch', loss.item(), global_step)\n",
        "        \n",
        "        # Calculate epoch training metrics\n",
        "        epoch_train_loss = train_loss / len(train_loader)\n",
        "        epoch_train_acc = train_correct / train_total\n",
        "        \n",
        "        # Validation/Test Phase\n",
        "        model.eval()  # Set model to evaluation mode (disables dropout)\n",
        "        test_loss = 0.0\n",
        "        test_correct = 0\n",
        "        test_total = 0\n",
        "        \n",
        "        with torch.no_grad():  # Disable gradient computation for efficiency\n",
        "            for data, targets in test_loader:\n",
        "                data, targets = data.to(device), targets.to(device)\n",
        "                outputs = model(data)\n",
        "                loss = criterion(outputs, targets)\n",
        "                \n",
        "                test_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                test_total += targets.size(0)\n",
        "                test_correct += predicted.eq(targets).sum().item()\n",
        "        \n",
        "        # Calculate epoch test metrics\n",
        "        epoch_test_loss = test_loss / len(test_loader)\n",
        "        epoch_test_acc = test_correct / test_total\n",
        "        \n",
        "        # Update learning rate scheduler\n",
        "        scheduler.step()\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        \n",
        "        # Store history\n",
        "        history['train_loss'].append(epoch_train_loss)\n",
        "        history['train_acc'].append(epoch_train_acc)\n",
        "        history['test_loss'].append(epoch_test_loss)\n",
        "        history['test_acc'].append(epoch_test_acc)\n",
        "        history['learning_rates'].append(current_lr)\n",
        "        \n",
        "        # Log epoch-level metrics to TensorBoard\n",
        "        writer.add_scalar('Loss/Train', epoch_train_loss, epoch)\n",
        "        writer.add_scalar('Loss/Test', epoch_test_loss, epoch)\n",
        "        writer.add_scalar('Accuracy/Train', epoch_train_acc, epoch)\n",
        "        writer.add_scalar('Accuracy/Test', epoch_test_acc, epoch)\n",
        "        writer.add_scalar('Learning_Rate', current_lr, epoch)\n",
        "        \n",
        "        # Log parameter histograms (every 5 epochs)\n",
        "        if epoch % 5 == 0:\n",
        "            for name, param in model.named_parameters():\n",
        "                writer.add_histogram(f'Parameters/{name}', param, epoch)\n",
        "        \n",
        "        # Save best model\n",
        "        if epoch_test_acc > best_test_acc:\n",
        "            best_test_acc = epoch_test_acc\n",
        "            torch.save(model.state_dict(), 'best_sentiment_model.pth')\n",
        "        \n",
        "        # Print progress\n",
        "        print(f'Epoch {epoch+1:2d}/{num_epochs} | '\n",
        "              f'Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f} | '\n",
        "              f'Test Loss: {epoch_test_loss:.4f} | Test Acc: {epoch_test_acc:.4f} | '\n",
        "              f'LR: {current_lr:.6f}')\n",
        "        \n",
        "        # Early stopping if perfect accuracy\n",
        "        if epoch_test_acc >= 0.99:\n",
        "            print(f\"\ud83c\udf89 Early stopping: Achieved {epoch_test_acc:.1%} test accuracy!\")\n",
        "            break\n",
        "    \n",
        "    writer.close()\n",
        "    \n",
        "    print(f\"\\n\ud83c\udfc1 Training Complete!\")\n",
        "    print(f\"   \ud83d\udcc8 Best test accuracy: {best_test_acc:.4f} ({best_test_acc:.1%})\")\n",
        "    print(f\"   \ud83d\udcbe Best model saved as: best_sentiment_model.pth\")\n",
        "    print(f\"   \ud83d\udcca TensorBoard logs: {log_dir}\")\n",
        "    \n",
        "    return history\n",
        "\n",
        "# Start training\n",
        "training_history = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    test_loader=test_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    num_epochs=num_epochs,\n",
        "    device=DEVICE,\n",
        "    writer=writer\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training Results and Visualization\n",
        "\n",
        "Using seaborn for training metrics visualization (following repository standards):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Results Visualization with Seaborn\n",
        "# Following repository guidelines for seaborn usage in notebooks\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    Plot training metrics using seaborn for better aesthetics.\n",
        "    \n",
        "    Following repository policy: Use seaborn instead of matplotlib when possible.\n",
        "    \"\"\"\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    \n",
        "    # Create DataFrame for seaborn plotting\n",
        "    df_loss = pd.DataFrame({\n",
        "        'Epoch': list(epochs) * 2,\n",
        "        'Loss': history['train_loss'] + history['test_loss'],\n",
        "        'Dataset': ['Train'] * len(epochs) + ['Test'] * len(epochs)\n",
        "    })\n",
        "    \n",
        "    df_acc = pd.DataFrame({\n",
        "        'Epoch': list(epochs) * 2,\n",
        "        'Accuracy': history['train_acc'] + history['test_acc'],\n",
        "        'Dataset': ['Train'] * len(epochs) + ['Test'] * len(epochs)\n",
        "    })\n",
        "    \n",
        "    # Create comprehensive visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('\ud83c\udde6\ud83c\uddfa Australian Tourism Sentiment Analysis - Training Results', \n",
        "                 fontsize=16, fontweight='bold', y=0.98)\n",
        "    \n",
        "    # Loss plot\n",
        "    sns.lineplot(data=df_loss, x='Epoch', y='Loss', hue='Dataset', ax=axes[0,0])\n",
        "    axes[0,0].set_title('Training & Test Loss', fontsize=12, fontweight='bold')\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Accuracy plot\n",
        "    sns.lineplot(data=df_acc, x='Epoch', y='Accuracy', hue='Dataset', ax=axes[0,1])\n",
        "    axes[0,1].set_title('Training & Test Accuracy', fontsize=12, fontweight='bold')\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "    axes[0,1].set_ylim(0, 1)\n",
        "    \n",
        "    # Learning rate plot\n",
        "    axes[1,0].plot(epochs, history['learning_rates'], 'g-', linewidth=2)\n",
        "    axes[1,0].set_title('Learning Rate Schedule', fontsize=12, fontweight='bold')\n",
        "    axes[1,0].set_xlabel('Epoch')\n",
        "    axes[1,0].set_ylabel('Learning Rate')\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "    axes[1,0].set_yscale('log')  # Log scale for better visibility\n",
        "    \n",
        "    # Final metrics summary\n",
        "    final_train_acc = history['train_acc'][-1]\n",
        "    final_test_acc = history['test_acc'][-1]\n",
        "    final_train_loss = history['train_loss'][-1]\n",
        "    final_test_loss = history['test_loss'][-1]\n",
        "    \n",
        "    metrics_text = f\"\"\"\n",
        "Final Training Results:\n",
        "\n",
        "\ud83d\udcca Accuracy:\n",
        "   \u2022 Training: {final_train_acc:.1%}\n",
        "   \u2022 Test: {final_test_acc:.1%}\n",
        "\n",
        "\ud83d\udcc9 Loss:\n",
        "   \u2022 Training: {final_train_loss:.4f}\n",
        "   \u2022 Test: {final_test_loss:.4f}\n",
        "\n",
        "\ud83c\udfaf Model Performance:\n",
        "   \u2022 {'Excellent' if final_test_acc > 0.9 else 'Good' if final_test_acc > 0.8 else 'Fair'} generalization\n",
        "   \u2022 {'Low' if abs(final_train_acc - final_test_acc) < 0.1 else 'Some'} overfitting\n",
        "\n",
        "\ud83c\udde6\ud83c\uddfa Australian Context:\n",
        "   \u2022 Multilingual support \u2713\n",
        "   \u2022 Tourism domain \u2713\n",
        "   \u2022 City-specific analysis \u2713\n",
        "    \"\"\"\n",
        "    \n",
        "    axes[1,1].text(0.05, 0.95, metrics_text.strip(), \n",
        "                   transform=axes[1,1].transAxes, \n",
        "                   verticalalignment='top',\n",
        "                   fontsize=10, \n",
        "                   bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
        "    axes[1,1].set_title('Training Summary', fontsize=12, fontweight='bold')\n",
        "    axes[1,1].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return final_test_acc\n",
        "\n",
        "# Plot training results\n",
        "final_accuracy = plot_training_history(training_history)\n",
        "\n",
        "print(f\"\\n\ud83c\udf89 Training visualization complete!\")\n",
        "print(f\"   Final test accuracy: {final_accuracy:.1%}\")\n",
        "print(f\"   Model ready for predictions on Australian tourism reviews!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Model Testing and Predictions\n",
        "\n",
        "Testing our trained model on new Australian tourism texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the trained model with new Australian tourism examples\n",
        "\n",
        "# Load the best model if available\n",
        "try:\n",
        "    model.load_state_dict(torch.load('best_sentiment_model.pth', map_location=DEVICE))\n",
        "    print(\"\u2705 Best model loaded successfully!\")\n",
        "except:\n",
        "    print(\"\u2139\ufe0f Using current model weights (best model not found)\")\n",
        "\n",
        "# Test samples with Australian context\n",
        "test_samples = [\n",
        "    # English samples\n",
        "    \"The Sydney Opera House concert was absolutely magnificent! World-class acoustics.\",\n",
        "    \"Melbourne traffic is horrible and the weather ruined our vacation completely.\",\n",
        "    \"Brisbane's South Bank is a beautiful place for families to relax and enjoy.\",\n",
        "    \"Perth beaches are overrated and the service at hotels was disappointing.\",\n",
        "    \"Gold Coast theme parks provide amazing entertainment for the whole family!\",\n",
        "    \n",
        "    # Vietnamese samples\n",
        "    \"C\u1ea7u Harbour Bridge \u1edf Sydney th\u1eadt tuy\u1ec7t v\u1eddi v\u00e0 \u0111\u00e1ng \u0111\u1ec3 leo l\u00ean xem c\u1ea3nh!\",\n",
        "    \"Th\u1eddi ti\u1ebft Melbourne qu\u00e1 kh\u00f3 \u0111o\u00e1n v\u00e0 l\u00e0m h\u1ecfng c\u1ea3 chuy\u1ebfn du l\u1ecbch c\u1ee7a ch\u00fang t\u00f4i.\",\n",
        "    \"\u1ea8m th\u1ef1c \u0111\u01b0\u1eddng ph\u1ed1 \u1edf Adelaide r\u1ea5t ngon v\u00e0 gi\u00e1 c\u1ea3 h\u1ee3p l\u00fd.\",\n",
        "    \"D\u1ecbch v\u1ee5 t\u1ea1i kh\u00e1ch s\u1ea1n Darwin r\u1ea5t t\u1ec7 v\u00e0 nh\u00e2n vi\u00ean kh\u00f4ng th\u00e2n thi\u1ec7n.\",\n",
        "    \"V\u01b0\u1eddn th\u00fa Taronga \u1edf Sydney c\u00f3 nhi\u1ec1u \u0111\u1ed9ng v\u1eadt th\u00fa v\u1ecb v\u00e0 view tuy\u1ec7t \u0111\u1eb9p!\"\n",
        "]\n",
        "\n",
        "print(\"\ud83e\uddea Testing Australian Tourism Sentiment Classification:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Test each sample\n",
        "correct_predictions = 0\n",
        "total_predictions = len(test_samples)\n",
        "\n",
        "# Expected sentiments (for manual evaluation)\n",
        "expected_sentiments = ['Positive', 'Negative', 'Positive', 'Negative', 'Positive',\n",
        "                      'Positive', 'Negative', 'Positive', 'Negative', 'Positive']\n",
        "\n",
        "for i, text in enumerate(test_samples):\n",
        "    # Get prediction\n",
        "    result = model.predict_sentiment(text, vocab, DEVICE)\n",
        "    \n",
        "    # Detect language\n",
        "    is_vietnamese = any(char in text for char in '\u00e0\u00e1\u1ea3\u00e3\u1ea1\u0103\u1eaf\u1eb1\u1eb3\u1eb5\u1eb7\u00e2\u1ea5\u1ea7\u1ea9\u1eab\u1ead\u00e8\u00e9\u1ebb\u1ebd\u1eb9\u00ea\u1ebf\u1ec1\u1ec3\u1ec5\u1ec7\u00ec\u00ed\u1ec9\u0129\u1ecb\u00f2\u00f3\u1ecf\u00f5\u1ecd\u00f4\u1ed1\u1ed3\u1ed5\u1ed7\u1ed9\u01a1\u1edb\u1edd\u1edf\u1ee1\u1ee3\u00f9\u00fa\u1ee7\u0169\u1ee5\u01b0\u1ee9\u1eeb\u1eed\u1eef\u1ef1\u1ef3\u00fd\u1ef7\u1ef9\u1ef5\u0111')\n",
        "    language = \"\ud83c\uddfb\ud83c\uddf3 Vietnamese\" if is_vietnamese else \"\ud83c\uddfa\ud83c\uddf8 English\"\n",
        "    \n",
        "    # Check if prediction matches expected\n",
        "    is_correct = result['predicted_sentiment'] == expected_sentiments[i]\n",
        "    if is_correct:\n",
        "        correct_predictions += 1\n",
        "    \n",
        "    # Display results\n",
        "    print(f\"\\nTest {i+1}/10 ({language}):\")\n",
        "    print(f\"   Text: \\\"{text[:60]}{'...' if len(text) > 60 else ''}\\\"\")\n",
        "    print(f\"   Predicted: {result['predicted_sentiment']} ({result['confidence']:.1%} confidence)\")\n",
        "    print(f\"   Expected: {expected_sentiments[i]} {'\u2713' if is_correct else '\u2717'}\")\n",
        "    \n",
        "    if result['australian_cities_mentioned']:\n",
        "        print(f\"   \ud83c\udfd9\ufe0f Australian cities mentioned: {', '.join(result['australian_cities_mentioned'])}\")\n",
        "    \n",
        "    # Show probability breakdown for interesting cases\n",
        "    if abs(result['probabilities']['positive'] - result['probabilities']['negative']) < 0.3:\n",
        "        print(f\"   \ud83d\udcca Close call - Positive: {result['probabilities']['positive']:.1%}, \"\n",
        "              f\"Negative: {result['probabilities']['negative']:.1%}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"\ud83c\udfaf Manual Evaluation Results:\")\n",
        "print(f\"   Accuracy on test samples: {correct_predictions}/{total_predictions} ({correct_predictions/total_predictions:.1%})\")\n",
        "print(f\"   \u2705 Correctly classified: {correct_predictions} samples\")\n",
        "print(f\"   \u274c Misclassified: {total_predictions - correct_predictions} samples\")\n",
        "\n",
        "# Model performance summary\n",
        "print(f\"\\n\ud83e\udde0 Model Performance Summary:\")\n",
        "print(f\"   \ud83d\udcc8 Training accuracy: {training_history['train_acc'][-1]:.1%}\")\n",
        "print(f\"   \ud83d\udcca Test accuracy: {training_history['test_acc'][-1]:.1%}\")\n",
        "print(f\"   \ud83e\uddea Manual test accuracy: {correct_predictions/total_predictions:.1%}\")\n",
        "print(f\"   \ud83c\udf0f Multilingual support: English + Vietnamese \u2713\")\n",
        "print(f\"   \ud83c\udde6\ud83c\uddfa Australian context: Tourism reviews \u2713\")\n",
        "print(f\"   \ud83c\udfd9\ufe0f City detection: Sydney, Melbourne, Brisbane, etc. \u2713\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary and Next Steps\n",
        "\n",
        "### What We Accomplished\n",
        "\n",
        "\ud83c\udf89 **Successfully built and trained a basic neural network for sentiment analysis!**\n",
        "\n",
        "#### Key PyTorch Concepts Learned:\n",
        "- **`nn.Module` Architecture**: Created a custom neural network class with proper inheritance\n",
        "- **Manual Training Loops**: Implemented explicit forward/backward passes vs TensorFlow's `model.fit()`\n",
        "- **Device Management**: Proper CPU/GPU handling with device detection\n",
        "- **Data Processing**: Custom tokenization and dataset creation for multilingual text\n",
        "- **TensorBoard Integration**: Comprehensive training monitoring and visualization\n",
        "\n",
        "#### Australian Context Features:\n",
        "- \ud83c\udde6\ud83c\uddfa **Tourism Domain**: Sentiment analysis for Australian travel reviews\n",
        "- \ud83c\udf0f **Multilingual Support**: English and Vietnamese text processing\n",
        "- \ud83c\udfd9\ufe0f **City Recognition**: Sydney, Melbourne, Brisbane, Perth detection\n",
        "- \ud83d\udcca **Real-world Application**: Practical sentiment classification\n",
        "\n",
        "#### PyTorch vs TensorFlow Key Differences:\n",
        "\n",
        "| Aspect | TensorFlow | PyTorch |\n",
        "|--------|------------|----------|\n",
        "| **Model Definition** | `tf.keras.Sequential` | Custom `nn.Module` classes |\n",
        "| **Training** | `model.fit()` | Manual loops with `loss.backward()` |\n",
        "| **Gradients** | Automatic | Manual `optimizer.zero_grad()` |\n",
        "| **Device Handling** | Mostly automatic | Explicit `.to(device)` |\n",
        "| **Debugging** | Can be complex | Python-native, easier debugging |\n",
        "| **Control** | High-level abstractions | Low-level control |\n",
        "\n",
        "### Next Steps for Advanced Learning:\n",
        "\n",
        "1. **\ud83d\udd04 Recurrent Neural Networks**: LSTMs and GRUs for sequence modeling\n",
        "2. **\ud83e\udd17 Hugging Face Integration**: Pre-trained transformers (BERT, RoBERTa)\n",
        "3. **\ud83d\udcca Advanced Architectures**: Attention mechanisms and Transformers\n",
        "4. **\ud83d\ude80 Model Optimization**: Quantization, pruning, and deployment\n",
        "5. **\ud83c\udf10 Production Deployment**: Model serving and REST APIs\n",
        "\n",
        "### TensorBoard Viewing Instructions:\n",
        "\n",
        "To view the training logs and metrics:\n",
        "\n",
        "```bash\n",
        "# In terminal/command prompt:\n",
        "tensorboard --logdir runs/australian_tourism_sentiment_*\n",
        "\n",
        "# Then open: http://localhost:6006\n",
        "```\n",
        "\n",
        "**Available visualizations:**\n",
        "- \ud83d\udcc8 Loss and accuracy curves\n",
        "- \ud83e\udde0 Model architecture graph\n",
        "- \ud83d\udcca Parameter histograms\n",
        "- \u2699\ufe0f Hyperparameter tracking\n",
        "\n",
        "---\n",
        "\n",
        "**\ud83c\udf93 Congratulations!** You've successfully implemented a neural network in PyTorch with Australian tourism context and multilingual support. This foundation prepares you for more advanced NLP tasks and modern transformer architectures.\n",
        "\n",
        "**\ud83d\udcda Continue your PyTorch journey** with the other notebooks in this repository for deeper neural network concepts and real-world applications!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}