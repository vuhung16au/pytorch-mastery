#!/usr/bin/env python3
"""
Add Word2Vec implementation and training to the embeddings notebook
"""

import json

def add_word2vec_implementation():
    """Add Word2Vec model implementation and training cells."""
    
    # Load existing notebook
    with open("word_embeddings_nllp.ipynb", "r") as f:
        notebook = json.load(f)
    
    # Cell 8: PyTorch Word2Vec Implementation
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "class AustralianWord2Vec(nn.Module):\n",
            "    \"\"\"\n",
            "    PyTorch implementation of Word2Vec Skip-gram model for Australian tourism corpus.\n",
            "    \n",
            "    TensorFlow equivalent:\n",
            "        embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_dim)\n",
            "        \n",
            "    This implementation uses:\n",
            "    - Skip-gram architecture for better rare word representation\n",
            "    - Negative sampling for efficient training\n",
            "    - Australian tourism vocabulary optimization\n",
            "    \n",
            "    Args:\n",
            "        vocab_size (int): Size of vocabulary\n",
            "        embed_dim (int): Embedding dimension (typically 100-300)\n",
            "        context_window (int): Context window size (typically 5-10)\n",
            "    \"\"\"\n",
            "    \n",
            "    def __init__(self, vocab_size, embed_dim=200, context_window=5):\n",
            "        super(AustralianWord2Vec, self).__init__()\n",
            "        \n",
            "        self.vocab_size = vocab_size\n",
            "        self.embed_dim = embed_dim\n",
            "        self.context_window = context_window\n",
            "        \n",
            "        # Input embeddings (center words)\n",
            "        self.in_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
            "        \n",
            "        # Output embeddings (context words)\n",
            "        self.out_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
            "        \n",
            "        # Initialize embeddings with small random values\n",
            "        self._init_embeddings()\n",
            "        \n",
            "        # Store Australian cities for analysis\n",
            "        self.australian_cities = ['sydney', 'melbourne', 'brisbane', 'perth', \n",
            "                                'adelaide', 'darwin', 'hobart', 'canberra']\n",
            "    \n",
            "    def _init_embeddings(self):\n",
            "        \"\"\"Initialize embedding weights.\"\"\"\n",
            "        # Initialize with small random values\n",
            "        nn.init.uniform_(self.in_embeddings.weight, -0.5/self.embed_dim, 0.5/self.embed_dim)\n",
            "        nn.init.uniform_(self.out_embeddings.weight, -0.5/self.embed_dim, 0.5/self.embed_dim)\n",
            "    \n",
            "    def forward(self, center_words, context_words, negative_words=None):\n",
            "        \"\"\"\n",
            "        Forward pass for Word2Vec training.\n",
            "        \n",
            "        Args:\n",
            "            center_words (torch.Tensor): Center word indices [batch_size]\n",
            "            context_words (torch.Tensor): Context word indices [batch_size]\n",
            "            negative_words (torch.Tensor): Negative sample indices [batch_size, num_negative]\n",
            "            \n",
            "        Returns:\n",
            "            torch.Tensor: Loss value\n",
            "        \"\"\"\n",
            "        batch_size = center_words.size(0)\n",
            "        \n",
            "        # Get center word embeddings\n",
            "        center_embeds = self.in_embeddings(center_words)  # [batch_size, embed_dim]\n",
            "        \n",
            "        # Get context word embeddings  \n",
            "        context_embeds = self.out_embeddings(context_words)  # [batch_size, embed_dim]\n",
            "        \n",
            "        # Positive samples score\n",
            "        pos_score = torch.sum(center_embeds * context_embeds, dim=1)  # [batch_size]\n",
            "        pos_loss = -F.logsigmoid(pos_score).mean()\n",
            "        \n",
            "        # Negative sampling loss\n",
            "        neg_loss = 0\n",
            "        if negative_words is not None:\n",
            "            num_negative = negative_words.size(1)\n",
            "            \n",
            "            # Get negative word embeddings\n",
            "            neg_embeds = self.out_embeddings(negative_words)  # [batch_size, num_negative, embed_dim]\n",
            "            \n",
            "            # Compute negative scores\n",
            "            center_embeds_expanded = center_embeds.unsqueeze(1).expand(-1, num_negative, -1)\n",
            "            neg_scores = torch.sum(center_embeds_expanded * neg_embeds, dim=2)  # [batch_size, num_negative]\n",
            "            \n",
            "            neg_loss = -F.logsigmoid(-neg_scores).mean()\n",
            "        \n",
            "        return pos_loss + neg_loss\n",
            "    \n",
            "    def get_word_embeddings(self):\n",
            "        \"\"\"Get trained word embeddings.\"\"\"\n",
            "        return self.in_embeddings.weight.data\n",
            "    \n",
            "    def similarity(self, word1_idx, word2_idx):\n",
            "        \"\"\"Compute cosine similarity between two words.\"\"\"\n",
            "        embeddings = self.get_word_embeddings()\n",
            "        \n",
            "        emb1 = embeddings[word1_idx]\n",
            "        emb2 = embeddings[word2_idx]\n",
            "        \n",
            "        # Cosine similarity\n",
            "        cos_sim = F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0))\n",
            "        return cos_sim.item()\n",
            "    \n",
            "    def most_similar(self, word_idx, word_to_idx, idx_to_word, top_k=10):\n",
            "        \"\"\"Find most similar words to a given word.\"\"\"\n",
            "        embeddings = self.get_word_embeddings()\n",
            "        word_embed = embeddings[word_idx].unsqueeze(0)\n",
            "        \n",
            "        # Compute similarities with all words\n",
            "        similarities = F.cosine_similarity(word_embed, embeddings)\n",
            "        \n",
            "        # Get top-k most similar (excluding the word itself)\n",
            "        similarities[word_idx] = -1  # Exclude the word itself\n",
            "        top_indices = similarities.topk(top_k).indices\n",
            "        \n",
            "        similar_words = []\n",
            "        for idx in top_indices:\n",
            "            word = idx_to_word.get(idx.item(), '<UNK>')\n",
            "            similarity_score = similarities[idx].item()\n",
            "            similar_words.append((word, similarity_score))\n",
            "        \n",
            "        return similar_words\n",
            "\n",
            "print(\"🔤 Australian Word2Vec model class defined!\")\n",
            "print(\"   Architecture: Skip-gram with negative sampling\")\n",
            "print(\"   Optimized for: Australian tourism vocabulary\")\n",
            "print(\"   Features: Similarity computation, most similar words\")"
        ]
    })
    
    # Cell 9: Vocabulary Building and Dataset Creation
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "class Word2VecDataset(Dataset):\n",
            "    \"\"\"\n",
            "    PyTorch Dataset for Word2Vec training with Australian tourism corpus.\n",
            "    \n",
            "    Generates (center_word, context_word) pairs for skip-gram training.\n",
            "    \"\"\"\n",
            "    \n",
            "    def __init__(self, tokenized_corpus, word_to_idx, context_window=5, num_negative=5):\n",
            "        self.tokenized_corpus = tokenized_corpus\n",
            "        self.word_to_idx = word_to_idx\n",
            "        self.idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
            "        self.context_window = context_window\n",
            "        self.num_negative = num_negative\n",
            "        self.vocab_size = len(word_to_idx)\n",
            "        \n",
            "        # Generate training pairs\n",
            "        self.training_pairs = self._generate_training_pairs()\n",
            "        \n",
            "        # Create word frequency table for negative sampling\n",
            "        self.word_freqs = self._build_frequency_table()\n",
            "        \n",
            "    def _generate_training_pairs(self):\n",
            "        \"\"\"Generate (center, context) word pairs.\"\"\"\n",
            "        pairs = []\n",
            "        \n",
            "        for sentence in self.tokenized_corpus:\n",
            "            # Convert words to indices\n",
            "            word_indices = [self.word_to_idx.get(word, self.word_to_idx.get('<UNK>', 0)) \n",
            "                           for word in sentence]\n",
            "            \n",
            "            # Generate context pairs\n",
            "            for center_idx, center_word_idx in enumerate(word_indices):\n",
            "                # Define context window\n",
            "                start = max(0, center_idx - self.context_window)\n",
            "                end = min(len(word_indices), center_idx + self.context_window + 1)\n",
            "                \n",
            "                # Generate pairs\n",
            "                for context_idx in range(start, end):\n",
            "                    if context_idx != center_idx:\n",
            "                        pairs.append((center_word_idx, word_indices[context_idx]))\n",
            "        \n",
            "        return pairs\n",
            "    \n",
            "    def _build_frequency_table(self):\n",
            "        \"\"\"Build word frequency table for negative sampling.\"\"\"\n",
            "        word_counts = Counter()\n",
            "        \n",
            "        for sentence in self.tokenized_corpus:\n",
            "            for word in sentence:\n",
            "                word_counts[word] += 1\n",
            "        \n",
            "        # Convert to frequency distribution\n",
            "        total_words = sum(word_counts.values())\n",
            "        word_freqs = np.zeros(self.vocab_size)\n",
            "        \n",
            "        for word, count in word_counts.items():\n",
            "            if word in self.word_to_idx:\n",
            "                idx = self.word_to_idx[word]\n",
            "                # Use subsampling for frequent words (power = 0.75)\n",
            "                word_freqs[idx] = (count / total_words) ** 0.75\n",
            "        \n",
            "        # Normalize\n",
            "        word_freqs = word_freqs / word_freqs.sum()\n",
            "        \n",
            "        return word_freqs\n",
            "    \n",
            "    def _negative_sampling(self, batch_size):\n",
            "        \"\"\"Generate negative samples.\"\"\"\n",
            "        negative_samples = np.random.choice(\n",
            "            self.vocab_size, \n",
            "            size=(batch_size, self.num_negative),\n",
            "            p=self.word_freqs\n",
            "        )\n",
            "        return torch.LongTensor(negative_samples)\n",
            "    \n",
            "    def __len__(self):\n",
            "        return len(self.training_pairs)\n",
            "    \n",
            "    def __getitem__(self, idx):\n",
            "        center_word, context_word = self.training_pairs[idx]\n",
            "        return torch.LongTensor([center_word]), torch.LongTensor([context_word])\n",
            "\n",
            "# Build vocabulary from tokenized corpus\n",
            "def build_vocabulary(tokenized_corpus, min_count=2):\n",
            "    \"\"\"Build vocabulary with minimum word frequency threshold.\"\"\"\n",
            "    word_counts = Counter()\n",
            "    \n",
            "    # Count word frequencies\n",
            "    for sentence in tokenized_corpus:\n",
            "        for word in sentence:\n",
            "            word_counts[word] += 1\n",
            "    \n",
            "    # Filter by minimum count\n",
            "    filtered_words = {word: count for word, count in word_counts.items() \n",
            "                     if count >= min_count}\n",
            "    \n",
            "    # Create word-to-index mapping\n",
            "    word_to_idx = {'<UNK>': 0}  # Unknown words\n",
            "    idx_to_word = {0: '<UNK>'}\n",
            "    \n",
            "    for idx, word in enumerate(sorted(filtered_words.keys()), 1):\n",
            "        word_to_idx[word] = idx\n",
            "        idx_to_word[idx] = word\n",
            "    \n",
            "    return word_to_idx, idx_to_word, filtered_words\n",
            "\n",
            "# Build vocabulary for Australian tourism corpus\n",
            "word_to_idx, idx_to_word, word_counts = build_vocabulary(tokenized_corpus, min_count=2)\n",
            "vocab_size = len(word_to_idx)\n",
            "\n",
            "print(\"📚 Australian Tourism Vocabulary Built\")\n",
            "print(\"=\" * 40)\n",
            "print(f\"   Vocabulary size: {vocab_size:,}\")\n",
            "print(f\"   Total training pairs: {len(tokenized_corpus)} sentences\")\n",
            "\n",
            "# Show sample vocabulary\n",
            "print(f\"\\n🇦🇺 Sample Australian words in vocabulary:\")\n",
            "australian_sample = [word for word in word_to_idx.keys() \n",
            "                    if word in ['sydney', 'melbourne', 'brisbane', 'perth', 'adelaide', \n",
            "                               'darwin', 'hobart', 'canberra', 'australia', 'australian']]\n",
            "print(f\"   Cities: {', '.join(australian_sample[:8])}\")\n",
            "\n",
            "# Show most frequent words\n",
            "most_frequent = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
            "print(f\"\\n📊 Most frequent words:\")\n",
            "for word, count in most_frequent:\n",
            "    print(f\"   {word}: {count}\")\n",
            "\n",
            "# Create dataset and dataloader\n",
            "dataset = Word2VecDataset(tokenized_corpus, word_to_idx, context_window=5, num_negative=5)\n",
            "dataloader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=0)\n",
            "\n",
            "print(f\"\\n⚡ Dataset created:\")\n",
            "print(f\"   Training pairs: {len(dataset):,}\")\n",
            "print(f\"   Batch size: 256\")\n",
            "print(f\"   Context window: 5\")\n",
            "print(f\"   Negative samples: 5\")\n",
            "\n",
            "print(f\"\\n✅ Ready for Word2Vec training!\")"
        ]
    })
    
    # Cell 10: Word2Vec Training Loop
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Initialize Word2Vec model\n",
            "embed_dim = 200 if DEVICE.type != 'cpu' else 100  # Adjust based on device\n",
            "model = AustralianWord2Vec(\n",
            "    vocab_size=vocab_size,\n",
            "    embed_dim=embed_dim,\n",
            "    context_window=5\n",
            ").to(DEVICE)\n",
            "\n",
            "# Training configuration\n",
            "learning_rate = 0.001 if DEVICE.type != 'cpu' else 0.002\n",
            "num_epochs = 10 if DEVICE.type != 'cpu' else 5\n",
            "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
            "\n",
            "# TensorBoard setup\n",
            "from datetime import datetime\n",
            "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
            "log_dir = f\"runs/australian_word2vec_{timestamp}\"\n",
            "writer = SummaryWriter(log_dir)\n",
            "\n",
            "print(f\"🏋️ Training Australian Tourism Word2Vec\")\n",
            "print(\"=\" * 45)\n",
            "print(f\"   Model: Skip-gram with negative sampling\")\n",
            "print(f\"   Embedding dimension: {embed_dim}\")\n",
            "print(f\"   Vocabulary size: {vocab_size:,}\")\n",
            "print(f\"   Learning rate: {learning_rate}\")\n",
            "print(f\"   Epochs: {num_epochs}\")\n",
            "print(f\"   Device: {DEVICE}\")\n",
            "print(f\"   TensorBoard logs: {log_dir}\")\n",
            "\n",
            "def train_word2vec(model, dataloader, optimizer, writer, num_epochs, device):\n",
            "    \"\"\"Train Word2Vec model with Australian tourism corpus.\"\"\"\n",
            "    \n",
            "    model.train()\n",
            "    total_loss = 0\n",
            "    step = 0\n",
            "    \n",
            "    print(f\"\\n🚀 Starting Word2Vec training...\")\n",
            "    \n",
            "    for epoch in range(num_epochs):\n",
            "        epoch_loss = 0\n",
            "        epoch_steps = 0\n",
            "        \n",
            "        for batch_idx, (center_words, context_words) in enumerate(dataloader):\n",
            "            # Move to device\n",
            "            center_words = center_words.squeeze().to(device)\n",
            "            context_words = context_words.squeeze().to(device)\n",
            "            \n",
            "            # Generate negative samples\n",
            "            batch_size = center_words.size(0)\n",
            "            negative_words = dataset._negative_sampling(batch_size).to(device)\n",
            "            \n",
            "            # Forward pass\n",
            "            optimizer.zero_grad()\n",
            "            loss = model(center_words, context_words, negative_words)\n",
            "            \n",
            "            # Backward pass\n",
            "            loss.backward()\n",
            "            \n",
            "            # Gradient clipping\n",
            "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
            "            \n",
            "            optimizer.step()\n",
            "            \n",
            "            # Accumulate loss\n",
            "            epoch_loss += loss.item()\n",
            "            epoch_steps += 1\n",
            "            step += 1\n",
            "            \n",
            "            # Log to TensorBoard\n",
            "            if batch_idx % 100 == 0:\n",
            "                writer.add_scalar('Loss/Batch', loss.item(), step)\n",
            "                \n",
            "                # Log some embedding norms\n",
            "                if batch_idx % 500 == 0:\n",
            "                    with torch.no_grad():\n",
            "                        embed_norms = torch.norm(model.in_embeddings.weight, dim=1).mean()\n",
            "                        writer.add_scalar('Embeddings/Average_Norm', embed_norms.item(), step)\n",
            "        \n",
            "        # Calculate average epoch loss\n",
            "        avg_epoch_loss = epoch_loss / epoch_steps\n",
            "        \n",
            "        # Log epoch metrics\n",
            "        writer.add_scalar('Loss/Epoch', avg_epoch_loss, epoch)\n",
            "        \n",
            "        print(f\"   Epoch {epoch+1:2d}/{num_epochs}: Loss = {avg_epoch_loss:.6f}\")\n",
            "        \n",
            "        # Log embedding samples for specific Australian words\n",
            "        if epoch % 2 == 0:  # Every 2 epochs\n",
            "            with torch.no_grad():\n",
            "                for city in ['sydney', 'melbourne', 'brisbane']:\n",
            "                    if city in word_to_idx:\n",
            "                        city_idx = word_to_idx[city]\n",
            "                        city_embedding = model.in_embeddings.weight[city_idx]\n",
            "                        writer.add_histogram(f'Embeddings/{city.title()}', city_embedding, epoch)\n",
            "    \n",
            "    writer.close()\n",
            "    print(f\"\\n🎉 Word2Vec training completed!\")\n",
            "    print(f\"   Final average loss: {avg_epoch_loss:.6f}\")\n",
            "    \n",
            "    return model\n",
            "\n",
            "# Train the model\n",
            "trained_model = train_word2vec(model, dataloader, optimizer, writer, num_epochs, DEVICE)\n",
            "\n",
            "# Save the trained model\n",
            "torch.save({\n",
            "    'model_state_dict': trained_model.state_dict(),\n",
            "    'word_to_idx': word_to_idx,\n",
            "    'idx_to_word': idx_to_word,\n",
            "    'embed_dim': embed_dim,\n",
            "    'vocab_size': vocab_size\n",
            "}, 'australian_word2vec_model.pth')\n",
            "\n",
            "print(f\"\\n💾 Model saved as: australian_word2vec_model.pth\")\n",
            "print(f\"📊 TensorBoard logs available at: {log_dir}\")\n",
            "print(f\"   Run: tensorboard --logdir {log_dir}\")"
        ]
    })
    
    # Save the updated notebook
    with open("word_embeddings_nllp.ipynb", "w") as f:
        json.dump(notebook, f, indent=2)
    
    return notebook

if __name__ == "__main__":
    add_word2vec_implementation()
    print("✅ Word2Vec implementation added to embeddings notebook!")