{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW56l2sDbjzD"
      },
      "source": [
        "# Word Embeddings: Encoding Lexical Semantics üá¶üá∫\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/pytorch-mastery/blob/main/examples/pytorch-nlp/02_word_embeddings_nllp.ipynb)\n",
        "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/pytorch-mastery/blob/main/examples/pytorch-nlp/02_word_embeddings_nllp.ipynb)\n",
        "\n",
        "A comprehensive guide to word embeddings using PyTorch, featuring Australian tourism examples and English-Vietnamese multilingual support. Learn how to encode lexical semantics and capture semantic relationships in Australian tourism vocabulary.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "- üî§ **Master word embedding techniques** including Word2Vec, GloVe, and FastText\n",
        "- üá¶üá∫ **Train custom embeddings** on Australian tourism corpus\n",
        "- üåè **Handle multilingual embeddings** for English-Vietnamese text\n",
        "- üìä **Visualize semantic relationships** between Australian cities and landmarks\n",
        "- üîÑ **Compare PyTorch vs TensorFlow** embedding implementations\n",
        "- üéØ **Apply embeddings** to real Australian NLP tasks\n",
        "\n",
        "## What You'll Build\n",
        "\n",
        "1. **Australian Tourism Word2Vec Model** - Capture semantic relationships in tourism vocabulary\n",
        "2. **Multilingual Embedding Space** - Align English and Vietnamese tourism terms\n",
        "3. **Semantic Similarity Engine** - Find similar Australian cities and attractions\n",
        "4. **Interactive Visualization** - Explore embedding space with t-SNE and PCA\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d7H9wtzbjzF",
        "outputId": "4c979d73-a578-4d06-e424-122b78f157ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Environment Detection:\n",
            "   Local Development: False\n",
            "   Google Colab: True\n",
            "   Kaggle Notebooks: False\n",
            "\n",
            "‚öôÔ∏è  Setting up Google Colab environment...\n",
            "41 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "software-properties-common is already the newest version (0.99.22.9).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# Environment Detection and Setup\n",
        "import sys\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Detect the runtime environment\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules or \"kaggle\" in os.environ.get('KAGGLE_URL_BASE', '')\n",
        "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
        "\n",
        "print(f\"üîç Environment Detection:\")\n",
        "print(f\"   Local Development: {IS_LOCAL}\")\n",
        "print(f\"   Google Colab: {IS_COLAB}\")\n",
        "print(f\"   Kaggle Notebooks: {IS_KAGGLE}\")\n",
        "\n",
        "# Platform-specific system setup\n",
        "if IS_COLAB:\n",
        "    print(\"\\n‚öôÔ∏è  Setting up Google Colab environment...\")\n",
        "    !apt update -qq\n",
        "    !apt install -y -qq software-properties-common\n",
        "elif IS_KAGGLE:\n",
        "    print(\"\\n‚öôÔ∏è  Setting up Kaggle environment...\")\n",
        "    # Kaggle usually has most packages pre-installed\n",
        "else:\n",
        "    print(\"\\n‚öôÔ∏è  Setting up local environment...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKfzGpGnbjzG",
        "outputId": "ecef423a-415b-4cf0-a5da-467f30e634fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Installing packages for word embeddings...\n",
            "   ‚úÖ torch\n",
            "   ‚úÖ transformers\n",
            "   ‚úÖ datasets\n",
            "   ‚úÖ tokenizers\n",
            "   ‚úÖ pandas\n",
            "   ‚úÖ seaborn\n",
            "   ‚úÖ matplotlib\n",
            "   ‚úÖ scikit-learn\n",
            "   ‚úÖ tensorboard\n",
            "   ‚úÖ gensim\n",
            "   ‚úÖ plotly\n",
            "\n",
            "üéâ Package installation completed!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages for word embeddings\n",
        "required_packages = [\n",
        "    \"torch\",\n",
        "    \"transformers\",\n",
        "    \"datasets\",\n",
        "    \"tokenizers\",\n",
        "    \"pandas\",\n",
        "    \"seaborn\",\n",
        "    \"matplotlib\",\n",
        "    \"scikit-learn\",\n",
        "    \"tensorboard\",\n",
        "    \"gensim\",  # For Word2Vec and GloVe implementations\n",
        "    \"plotly\",  # For interactive visualizations\n",
        "]\n",
        "\n",
        "print(\"üì¶ Installing packages for word embeddings...\")\n",
        "for package in required_packages:\n",
        "    if IS_COLAB or IS_KAGGLE:\n",
        "        !pip install -q {package}\n",
        "    else:\n",
        "        try:\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package],\n",
        "                          capture_output=True, check=True)\n",
        "        except subprocess.CalledProcessError:\n",
        "            print(f\"   ‚ö†Ô∏è  {package} installation skipped (likely already installed)\")\n",
        "            continue\n",
        "    print(f\"   ‚úÖ {package}\")\n",
        "\n",
        "print(\"\\nüéâ Package installation completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAgeuMCdbjzG",
        "outputId": "b376bfc2-7f0f-47bd-f738-01e383591c29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Gensim imported successfully\n",
            "üî§ Word Embeddings Environment Ready!\n",
            "   PyTorch version: 2.8.0+cu126\n",
            "   Libraries loaded successfully\n"
          ]
        }
      ],
      "source": [
        "# Import essential libraries for word embeddings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Data handling and visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Machine learning and embeddings\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Text processing\n",
        "import re\n",
        "import string\n",
        "from collections import Counter, defaultdict\n",
        "import random\n",
        "from itertools import combinations\n",
        "\n",
        "# Gensim for pre-trained embeddings and Word2Vec\n",
        "try:\n",
        "    from gensim.models import Word2Vec, FastText\n",
        "    from gensim.models.keyedvectors import KeyedVectors\n",
        "    print(\"‚úÖ Gensim imported successfully\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  Gensim not available - will use PyTorch implementations only\")\n",
        "\n",
        "# Set style for better notebook aesthetics\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_palette(\"Set2\")\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(f\"üî§ Word Embeddings Environment Ready!\")\n",
        "print(f\"   PyTorch version: {torch.__version__}\")\n",
        "print(f\"   Libraries loaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP3JY4U0bjzH",
        "outputId": "d2bd37bb-7225-40e9-e512-2eddf13598bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíª CPU mode: x86_64\n",
            "   Threads: 1\n",
            "   üí° Tip: Use smaller embedding dimensions for faster training\n",
            "\n",
            "‚úÖ Device selected: cpu\n"
          ]
        }
      ],
      "source": [
        "import platform\n",
        "\n",
        "def detect_device():\n",
        "    \"\"\"Detect optimal device for word embeddings training.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "\n",
        "        print(f\"üöÄ CUDA GPU detected: {gpu_name}\")\n",
        "        print(f\"   GPU Memory: {gpu_memory:.1f} GB\")\n",
        "        print(f\"   Optimal for large embedding training\")\n",
        "\n",
        "        return device\n",
        "\n",
        "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "        system_info = platform.uname()\n",
        "\n",
        "        print(f\"üçé Apple Silicon MPS detected: {system_info.machine}\")\n",
        "        print(f\"   Optimized for M1/M2/M3 chips\")\n",
        "        print(f\"   Good performance for embedding training\")\n",
        "\n",
        "        return device\n",
        "\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        cpu_count = torch.get_num_threads()\n",
        "\n",
        "        print(f\"üíª CPU mode: {platform.processor()}\")\n",
        "        print(f\"   Threads: {cpu_count}\")\n",
        "        print(f\"   üí° Tip: Use smaller embedding dimensions for faster training\")\n",
        "\n",
        "        return device\n",
        "\n",
        "# Detect and set device\n",
        "DEVICE = detect_device()\n",
        "print(f\"\\n‚úÖ Device selected: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7yP_DatbjzH",
        "outputId": "bc7a8289-3630-438d-b911-4ce08477e3ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üá¶üá∫ Australian Tourism Corpus Created\n",
            "=============================================\n",
            "   English texts: 42\n",
            "   Vietnamese texts: 17\n",
            "   Total corpus size: 59\n",
            "\n",
            "üìù Sample English text:\n",
            "   Sydney Opera House is an iconic architectural masterpiece located on Bennelong Point in Sydney Harbour.\n",
            "\n",
            "üìù Sample Vietnamese text:\n",
            "   Nh√† h√°t Opera Sydney l√† ki·ªát t√°c ki·∫øn tr√∫c bi·ªÉu t∆∞·ª£ng t·ªça l·∫°c t·∫°i Bennelong Point ·ªü C·∫£ng Sydney.\n",
            "\n",
            "üìä Corpus Statistics:\n",
            "   Total words: 770\n",
            "   Unique words: 388\n",
            "   Vocabulary richness: 0.504\n",
            "\n",
            "üèôÔ∏è  Top Australian terms: sydney, australia, australian, melbourne, island\n"
          ]
        }
      ],
      "source": [
        "# Create comprehensive Australian tourism corpus for embedding training\n",
        "def create_australian_tourism_corpus():\n",
        "    \"\"\"\n",
        "    Create a rich corpus of Australian tourism content for training embeddings.\n",
        "\n",
        "    Returns:\n",
        "        dict: Contains English and Vietnamese text with metadata\n",
        "    \"\"\"\n",
        "\n",
        "    # English corpus - Australian tourism content\n",
        "    english_corpus = [\n",
        "        # Sydney content\n",
        "        \"Sydney Opera House is an iconic architectural masterpiece located on Bennelong Point in Sydney Harbour.\",\n",
        "        \"The Sydney Harbour Bridge offers spectacular views of the harbour and city skyline.\",\n",
        "        \"Bondi Beach is famous for surfing and hosts many international surfing competitions.\",\n",
        "        \"The Royal Botanic Gardens Sydney showcase native Australian flora and fauna.\",\n",
        "        \"Darling Harbour features world-class museums, restaurants, and entertainment venues.\",\n",
        "        \"The Rocks historic area preserves Sydney's convict heritage and colonial architecture.\",\n",
        "\n",
        "        # Melbourne content\n",
        "        \"Melbourne is renowned for its vibrant coffee culture and laneway street art.\",\n",
        "        \"The Royal Exhibition Building in Carlton Gardens is a UNESCO World Heritage site.\",\n",
        "        \"Federation Square hosts cultural events and houses major galleries and museums.\",\n",
        "        \"Melbourne's tram network is the largest in the world and iconic to the city.\",\n",
        "        \"The Yarra River flows through Melbourne's central business district and parks.\",\n",
        "        \"Queen Victoria Market offers fresh produce, gourmet food, and unique souvenirs.\",\n",
        "\n",
        "        # Queensland content\n",
        "        \"The Great Barrier Reef is the world's largest coral reef system and UNESCO World Heritage site.\",\n",
        "        \"Brisbane's South Bank features cultural institutions, restaurants, and riverside parks.\",\n",
        "        \"Gold Coast is famous for its theme parks, surfing beaches, and nightlife.\",\n",
        "        \"Cairns serves as the gateway to the Great Barrier Reef and Daintree Rainforest.\",\n",
        "        \"Fraser Island is the world's largest sand island with unique ecosystems.\",\n",
        "        \"Whitsunday Islands offer pristine beaches and excellent sailing conditions.\",\n",
        "\n",
        "        # Western Australia content\n",
        "        \"Perth is one of the most isolated major cities in the world.\",\n",
        "        \"Fremantle port city features well-preserved colonial architecture and maritime heritage.\",\n",
        "        \"Rottnest Island is home to quokkas and beautiful secluded beaches.\",\n",
        "        \"The Pinnacles Desert showcases thousands of limestone pillars in unique formations.\",\n",
        "        \"Margaret River region produces world-class wines and gourmet food.\",\n",
        "        \"Broome features Cable Beach with stunning sunsets and pearl diving history.\",\n",
        "\n",
        "        # South Australia content\n",
        "        \"Adelaide is known as the Festival City with numerous cultural celebrations.\",\n",
        "        \"Barossa Valley produces premium wines and hosts international wine festivals.\",\n",
        "        \"Kangaroo Island wildlife sanctuary protects native Australian animals in natural habitat.\",\n",
        "        \"Adelaide Hills wine region offers cool climate varieties and scenic vineyards.\",\n",
        "        \"Flinders Ranges feature ancient mountain landscapes and Aboriginal cultural sites.\",\n",
        "\n",
        "        # Northern Territory content\n",
        "        \"Uluru is a sacred Aboriginal site and iconic symbol of Australia.\",\n",
        "        \"Kata Tjuta rock formations complement Uluru in the heart of Australia.\",\n",
        "        \"Darwin serves as the gateway to Kakadu National Park and Top End wilderness.\",\n",
        "        \"Kakadu National Park preserves ancient Aboriginal rock art and diverse ecosystems.\",\n",
        "        \"Alice Springs is the heart of the Australian outback and Red Centre.\",\n",
        "\n",
        "        # Tasmania content\n",
        "        \"Hobart's Museum of Old and New Art challenges visitors with provocative contemporary art.\",\n",
        "        \"Cradle Mountain-Lake St Clair National Park offers pristine wilderness hiking.\",\n",
        "        \"Salamanca Market in Hobart features local artisans and Tasmania's finest produce.\",\n",
        "        \"Devil's island Tasmania protects the endangered Tasmanian devil in natural habitat.\",\n",
        "\n",
        "        # ACT content\n",
        "        \"Canberra houses Australia's national institutions including Parliament House and galleries.\",\n",
        "        \"Australian War Memorial commemorates the service of Australian armed forces.\",\n",
        "        \"National Gallery of Australia showcases the finest Australian and international art.\",\n",
        "        \"Lake Burley Griffin provides recreational activities in the heart of Canberra.\"\n",
        "    ]\n",
        "\n",
        "    # Vietnamese corpus - translations and local content\n",
        "    vietnamese_corpus = [\n",
        "        # Sydney translations\n",
        "        \"Nh√† h√°t Opera Sydney l√† ki·ªát t√°c ki·∫øn tr√∫c bi·ªÉu t∆∞·ª£ng t·ªça l·∫°c t·∫°i Bennelong Point ·ªü C·∫£ng Sydney.\",\n",
        "        \"C·∫ßu C·∫£ng Sydney mang ƒë·∫øn t·∫ßm nh√¨n ngo·∫°n m·ª•c ra c·∫£ng v√† ƒë∆∞·ªùng ch√¢n tr·ªùi th√†nh ph·ªë.\",\n",
        "        \"B√£i bi·ªÉn Bondi n·ªïi ti·∫øng v·ªõi l∆∞·ªõt s√≥ng v√† t·ªï ch·ª©c nhi·ªÅu cu·ªôc thi l∆∞·ªõt s√≥ng qu·ªëc t·∫ø.\",\n",
        "        \"V∆∞·ªùn B√°ch th·∫£o Ho√†ng gia Sydney tr∆∞ng b√†y h·ªá ƒë·ªông th·ª±c v·∫≠t b·∫£n ƒë·ªãa Australia.\",\n",
        "\n",
        "        # Melbourne translations\n",
        "        \"Melbourne n·ªïi ti·∫øng v·ªõi vƒÉn h√≥a c√† ph√™ s√¥i ƒë·ªông v√† ngh·ªá thu·∫≠t ƒë∆∞·ªùng ph·ªë trong c√°c con h·∫ªm.\",\n",
        "        \"T√≤a nh√† Tri·ªÉn l√£m Ho√†ng gia ·ªü Carlton Gardens l√† di s·∫£n th·∫ø gi·ªõi UNESCO.\",\n",
        "        \"Qu·∫£ng tr∆∞·ªùng Federation t·ªï ch·ª©c c√°c s·ª± ki·ªán vƒÉn h√≥a v√† c√≥ c√°c ph√≤ng tr∆∞ng b√†y l·ªõn.\",\n",
        "        \"M·∫°ng l∆∞·ªõi t√†u ƒëi·ªán Melbourne l√† l·ªõn nh·∫•t th·∫ø gi·ªõi v√† mang t√≠nh bi·ªÉu t∆∞·ª£ng c·ªßa th√†nh ph·ªë.\",\n",
        "\n",
        "        # Queensland translations\n",
        "        \"R·∫°n san h√¥ Great Barrier l√† h·ªá th·ªëng r·∫°n san h√¥ l·ªõn nh·∫•t th·∫ø gi·ªõi v√† di s·∫£n UNESCO.\",\n",
        "        \"South Bank Brisbane c√≥ c√°c t·ªï ch·ª©c vƒÉn h√≥a, nh√† h√†ng v√† c√¥ng vi√™n ven s√¥ng.\",\n",
        "        \"Gold Coast n·ªïi ti·∫øng v·ªõi c√°c c√¥ng vi√™n gi·∫£i tr√≠, b√£i bi·ªÉn l∆∞·ªõt s√≥ng v√† cu·ªôc s·ªëng v·ªÅ ƒë√™m.\",\n",
        "        \"Cairns l√† c·ª≠a ng√µ ƒë·∫øn R·∫°n san h√¥ Great Barrier v√† R·ª´ng m∆∞a Daintree.\",\n",
        "\n",
        "        # Other regions\n",
        "        \"Perth l√† m·ªôt trong nh·ªØng th√†nh ph·ªë l·ªõn bi·ªát l·∫≠p nh·∫•t tr√™n th·∫ø gi·ªõi.\",\n",
        "        \"Adelaide ƒë∆∞·ª£c bi·∫øt ƒë·∫øn l√† Th√†nh ph·ªë L·ªÖ h·ªôi v·ªõi nhi·ªÅu celebration vƒÉn h√≥a.\",\n",
        "        \"Uluru l√† ƒë·ªãa ƒëi·ªÉm thi√™ng li√™ng c·ªßa th·ªï d√¢n v√† bi·ªÉu t∆∞·ª£ng c·ªßa Australia.\",\n",
        "        \"Hobart c√≥ B·∫£o t√†ng Ngh·ªá thu·∫≠t C≈© v√† M·ªõi th√°ch th·ª©c du kh√°ch v·ªõi ngh·ªá thu·∫≠t ƒë∆∞∆°ng ƒë·∫°i.\",\n",
        "        \"Canberra ch·ª©a c√°c t·ªï ch·ª©c qu·ªëc gia c·ªßa Australia bao g·ªìm T√≤a nh√† Qu·ªëc h·ªôi.\"\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        'english': english_corpus,\n",
        "        'vietnamese': vietnamese_corpus,\n",
        "        'combined': english_corpus + vietnamese_corpus\n",
        "    }\n",
        "\n",
        "# Create the corpus\n",
        "tourism_corpus = create_australian_tourism_corpus()\n",
        "\n",
        "print(\"üá¶üá∫ Australian Tourism Corpus Created\")\n",
        "print(\"=\" * 45)\n",
        "print(f\"   English texts: {len(tourism_corpus['english'])}\")\n",
        "print(f\"   Vietnamese texts: {len(tourism_corpus['vietnamese'])}\")\n",
        "print(f\"   Total corpus size: {len(tourism_corpus['combined'])}\")\n",
        "\n",
        "# Display sample texts\n",
        "print(f\"\\nüìù Sample English text:\")\n",
        "print(f\"   {tourism_corpus['english'][0]}\")\n",
        "print(f\"\\nüìù Sample Vietnamese text:\")\n",
        "print(f\"   {tourism_corpus['vietnamese'][0]}\")\n",
        "\n",
        "# Analyze vocabulary\n",
        "all_words = []\n",
        "for text in tourism_corpus['combined']:\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    all_words.extend(words)\n",
        "\n",
        "vocab_counter = Counter(all_words)\n",
        "unique_words = len(vocab_counter)\n",
        "total_words = len(all_words)\n",
        "\n",
        "print(f\"\\nüìä Corpus Statistics:\")\n",
        "print(f\"   Total words: {total_words:,}\")\n",
        "print(f\"   Unique words: {unique_words:,}\")\n",
        "print(f\"   Vocabulary richness: {unique_words/total_words:.3f}\")\n",
        "\n",
        "# Show most common Australian terms\n",
        "australian_terms = [word for word, count in vocab_counter.most_common(20)\n",
        "                   if word in ['sydney', 'melbourne', 'brisbane', 'perth', 'adelaide',\n",
        "                              'darwin', 'hobart', 'canberra', 'australia', 'australian',\n",
        "                              'beach', 'harbour', 'reef', 'park', 'island']]\n",
        "print(f\"\\nüèôÔ∏è  Top Australian terms: {', '.join(australian_terms[:10])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOnsgt0BbjzH",
        "outputId": "fc8274d8-b5c5-4f16-fdf7-4ce5133fda16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî§ Text Preprocessing for Australian Tourism Embeddings\n",
            "=======================================================\n",
            "   Total sentences: 59\n",
            "   Total words: 672\n",
            "   Unique vocabulary: 362\n",
            "   Average sentence length: 11.4 words\n",
            "\n",
            "üá¶üá∫ Australian terms found: 32\n",
            "   Top terms: sydney, harbour, beach, gardens, convict, heritage, colonial, melbourne, federation, river\n",
            "\n",
            "üáªüá≥ Vietnamese terms found: 14\n",
            "   Sample terms: opera, nh√†, h√°t, c·∫£ng, c·∫ßu\n",
            "\n",
            "üìù Sample tokenized sentences:\n",
            "   1. ['sydney', 'opera', 'house', 'iconic', 'architectural', 'masterpiece', 'located', 'bennelong']... (11 words)\n",
            "   2. ['the', 'sydney', 'harbour', 'bridge', 'offers', 'spectacular', 'views', 'the']... (12 words)\n",
            "   3. ['bondi', 'beach', 'famous', 'for', 'surfing', 'and', 'hosts', 'many']... (11 words)\n",
            "\n",
            "‚úÖ Preprocessing completed - ready for embedding training!\n"
          ]
        }
      ],
      "source": [
        "class AustralianEmbeddingPreprocessor:\n",
        "    \"\"\"\n",
        "    Specialized text preprocessor for Australian tourism embeddings.\n",
        "\n",
        "    Handles both English and Vietnamese text while preserving important\n",
        "    Australian geographic and cultural terms.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Protected Australian terms that should not be heavily modified\n",
        "        self.protected_terms = {\n",
        "            'cities': ['sydney', 'melbourne', 'brisbane', 'perth', 'adelaide',\n",
        "                      'darwin', 'hobart', 'canberra'],\n",
        "            'landmarks': ['uluru', 'kata', 'tjuta', 'kakadu', 'pinnacles',\n",
        "                         'cradle', 'mountain', 'fraser', 'rottnest'],\n",
        "            'features': ['harbour', 'reef', 'outback', 'rainforest', 'desert',\n",
        "                        'beach', 'island', 'river', 'park', 'gardens'],\n",
        "            'cultural': ['aboriginal', 'indigenous', 'heritage', 'colonial',\n",
        "                        'convict', 'federation', 'anzac']\n",
        "        }\n",
        "\n",
        "        # Vietnamese-specific terms to preserve\n",
        "        self.vietnamese_terms = ['nh√†', 'h√°t', 'opera', 'c·∫ßu', 'c·∫£ng', 'b√£i', 'bi·ªÉn',\n",
        "                               'v∆∞·ªùn', 'b√°ch', 'th·∫£o', 'r·∫°n', 'san', 'h√¥', 'th√†nh', 'ph·ªë']\n",
        "\n",
        "    def tokenize_sentence(self, text):\n",
        "        \"\"\"\n",
        "        Tokenize text into sentences, preserving Australian terms.\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text\n",
        "\n",
        "        Returns:\n",
        "            list: List of tokenized words\n",
        "        \"\"\"\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove punctuation but preserve apostrophes in contractions\n",
        "        text = re.sub(r\"[^\\w\\s']\", ' ', text)\n",
        "\n",
        "        # Handle contractions\n",
        "        contractions = {\n",
        "            \"n't\": \" not\",\n",
        "            \"'re\": \" are\",\n",
        "            \"'s\": \" is\",\n",
        "            \"'ve\": \" have\",\n",
        "            \"'ll\": \" will\",\n",
        "            \"'d\": \" would\"\n",
        "        }\n",
        "\n",
        "        for contraction, expansion in contractions.items():\n",
        "            text = text.replace(contraction, expansion)\n",
        "\n",
        "        # Split into words\n",
        "        words = text.split()\n",
        "\n",
        "        # Filter out very short words (except important ones)\n",
        "        important_short_words = {'wa', 'sa', 'nt', 'tas', 'act', 'nsw', 'vic', 'qld'}\n",
        "        words = [word for word in words\n",
        "                if len(word) > 2 or word in important_short_words]\n",
        "\n",
        "        return words\n",
        "\n",
        "    def prepare_training_data(self, corpus):\n",
        "        \"\"\"\n",
        "        Prepare corpus for embedding training.\n",
        "\n",
        "        Args:\n",
        "            corpus (list): List of text documents\n",
        "\n",
        "        Returns:\n",
        "            list: List of tokenized sentences\n",
        "        \"\"\"\n",
        "        tokenized_corpus = []\n",
        "\n",
        "        for text in corpus:\n",
        "            # Split into sentences\n",
        "            sentences = re.split(r'[.!?]+', text)\n",
        "\n",
        "            for sentence in sentences:\n",
        "                sentence = sentence.strip()\n",
        "                if len(sentence) > 10:  # Skip very short sentences\n",
        "                    tokens = self.tokenize_sentence(sentence)\n",
        "                    if len(tokens) >= 3:  # Minimum sentence length\n",
        "                        tokenized_corpus.append(tokens)\n",
        "\n",
        "        return tokenized_corpus\n",
        "\n",
        "    def analyze_vocabulary(self, tokenized_corpus):\n",
        "        \"\"\"\n",
        "        Analyze vocabulary statistics from tokenized corpus.\n",
        "\n",
        "        Args:\n",
        "            tokenized_corpus (list): List of tokenized sentences\n",
        "\n",
        "        Returns:\n",
        "            dict: Vocabulary statistics\n",
        "        \"\"\"\n",
        "        all_words = []\n",
        "        for sentence in tokenized_corpus:\n",
        "            all_words.extend(sentence)\n",
        "\n",
        "        word_freq = Counter(all_words)\n",
        "\n",
        "        # Find Australian-specific terms\n",
        "        australian_words = []\n",
        "        for word, freq in word_freq.items():\n",
        "            if any(word in terms for terms in self.protected_terms.values()):\n",
        "                australian_words.append((word, freq))\n",
        "\n",
        "        # Find Vietnamese terms\n",
        "        vietnamese_words = [(word, freq) for word, freq in word_freq.items()\n",
        "                           if word in self.vietnamese_terms]\n",
        "\n",
        "        return {\n",
        "            'total_words': len(all_words),\n",
        "            'unique_words': len(word_freq),\n",
        "            'word_frequencies': word_freq,\n",
        "            'australian_terms': australian_words,\n",
        "            'vietnamese_terms': vietnamese_words,\n",
        "            'avg_sentence_length': np.mean([len(s) for s in tokenized_corpus])\n",
        "        }\n",
        "\n",
        "# Initialize preprocessor and prepare data\n",
        "preprocessor = AustralianEmbeddingPreprocessor()\n",
        "\n",
        "# Prepare training data\n",
        "tokenized_corpus = preprocessor.prepare_training_data(tourism_corpus['combined'])\n",
        "vocab_stats = preprocessor.analyze_vocabulary(tokenized_corpus)\n",
        "\n",
        "print(\"üî§ Text Preprocessing for Australian Tourism Embeddings\")\n",
        "print(\"=\" * 55)\n",
        "print(f\"   Total sentences: {len(tokenized_corpus)}\")\n",
        "print(f\"   Total words: {vocab_stats['total_words']:,}\")\n",
        "print(f\"   Unique vocabulary: {vocab_stats['unique_words']:,}\")\n",
        "print(f\"   Average sentence length: {vocab_stats['avg_sentence_length']:.1f} words\")\n",
        "\n",
        "print(f\"\\nüá¶üá∫ Australian terms found: {len(vocab_stats['australian_terms'])}\")\n",
        "australian_terms_str = ', '.join([term for term, freq in vocab_stats['australian_terms'][:10]])\n",
        "print(f\"   Top terms: {australian_terms_str}\")\n",
        "\n",
        "print(f\"\\nüáªüá≥ Vietnamese terms found: {len(vocab_stats['vietnamese_terms'])}\")\n",
        "if vocab_stats['vietnamese_terms']:\n",
        "    vietnamese_terms_str = ', '.join([term for term, freq in vocab_stats['vietnamese_terms'][:5]])\n",
        "    print(f\"   Sample terms: {vietnamese_terms_str}\")\n",
        "\n",
        "# Show sample tokenized sentences\n",
        "print(f\"\\nüìù Sample tokenized sentences:\")\n",
        "for i, sentence in enumerate(tokenized_corpus[:3]):\n",
        "    print(f\"   {i+1}. {sentence[:8]}... ({len(sentence)} words)\")\n",
        "\n",
        "print(f\"\\n‚úÖ Preprocessing completed - ready for embedding training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBSR9WrhbjzI",
        "outputId": "339b8640-9edf-4749-8245-2f854f19d00b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî§ Australian Word2Vec model class defined!\n",
            "   Architecture: Skip-gram with negative sampling\n",
            "   Optimized for: Australian tourism vocabulary\n",
            "   Features: Similarity computation, most similar words\n"
          ]
        }
      ],
      "source": [
        "class AustralianWord2Vec(nn.Module):\n",
        "    \"\"\"\n",
        "    PyTorch implementation of Word2Vec Skip-gram model for Australian tourism corpus.\n",
        "\n",
        "    TensorFlow equivalent:\n",
        "        embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    This implementation uses:\n",
        "    - Skip-gram architecture for better rare word representation\n",
        "    - Negative sampling for efficient training\n",
        "    - Australian tourism vocabulary optimization\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): Size of vocabulary\n",
        "        embed_dim (int): Embedding dimension (typically 100-300)\n",
        "        context_window (int): Context window size (typically 5-10)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim=200, context_window=5):\n",
        "        super(AustralianWord2Vec, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.context_window = context_window\n",
        "\n",
        "        # Input embeddings (center words)\n",
        "        self.in_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # Output embeddings (context words)\n",
        "        self.out_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # Initialize embeddings with small random values\n",
        "        self._init_embeddings()\n",
        "\n",
        "        # Store Australian cities for analysis\n",
        "        self.australian_cities = ['sydney', 'melbourne', 'brisbane', 'perth',\n",
        "                                'adelaide', 'darwin', 'hobart', 'canberra']\n",
        "\n",
        "    def _init_embeddings(self):\n",
        "        \"\"\"Initialize embedding weights.\"\"\"\n",
        "        # Initialize with small random values\n",
        "        nn.init.uniform_(self.in_embeddings.weight, -0.5/self.embed_dim, 0.5/self.embed_dim)\n",
        "        nn.init.uniform_(self.out_embeddings.weight, -0.5/self.embed_dim, 0.5/self.embed_dim)\n",
        "\n",
        "    def forward(self, center_words, context_words, negative_words=None):\n",
        "        \"\"\"\n",
        "        Forward pass for Word2Vec training.\n",
        "\n",
        "        Args:\n",
        "            center_words (torch.Tensor): Center word indices [batch_size]\n",
        "            context_words (torch.Tensor): Context word indices [batch_size]\n",
        "            negative_words (torch.Tensor): Negative sample indices [batch_size, num_negative]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Loss value\n",
        "        \"\"\"\n",
        "        batch_size = center_words.size(0)\n",
        "\n",
        "        # Get center word embeddings\n",
        "        center_embeds = self.in_embeddings(center_words)  # [batch_size, embed_dim]\n",
        "\n",
        "        # Get context word embeddings\n",
        "        context_embeds = self.out_embeddings(context_words)  # [batch_size, embed_dim]\n",
        "\n",
        "        # Positive samples score\n",
        "        pos_score = torch.sum(center_embeds * context_embeds, dim=1)  # [batch_size]\n",
        "        pos_loss = -F.logsigmoid(pos_score).mean()\n",
        "\n",
        "        # Negative sampling loss\n",
        "        neg_loss = 0\n",
        "        if negative_words is not None:\n",
        "            num_negative = negative_words.size(1)\n",
        "\n",
        "            # Get negative word embeddings\n",
        "            neg_embeds = self.out_embeddings(negative_words)  # [batch_size, num_negative, embed_dim]\n",
        "\n",
        "            # Compute negative scores\n",
        "            center_embeds_expanded = center_embeds.unsqueeze(1).expand(-1, num_negative, -1)\n",
        "            neg_scores = torch.sum(center_embeds_expanded * neg_embeds, dim=2)  # [batch_size, num_negative]\n",
        "\n",
        "            neg_loss = -F.logsigmoid(-neg_scores).mean()\n",
        "\n",
        "        return pos_loss + neg_loss\n",
        "\n",
        "    def get_word_embeddings(self):\n",
        "        \"\"\"Get trained word embeddings.\"\"\"\n",
        "        return self.in_embeddings.weight.data\n",
        "\n",
        "    def similarity(self, word1_idx, word2_idx):\n",
        "        \"\"\"Compute cosine similarity between two words.\"\"\"\n",
        "        embeddings = self.get_word_embeddings()\n",
        "\n",
        "        emb1 = embeddings[word1_idx]\n",
        "        emb2 = embeddings[word2_idx]\n",
        "\n",
        "        # Cosine similarity\n",
        "        cos_sim = F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0))\n",
        "        return cos_sim.item()\n",
        "\n",
        "    def most_similar(self, word_idx, word_to_idx, idx_to_word, top_k=10):\n",
        "        \"\"\"Find most similar words to a given word.\"\"\"\n",
        "        embeddings = self.get_word_embeddings()\n",
        "        word_embed = embeddings[word_idx].unsqueeze(0)\n",
        "\n",
        "        # Compute similarities with all words\n",
        "        similarities = F.cosine_similarity(word_embed, embeddings)\n",
        "\n",
        "        # Get top-k most similar (excluding the word itself)\n",
        "        similarities[word_idx] = -1  # Exclude the word itself\n",
        "        top_indices = similarities.topk(top_k).indices\n",
        "\n",
        "        similar_words = []\n",
        "        for idx in top_indices:\n",
        "            word = idx_to_word.get(idx.item(), '<UNK>')\n",
        "            similarity_score = similarities[idx].item()\n",
        "            similar_words.append((word, similarity_score))\n",
        "\n",
        "        return similar_words\n",
        "\n",
        "print(\"üî§ Australian Word2Vec model class defined!\")\n",
        "print(\"   Architecture: Skip-gram with negative sampling\")\n",
        "print(\"   Optimized for: Australian tourism vocabulary\")\n",
        "print(\"   Features: Similarity computation, most similar words\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPdYABNtbjzI",
        "outputId": "17b83283-17d8-45de-f978-5f55f8dbc809"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Australian Tourism Vocabulary Built\n",
            "========================================\n",
            "   Vocabulary size: 141\n",
            "   Total training pairs: 59 sentences\n",
            "\n",
            "üá¶üá∫ Sample Australian words in vocabulary:\n",
            "   Cities: adelaide, australia, australian, brisbane, canberra, hobart, melbourne, perth\n",
            "\n",
            "üìä Most frequent words:\n",
            "   and: 31\n",
            "   the: 26\n",
            "   sydney: 9\n",
            "   world: 8\n",
            "   australia: 7\n",
            "   australian: 6\n",
            "   c√°c: 6\n",
            "   features: 5\n",
            "   melbourne: 5\n",
            "   art: 5\n",
            "\n",
            "‚ö° Dataset created:\n",
            "   Training pairs: 4,950\n",
            "   Batch size: 256\n",
            "   Context window: 5\n",
            "   Negative samples: 5\n",
            "\n",
            "‚úÖ Ready for Word2Vec training!\n"
          ]
        }
      ],
      "source": [
        "class Word2VecDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for Word2Vec training with Australian tourism corpus.\n",
        "\n",
        "    Generates (center_word, context_word) pairs for skip-gram training.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokenized_corpus, word_to_idx, context_window=5, num_negative=5):\n",
        "        self.tokenized_corpus = tokenized_corpus\n",
        "        self.word_to_idx = word_to_idx\n",
        "        self.idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "        self.context_window = context_window\n",
        "        self.num_negative = num_negative\n",
        "        self.vocab_size = len(word_to_idx)\n",
        "\n",
        "        # Generate training pairs\n",
        "        self.training_pairs = self._generate_training_pairs()\n",
        "\n",
        "        # Create word frequency table for negative sampling\n",
        "        self.word_freqs = self._build_frequency_table()\n",
        "\n",
        "    def _generate_training_pairs(self):\n",
        "        \"\"\"Generate (center, context) word pairs.\"\"\"\n",
        "        pairs = []\n",
        "\n",
        "        for sentence in self.tokenized_corpus:\n",
        "            # Convert words to indices\n",
        "            word_indices = [self.word_to_idx.get(word, self.word_to_idx.get('<UNK>', 0))\n",
        "                           for word in sentence]\n",
        "\n",
        "            # Generate context pairs\n",
        "            for center_idx, center_word_idx in enumerate(word_indices):\n",
        "                # Define context window\n",
        "                start = max(0, center_idx - self.context_window)\n",
        "                end = min(len(word_indices), center_idx + self.context_window + 1)\n",
        "\n",
        "                # Generate pairs\n",
        "                for context_idx in range(start, end):\n",
        "                    if context_idx != center_idx:\n",
        "                        pairs.append((center_word_idx, word_indices[context_idx]))\n",
        "\n",
        "        return pairs\n",
        "\n",
        "    def _build_frequency_table(self):\n",
        "        \"\"\"Build word frequency table for negative sampling.\"\"\"\n",
        "        word_counts = Counter()\n",
        "\n",
        "        for sentence in self.tokenized_corpus:\n",
        "            for word in sentence:\n",
        "                word_counts[word] += 1\n",
        "\n",
        "        # Convert to frequency distribution\n",
        "        total_words = sum(word_counts.values())\n",
        "        word_freqs = np.zeros(self.vocab_size)\n",
        "\n",
        "        for word, count in word_counts.items():\n",
        "            if word in self.word_to_idx:\n",
        "                idx = self.word_to_idx[word]\n",
        "                # Use subsampling for frequent words (power = 0.75)\n",
        "                word_freqs[idx] = (count / total_words) ** 0.75\n",
        "\n",
        "        # Normalize\n",
        "        word_freqs = word_freqs / word_freqs.sum()\n",
        "\n",
        "        return word_freqs\n",
        "\n",
        "    def _negative_sampling(self, batch_size):\n",
        "        \"\"\"Generate negative samples.\"\"\"\n",
        "        negative_samples = np.random.choice(\n",
        "            self.vocab_size,\n",
        "            size=(batch_size, self.num_negative),\n",
        "            p=self.word_freqs\n",
        "        )\n",
        "        return torch.LongTensor(negative_samples)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.training_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        center_word, context_word = self.training_pairs[idx]\n",
        "        return torch.LongTensor([center_word]), torch.LongTensor([context_word])\n",
        "\n",
        "# Build vocabulary from tokenized corpus\n",
        "def build_vocabulary(tokenized_corpus, min_count=2):\n",
        "    \"\"\"Build vocabulary with minimum word frequency threshold.\"\"\"\n",
        "    word_counts = Counter()\n",
        "\n",
        "    # Count word frequencies\n",
        "    for sentence in tokenized_corpus:\n",
        "        for word in sentence:\n",
        "            word_counts[word] += 1\n",
        "\n",
        "    # Filter by minimum count\n",
        "    filtered_words = {word: count for word, count in word_counts.items()\n",
        "                     if count >= min_count}\n",
        "\n",
        "    # Create word-to-index mapping\n",
        "    word_to_idx = {'<UNK>': 0}  # Unknown words\n",
        "    idx_to_word = {0: '<UNK>'}\n",
        "\n",
        "    for idx, word in enumerate(sorted(filtered_words.keys()), 1):\n",
        "        word_to_idx[word] = idx\n",
        "        idx_to_word[idx] = word\n",
        "\n",
        "    return word_to_idx, idx_to_word, filtered_words\n",
        "\n",
        "# Build vocabulary for Australian tourism corpus\n",
        "word_to_idx, idx_to_word, word_counts = build_vocabulary(tokenized_corpus, min_count=2)\n",
        "vocab_size = len(word_to_idx)\n",
        "\n",
        "print(\"üìö Australian Tourism Vocabulary Built\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"   Vocabulary size: {vocab_size:,}\")\n",
        "print(f\"   Total training pairs: {len(tokenized_corpus)} sentences\")\n",
        "\n",
        "# Show sample vocabulary\n",
        "print(f\"\\nüá¶üá∫ Sample Australian words in vocabulary:\")\n",
        "australian_sample = [word for word in word_to_idx.keys()\n",
        "                    if word in ['sydney', 'melbourne', 'brisbane', 'perth', 'adelaide',\n",
        "                               'darwin', 'hobart', 'canberra', 'australia', 'australian']]\n",
        "print(f\"   Cities: {', '.join(australian_sample[:8])}\")\n",
        "\n",
        "# Show most frequent words\n",
        "most_frequent = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "print(f\"\\nüìä Most frequent words:\")\n",
        "for word, count in most_frequent:\n",
        "    print(f\"   {word}: {count}\")\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = Word2VecDataset(tokenized_corpus, word_to_idx, context_window=5, num_negative=5)\n",
        "dataloader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=0)\n",
        "\n",
        "print(f\"\\n‚ö° Dataset created:\")\n",
        "print(f\"   Training pairs: {len(dataset):,}\")\n",
        "print(f\"   Batch size: 256\")\n",
        "print(f\"   Context window: 5\")\n",
        "print(f\"   Negative samples: 5\")\n",
        "\n",
        "print(f\"\\n‚úÖ Ready for Word2Vec training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD54VBd4bjzJ",
        "outputId": "c8050361-5377-43bb-ab9f-773d8ae0addf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèãÔ∏è Training Australian Tourism Word2Vec\n",
            "=============================================\n",
            "   Model: Skip-gram with negative sampling\n",
            "   Embedding dimension: 100\n",
            "   Vocabulary size: 141\n",
            "   Learning rate: 0.002\n",
            "   Epochs: 5\n",
            "   Device: cpu\n",
            "   TensorBoard logs: runs/australian_word2vec_20250923_053937\n",
            "\n",
            "üöÄ Starting Word2Vec training...\n",
            "   Epoch  1/5: Loss = 1.380366\n",
            "   Epoch  2/5: Loss = 1.327919\n",
            "   Epoch  3/5: Loss = 1.235292\n",
            "   Epoch  4/5: Loss = 1.168256\n",
            "   Epoch  5/5: Loss = 1.131216\n",
            "\n",
            "üéâ Word2Vec training completed!\n",
            "   Final average loss: 1.131216\n",
            "\n",
            "üíæ Model saved as: australian_word2vec_model.pth\n",
            "üìä TensorBoard logs available at: runs/australian_word2vec_20250923_053937\n",
            "   Run: tensorboard --logdir runs/australian_word2vec_20250923_053937\n"
          ]
        }
      ],
      "source": [
        "# Initialize Word2Vec model\n",
        "embed_dim = 200 if DEVICE.type != 'cpu' else 100  # Adjust based on device\n",
        "model = AustralianWord2Vec(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=embed_dim,\n",
        "    context_window=5\n",
        ").to(DEVICE)\n",
        "\n",
        "# Training configuration\n",
        "learning_rate = 0.001 if DEVICE.type != 'cpu' else 0.002\n",
        "num_epochs = 10 if DEVICE.type != 'cpu' else 5\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# TensorBoard setup\n",
        "from datetime import datetime\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "log_dir = f\"runs/australian_word2vec_{timestamp}\"\n",
        "writer = SummaryWriter(log_dir)\n",
        "\n",
        "print(f\"üèãÔ∏è Training Australian Tourism Word2Vec\")\n",
        "print(\"=\" * 45)\n",
        "print(f\"   Model: Skip-gram with negative sampling\")\n",
        "print(f\"   Embedding dimension: {embed_dim}\")\n",
        "print(f\"   Vocabulary size: {vocab_size:,}\")\n",
        "print(f\"   Learning rate: {learning_rate}\")\n",
        "print(f\"   Epochs: {num_epochs}\")\n",
        "print(f\"   Device: {DEVICE}\")\n",
        "print(f\"   TensorBoard logs: {log_dir}\")\n",
        "\n",
        "def train_word2vec(model, dataloader, optimizer, writer, num_epochs, device):\n",
        "    \"\"\"Train Word2Vec model with Australian tourism corpus.\"\"\"\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    step = 0\n",
        "\n",
        "    print(f\"\\nüöÄ Starting Word2Vec training...\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        epoch_steps = 0\n",
        "\n",
        "        for batch_idx, (center_words, context_words) in enumerate(dataloader):\n",
        "            # Move to device\n",
        "            center_words = center_words.squeeze().to(device)\n",
        "            context_words = context_words.squeeze().to(device)\n",
        "\n",
        "            # Generate negative samples\n",
        "            batch_size = center_words.size(0)\n",
        "            negative_words = dataset._negative_sampling(batch_size).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss = model(center_words, context_words, negative_words)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_steps += 1\n",
        "            step += 1\n",
        "\n",
        "            # Log to TensorBoard\n",
        "            if batch_idx % 100 == 0:\n",
        "                writer.add_scalar('Loss/Batch', loss.item(), step)\n",
        "\n",
        "                # Log some embedding norms\n",
        "                if batch_idx % 500 == 0:\n",
        "                    with torch.no_grad():\n",
        "                        embed_norms = torch.norm(model.in_embeddings.weight, dim=1).mean()\n",
        "                        writer.add_scalar('Embeddings/Average_Norm', embed_norms.item(), step)\n",
        "\n",
        "        # Calculate average epoch loss\n",
        "        avg_epoch_loss = epoch_loss / epoch_steps\n",
        "\n",
        "        # Log epoch metrics\n",
        "        writer.add_scalar('Loss/Epoch', avg_epoch_loss, epoch)\n",
        "\n",
        "        print(f\"   Epoch {epoch+1:2d}/{num_epochs}: Loss = {avg_epoch_loss:.6f}\")\n",
        "\n",
        "        # Log embedding samples for specific Australian words\n",
        "        if epoch % 2 == 0:  # Every 2 epochs\n",
        "            with torch.no_grad():\n",
        "                for city in ['sydney', 'melbourne', 'brisbane']:\n",
        "                    if city in word_to_idx:\n",
        "                        city_idx = word_to_idx[city]\n",
        "                        city_embedding = model.in_embeddings.weight[city_idx]\n",
        "                        writer.add_histogram(f'Embeddings/{city.title()}', city_embedding, epoch)\n",
        "\n",
        "    writer.close()\n",
        "    print(f\"\\nüéâ Word2Vec training completed!\")\n",
        "    print(f\"   Final average loss: {avg_epoch_loss:.6f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "trained_model = train_word2vec(model, dataloader, optimizer, writer, num_epochs, DEVICE)\n",
        "\n",
        "# Save the trained model\n",
        "torch.save({\n",
        "    'model_state_dict': trained_model.state_dict(),\n",
        "    'word_to_idx': word_to_idx,\n",
        "    'idx_to_word': idx_to_word,\n",
        "    'embed_dim': embed_dim,\n",
        "    'vocab_size': vocab_size\n",
        "}, 'australian_word2vec_model.pth')\n",
        "\n",
        "print(f\"\\nüíæ Model saved as: australian_word2vec_model.pth\")\n",
        "print(f\"üìä TensorBoard logs available at: {log_dir}\")\n",
        "print(f\"   Run: tensorboard --logdir {log_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09d0359a"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "<!-- Add your concluding remarks here -->"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Steps"
      ],
      "metadata": {
        "id": "_cNrXCblE0ME"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}