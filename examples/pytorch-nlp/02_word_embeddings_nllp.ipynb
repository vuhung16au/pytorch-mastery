{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Word Embeddings: Encoding Lexical Semantics \ud83c\udde6\ud83c\uddfa\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/pytorch-mastery/blob/main/examples/pytorch-nlp/02_word_embeddings_nllp.ipynb)\n",
        "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/pytorch-mastery/blob/main/examples/pytorch-nlp/02_word_embeddings_nllp.ipynb)\n",
        "\n",
        "A comprehensive guide to word embeddings using PyTorch, featuring Australian tourism examples and English-Vietnamese multilingual support. Learn how to encode lexical semantics and capture semantic relationships in Australian tourism vocabulary.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "- \ud83d\udd24 **Master word embedding techniques** including Word2Vec, GloVe, and FastText\n",
        "- \ud83c\udde6\ud83c\uddfa **Train custom embeddings** on Australian tourism corpus\n",
        "- \ud83c\udf0f **Handle multilingual embeddings** for English-Vietnamese text\n",
        "- \ud83d\udcca **Visualize semantic relationships** between Australian cities and landmarks\n",
        "- \ud83d\udd04 **Compare PyTorch vs TensorFlow** embedding implementations\n",
        "- \ud83c\udfaf **Apply embeddings** to real Australian NLP tasks\n",
        "\n",
        "## What You'll Build\n",
        "\n",
        "1. **Australian Tourism Word2Vec Model** - Capture semantic relationships in tourism vocabulary\n",
        "2. **Multilingual Embedding Space** - Align English and Vietnamese tourism terms\n",
        "3. **Semantic Similarity Engine** - Find similar Australian cities and attractions\n",
        "4. **Interactive Visualization** - Explore embedding space with t-SNE and PCA\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment Detection and Setup\n",
        "import sys\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Detect the runtime environment\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules or \"kaggle\" in os.environ.get('KAGGLE_URL_BASE', '')\n",
        "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
        "\n",
        "print(f\"\ud83d\udd0d Environment Detection:\")\n",
        "print(f\"   Local Development: {IS_LOCAL}\")\n",
        "print(f\"   Google Colab: {IS_COLAB}\")\n",
        "print(f\"   Kaggle Notebooks: {IS_KAGGLE}\")\n",
        "\n",
        "# Platform-specific system setup\n",
        "if IS_COLAB:\n",
        "    print(\"\\n\u2699\ufe0f  Setting up Google Colab environment...\")\n",
        "    !apt update -qq\n",
        "    !apt install -y -qq software-properties-common\n",
        "elif IS_KAGGLE:\n",
        "    print(\"\\n\u2699\ufe0f  Setting up Kaggle environment...\")\n",
        "    # Kaggle usually has most packages pre-installed\n",
        "else:\n",
        "    print(\"\\n\u2699\ufe0f  Setting up local environment...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages for word embeddings\n",
        "required_packages = [\n",
        "    \"torch\",\n",
        "    \"transformers\",\n",
        "    \"datasets\", \n",
        "    \"tokenizers\",\n",
        "    \"pandas\",\n",
        "    \"seaborn\",\n",
        "    \"matplotlib\",\n",
        "    \"scikit-learn\",\n",
        "    \"tensorboard\",\n",
        "    \"gensim\",  # For Word2Vec and GloVe implementations\n",
        "    \"plotly\",  # For interactive visualizations\n",
        "]\n",
        "\n",
        "print(\"\ud83d\udce6 Installing packages for word embeddings...\")\n",
        "for package in required_packages:\n",
        "    if IS_COLAB or IS_KAGGLE:\n",
        "        !pip install -q {package}\n",
        "    else:\n",
        "        try:\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
        "                          capture_output=True, check=True)\n",
        "        except subprocess.CalledProcessError:\n",
        "            print(f\"   \u26a0\ufe0f  {package} installation skipped (likely already installed)\")\n",
        "            continue\n",
        "    print(f\"   \u2705 {package}\")\n",
        "\n",
        "print(\"\\n\ud83c\udf89 Package installation completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import essential libraries for word embeddings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Data handling and visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Machine learning and embeddings\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Text processing\n",
        "import re\n",
        "import string\n",
        "from collections import Counter, defaultdict\n",
        "import random\n",
        "from itertools import combinations\n",
        "\n",
        "# Gensim for pre-trained embeddings and Word2Vec\n",
        "try:\n",
        "    from gensim.models import Word2Vec, FastText\n",
        "    from gensim.models.keyedvectors import KeyedVectors\n",
        "    print(\"\u2705 Gensim imported successfully\")\n",
        "except ImportError:\n",
        "    print(\"\u26a0\ufe0f  Gensim not available - will use PyTorch implementations only\")\n",
        "\n",
        "# Set style for better notebook aesthetics\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_palette(\"Set2\")\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(16)\n",
        "np.random.seed(16)\n",
        "random.seed(16)\n",
        "\n",
        "print(f\"\ud83d\udd24 Word Embeddings Environment Ready!\")\n",
        "print(f\"   PyTorch version: {torch.__version__}\")\n",
        "print(f\"   Libraries loaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import platform\n",
        "\n",
        "def detect_device():\n",
        "    \"\"\"Detect optimal device for word embeddings training.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "        \n",
        "        print(f\"\ud83d\ude80 CUDA GPU detected: {gpu_name}\")\n",
        "        print(f\"   GPU Memory: {gpu_memory:.1f} GB\")\n",
        "        print(f\"   Optimal for large embedding training\")\n",
        "        \n",
        "        return device\n",
        "    \n",
        "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "        system_info = platform.uname()\n",
        "        \n",
        "        print(f\"\ud83c\udf4e Apple Silicon MPS detected: {system_info.machine}\")\n",
        "        print(f\"   Optimized for M1/M2/M3 chips\")\n",
        "        print(f\"   Good performance for embedding training\")\n",
        "        \n",
        "        return device\n",
        "    \n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        cpu_count = torch.get_num_threads()\n",
        "        \n",
        "        print(f\"\ud83d\udcbb CPU mode: {platform.processor()}\")\n",
        "        print(f\"   Threads: {cpu_count}\")\n",
        "        print(f\"   \ud83d\udca1 Tip: Use smaller embedding dimensions for faster training\")\n",
        "        \n",
        "        return device\n",
        "\n",
        "# Detect and set device\n",
        "DEVICE = detect_device()\n",
        "print(f\"\\n\u2705 Device selected: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive Australian tourism corpus for embedding training\n",
        "def create_australian_tourism_corpus():\n",
        "    \"\"\"\n",
        "    Create a rich corpus of Australian tourism content for training embeddings.\n",
        "    \n",
        "    Returns:\n",
        "        dict: Contains English and Vietnamese text with metadata\n",
        "    \"\"\"\n",
        "    \n",
        "    # English corpus - Australian tourism content\n",
        "    english_corpus = [\n",
        "        # Sydney content\n",
        "        \"Sydney Opera House is an iconic architectural masterpiece located on Bennelong Point in Sydney Harbour.\",\n",
        "        \"The Sydney Harbour Bridge offers spectacular views of the harbour and city skyline.\",\n",
        "        \"Bondi Beach is famous for surfing and hosts many international surfing competitions.\",\n",
        "        \"The Royal Botanic Gardens Sydney showcase native Australian flora and fauna.\",\n",
        "        \"Darling Harbour features world-class museums, restaurants, and entertainment venues.\",\n",
        "        \"The Rocks historic area preserves Sydney's convict heritage and colonial architecture.\",\n",
        "        \n",
        "        # Melbourne content\n",
        "        \"Melbourne is renowned for its vibrant coffee culture and laneway street art.\",\n",
        "        \"The Royal Exhibition Building in Carlton Gardens is a UNESCO World Heritage site.\",\n",
        "        \"Federation Square hosts cultural events and houses major galleries and museums.\",\n",
        "        \"Melbourne's tram network is the largest in the world and iconic to the city.\",\n",
        "        \"The Yarra River flows through Melbourne's central business district and parks.\",\n",
        "        \"Queen Victoria Market offers fresh produce, gourmet food, and unique souvenirs.\",\n",
        "        \n",
        "        # Queensland content\n",
        "        \"The Great Barrier Reef is the world's largest coral reef system and UNESCO World Heritage site.\",\n",
        "        \"Brisbane's South Bank features cultural institutions, restaurants, and riverside parks.\",\n",
        "        \"Gold Coast is famous for its theme parks, surfing beaches, and nightlife.\",\n",
        "        \"Cairns serves as the gateway to the Great Barrier Reef and Daintree Rainforest.\",\n",
        "        \"Fraser Island is the world's largest sand island with unique ecosystems.\",\n",
        "        \"Whitsunday Islands offer pristine beaches and excellent sailing conditions.\",\n",
        "        \n",
        "        # Western Australia content  \n",
        "        \"Perth is one of the most isolated major cities in the world.\",\n",
        "        \"Fremantle port city features well-preserved colonial architecture and maritime heritage.\",\n",
        "        \"Rottnest Island is home to quokkas and beautiful secluded beaches.\",\n",
        "        \"The Pinnacles Desert showcases thousands of limestone pillars in unique formations.\",\n",
        "        \"Margaret River region produces world-class wines and gourmet food.\",\n",
        "        \"Broome features Cable Beach with stunning sunsets and pearl diving history.\",\n",
        "        \n",
        "        # South Australia content\n",
        "        \"Adelaide is known as the Festival City with numerous cultural celebrations.\",\n",
        "        \"Barossa Valley produces premium wines and hosts international wine festivals.\",\n",
        "        \"Kangaroo Island wildlife sanctuary protects native Australian animals in natural habitat.\",\n",
        "        \"Adelaide Hills wine region offers cool climate varieties and scenic vineyards.\",\n",
        "        \"Flinders Ranges feature ancient mountain landscapes and Aboriginal cultural sites.\",\n",
        "        \n",
        "        # Northern Territory content\n",
        "        \"Uluru is a sacred Aboriginal site and iconic symbol of Australia.\",\n",
        "        \"Kata Tjuta rock formations complement Uluru in the heart of Australia.\",\n",
        "        \"Darwin serves as the gateway to Kakadu National Park and Top End wilderness.\",\n",
        "        \"Kakadu National Park preserves ancient Aboriginal rock art and diverse ecosystems.\",\n",
        "        \"Alice Springs is the heart of the Australian outback and Red Centre.\",\n",
        "        \n",
        "        # Tasmania content\n",
        "        \"Hobart's Museum of Old and New Art challenges visitors with provocative contemporary art.\",\n",
        "        \"Cradle Mountain-Lake St Clair National Park offers pristine wilderness hiking.\",\n",
        "        \"Salamanca Market in Hobart features local artisans and Tasmania's finest produce.\",\n",
        "        \"Devil's island Tasmania protects the endangered Tasmanian devil in natural habitat.\",\n",
        "        \n",
        "        # ACT content\n",
        "        \"Canberra houses Australia's national institutions including Parliament House and galleries.\",\n",
        "        \"Australian War Memorial commemorates the service of Australian armed forces.\",\n",
        "        \"National Gallery of Australia showcases the finest Australian and international art.\",\n",
        "        \"Lake Burley Griffin provides recreational activities in the heart of Canberra.\"\n",
        "    ]\n",
        "    \n",
        "    # Vietnamese corpus - translations and local content\n",
        "    vietnamese_corpus = [\n",
        "        # Sydney translations\n",
        "        \"Nh\u00e0 h\u00e1t Opera Sydney l\u00e0 ki\u1ec7t t\u00e1c ki\u1ebfn tr\u00fac bi\u1ec3u t\u01b0\u1ee3ng t\u1ecda l\u1ea1c t\u1ea1i Bennelong Point \u1edf C\u1ea3ng Sydney.\",\n",
        "        \"C\u1ea7u C\u1ea3ng Sydney mang \u0111\u1ebfn t\u1ea7m nh\u00ecn ngo\u1ea1n m\u1ee5c ra c\u1ea3ng v\u00e0 \u0111\u01b0\u1eddng ch\u00e2n tr\u1eddi th\u00e0nh ph\u1ed1.\",\n",
        "        \"B\u00e3i bi\u1ec3n Bondi n\u1ed5i ti\u1ebfng v\u1edbi l\u01b0\u1edbt s\u00f3ng v\u00e0 t\u1ed5 ch\u1ee9c nhi\u1ec1u cu\u1ed9c thi l\u01b0\u1edbt s\u00f3ng qu\u1ed1c t\u1ebf.\",\n",
        "        \"V\u01b0\u1eddn B\u00e1ch th\u1ea3o Ho\u00e0ng gia Sydney tr\u01b0ng b\u00e0y h\u1ec7 \u0111\u1ed9ng th\u1ef1c v\u1eadt b\u1ea3n \u0111\u1ecba Australia.\",\n",
        "        \n",
        "        # Melbourne translations\n",
        "        \"Melbourne n\u1ed5i ti\u1ebfng v\u1edbi v\u0103n h\u00f3a c\u00e0 ph\u00ea s\u00f4i \u0111\u1ed9ng v\u00e0 ngh\u1ec7 thu\u1eadt \u0111\u01b0\u1eddng ph\u1ed1 trong c\u00e1c con h\u1ebbm.\",\n",
        "        \"T\u00f2a nh\u00e0 Tri\u1ec3n l\u00e3m Ho\u00e0ng gia \u1edf Carlton Gardens l\u00e0 di s\u1ea3n th\u1ebf gi\u1edbi UNESCO.\",\n",
        "        \"Qu\u1ea3ng tr\u01b0\u1eddng Federation t\u1ed5 ch\u1ee9c c\u00e1c s\u1ef1 ki\u1ec7n v\u0103n h\u00f3a v\u00e0 c\u00f3 c\u00e1c ph\u00f2ng tr\u01b0ng b\u00e0y l\u1edbn.\",\n",
        "        \"M\u1ea1ng l\u01b0\u1edbi t\u00e0u \u0111i\u1ec7n Melbourne l\u00e0 l\u1edbn nh\u1ea5t th\u1ebf gi\u1edbi v\u00e0 mang t\u00ednh bi\u1ec3u t\u01b0\u1ee3ng c\u1ee7a th\u00e0nh ph\u1ed1.\",\n",
        "        \n",
        "        # Queensland translations\n",
        "        \"R\u1ea1n san h\u00f4 Great Barrier l\u00e0 h\u1ec7 th\u1ed1ng r\u1ea1n san h\u00f4 l\u1edbn nh\u1ea5t th\u1ebf gi\u1edbi v\u00e0 di s\u1ea3n UNESCO.\",\n",
        "        \"South Bank Brisbane c\u00f3 c\u00e1c t\u1ed5 ch\u1ee9c v\u0103n h\u00f3a, nh\u00e0 h\u00e0ng v\u00e0 c\u00f4ng vi\u00ean ven s\u00f4ng.\",\n",
        "        \"Gold Coast n\u1ed5i ti\u1ebfng v\u1edbi c\u00e1c c\u00f4ng vi\u00ean gi\u1ea3i tr\u00ed, b\u00e3i bi\u1ec3n l\u01b0\u1edbt s\u00f3ng v\u00e0 cu\u1ed9c s\u1ed1ng v\u1ec1 \u0111\u00eam.\",\n",
        "        \"Cairns l\u00e0 c\u1eeda ng\u00f5 \u0111\u1ebfn R\u1ea1n san h\u00f4 Great Barrier v\u00e0 R\u1eebng m\u01b0a Daintree.\",\n",
        "        \n",
        "        # Other regions\n",
        "        \"Perth l\u00e0 m\u1ed9t trong nh\u1eefng th\u00e0nh ph\u1ed1 l\u1edbn bi\u1ec7t l\u1eadp nh\u1ea5t tr\u00ean th\u1ebf gi\u1edbi.\",\n",
        "        \"Adelaide \u0111\u01b0\u1ee3c bi\u1ebft \u0111\u1ebfn l\u00e0 Th\u00e0nh ph\u1ed1 L\u1ec5 h\u1ed9i v\u1edbi nhi\u1ec1u celebration v\u0103n h\u00f3a.\",\n",
        "        \"Uluru l\u00e0 \u0111\u1ecba \u0111i\u1ec3m thi\u00eang li\u00eang c\u1ee7a th\u1ed5 d\u00e2n v\u00e0 bi\u1ec3u t\u01b0\u1ee3ng c\u1ee7a Australia.\",\n",
        "        \"Hobart c\u00f3 B\u1ea3o t\u00e0ng Ngh\u1ec7 thu\u1eadt C\u0169 v\u00e0 M\u1edbi th\u00e1ch th\u1ee9c du kh\u00e1ch v\u1edbi ngh\u1ec7 thu\u1eadt \u0111\u01b0\u01a1ng \u0111\u1ea1i.\",\n",
        "        \"Canberra ch\u1ee9a c\u00e1c t\u1ed5 ch\u1ee9c qu\u1ed1c gia c\u1ee7a Australia bao g\u1ed3m T\u00f2a nh\u00e0 Qu\u1ed1c h\u1ed9i.\"\n",
        "    ]\n",
        "    \n",
        "    return {\n",
        "        'english': english_corpus,\n",
        "        'vietnamese': vietnamese_corpus,\n",
        "        'combined': english_corpus + vietnamese_corpus\n",
        "    }\n",
        "\n",
        "# Create the corpus\n",
        "tourism_corpus = create_australian_tourism_corpus()\n",
        "\n",
        "print(\"\ud83c\udde6\ud83c\uddfa Australian Tourism Corpus Created\")\n",
        "print(\"=\" * 45)\n",
        "print(f\"   English texts: {len(tourism_corpus['english'])}\")\n",
        "print(f\"   Vietnamese texts: {len(tourism_corpus['vietnamese'])}\")\n",
        "print(f\"   Total corpus size: {len(tourism_corpus['combined'])}\")\n",
        "\n",
        "# Display sample texts\n",
        "print(f\"\\n\ud83d\udcdd Sample English text:\")\n",
        "print(f\"   {tourism_corpus['english'][0]}\")\n",
        "print(f\"\\n\ud83d\udcdd Sample Vietnamese text:\")\n",
        "print(f\"   {tourism_corpus['vietnamese'][0]}\")\n",
        "\n",
        "# Analyze vocabulary\n",
        "all_words = []\n",
        "for text in tourism_corpus['combined']:\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    all_words.extend(words)\n",
        "\n",
        "vocab_counter = Counter(all_words)\n",
        "unique_words = len(vocab_counter)\n",
        "total_words = len(all_words)\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Corpus Statistics:\")\n",
        "print(f\"   Total words: {total_words:,}\")\n",
        "print(f\"   Unique words: {unique_words:,}\")\n",
        "print(f\"   Vocabulary richness: {unique_words/total_words:.3f}\")\n",
        "\n",
        "# Show most common Australian terms\n",
        "australian_terms = [word for word, count in vocab_counter.most_common(20) \n",
        "                   if word in ['sydney', 'melbourne', 'brisbane', 'perth', 'adelaide', \n",
        "                              'darwin', 'hobart', 'canberra', 'australia', 'australian',\n",
        "                              'beach', 'harbour', 'reef', 'park', 'island']]\n",
        "print(f\"\\n\ud83c\udfd9\ufe0f  Top Australian terms: {', '.join(australian_terms[:10])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AustralianEmbeddingPreprocessor:\n",
        "    \"\"\"\n",
        "    Specialized text preprocessor for Australian tourism embeddings.\n",
        "    \n",
        "    Handles both English and Vietnamese text while preserving important\n",
        "    Australian geographic and cultural terms.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Protected Australian terms that should not be heavily modified\n",
        "        self.protected_terms = {\n",
        "            'cities': ['sydney', 'melbourne', 'brisbane', 'perth', 'adelaide', \n",
        "                      'darwin', 'hobart', 'canberra'],\n",
        "            'landmarks': ['uluru', 'kata', 'tjuta', 'kakadu', 'pinnacles', \n",
        "                         'cradle', 'mountain', 'fraser', 'rottnest'],\n",
        "            'features': ['harbour', 'reef', 'outback', 'rainforest', 'desert',\n",
        "                        'beach', 'island', 'river', 'park', 'gardens'],\n",
        "            'cultural': ['aboriginal', 'indigenous', 'heritage', 'colonial',\n",
        "                        'convict', 'federation', 'anzac']\n",
        "        }\n",
        "        \n",
        "        # Vietnamese-specific terms to preserve\n",
        "        self.vietnamese_terms = ['nh\u00e0', 'h\u00e1t', 'opera', 'c\u1ea7u', 'c\u1ea3ng', 'b\u00e3i', 'bi\u1ec3n',\n",
        "                               'v\u01b0\u1eddn', 'b\u00e1ch', 'th\u1ea3o', 'r\u1ea1n', 'san', 'h\u00f4', 'th\u00e0nh', 'ph\u1ed1']\n",
        "    \n",
        "    def tokenize_sentence(self, text):\n",
        "        \"\"\"\n",
        "        Tokenize text into sentences, preserving Australian terms.\n",
        "        \n",
        "        Args:\n",
        "            text (str): Input text\n",
        "            \n",
        "        Returns:\n",
        "            list: List of tokenized words\n",
        "        \"\"\"\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "        \n",
        "        # Remove punctuation but preserve apostrophes in contractions\n",
        "        text = re.sub(r\"[^\\w\\s']\", ' ', text)\n",
        "        \n",
        "        # Handle contractions\n",
        "        contractions = {\n",
        "            \"n't\": \" not\",\n",
        "            \"'re\": \" are\", \n",
        "            \"'s\": \" is\",\n",
        "            \"'ve\": \" have\",\n",
        "            \"'ll\": \" will\",\n",
        "            \"'d\": \" would\"\n",
        "        }\n",
        "        \n",
        "        for contraction, expansion in contractions.items():\n",
        "            text = text.replace(contraction, expansion)\n",
        "        \n",
        "        # Split into words\n",
        "        words = text.split()\n",
        "        \n",
        "        # Filter out very short words (except important ones)\n",
        "        important_short_words = {'wa', 'sa', 'nt', 'tas', 'act', 'nsw', 'vic', 'qld'}\n",
        "        words = [word for word in words \n",
        "                if len(word) > 2 or word in important_short_words]\n",
        "        \n",
        "        return words\n",
        "    \n",
        "    def prepare_training_data(self, corpus):\n",
        "        \"\"\"\n",
        "        Prepare corpus for embedding training.\n",
        "        \n",
        "        Args:\n",
        "            corpus (list): List of text documents\n",
        "            \n",
        "        Returns:\n",
        "            list: List of tokenized sentences\n",
        "        \"\"\"\n",
        "        tokenized_corpus = []\n",
        "        \n",
        "        for text in corpus:\n",
        "            # Split into sentences\n",
        "            sentences = re.split(r'[.!?]+', text)\n",
        "            \n",
        "            for sentence in sentences:\n",
        "                sentence = sentence.strip()\n",
        "                if len(sentence) > 10:  # Skip very short sentences\n",
        "                    tokens = self.tokenize_sentence(sentence)\n",
        "                    if len(tokens) >= 3:  # Minimum sentence length\n",
        "                        tokenized_corpus.append(tokens)\n",
        "        \n",
        "        return tokenized_corpus\n",
        "    \n",
        "    def analyze_vocabulary(self, tokenized_corpus):\n",
        "        \"\"\"\n",
        "        Analyze vocabulary statistics from tokenized corpus.\n",
        "        \n",
        "        Args:\n",
        "            tokenized_corpus (list): List of tokenized sentences\n",
        "            \n",
        "        Returns:\n",
        "            dict: Vocabulary statistics\n",
        "        \"\"\"\n",
        "        all_words = []\n",
        "        for sentence in tokenized_corpus:\n",
        "            all_words.extend(sentence)\n",
        "        \n",
        "        word_freq = Counter(all_words)\n",
        "        \n",
        "        # Find Australian-specific terms\n",
        "        australian_words = []\n",
        "        for word, freq in word_freq.items():\n",
        "            if any(word in terms for terms in self.protected_terms.values()):\n",
        "                australian_words.append((word, freq))\n",
        "        \n",
        "        # Find Vietnamese terms\n",
        "        vietnamese_words = [(word, freq) for word, freq in word_freq.items() \n",
        "                           if word in self.vietnamese_terms]\n",
        "        \n",
        "        return {\n",
        "            'total_words': len(all_words),\n",
        "            'unique_words': len(word_freq),\n",
        "            'word_frequencies': word_freq,\n",
        "            'australian_terms': australian_words,\n",
        "            'vietnamese_terms': vietnamese_words,\n",
        "            'avg_sentence_length': np.mean([len(s) for s in tokenized_corpus])\n",
        "        }\n",
        "\n",
        "# Initialize preprocessor and prepare data\n",
        "preprocessor = AustralianEmbeddingPreprocessor()\n",
        "\n",
        "# Prepare training data\n",
        "tokenized_corpus = preprocessor.prepare_training_data(tourism_corpus['combined'])\n",
        "vocab_stats = preprocessor.analyze_vocabulary(tokenized_corpus)\n",
        "\n",
        "print(\"\ud83d\udd24 Text Preprocessing for Australian Tourism Embeddings\")\n",
        "print(\"=\" * 55)\n",
        "print(f\"   Total sentences: {len(tokenized_corpus)}\")\n",
        "print(f\"   Total words: {vocab_stats['total_words']:,}\")\n",
        "print(f\"   Unique vocabulary: {vocab_stats['unique_words']:,}\")\n",
        "print(f\"   Average sentence length: {vocab_stats['avg_sentence_length']:.1f} words\")\n",
        "\n",
        "print(f\"\\n\ud83c\udde6\ud83c\uddfa Australian terms found: {len(vocab_stats['australian_terms'])}\")\n",
        "australian_terms_str = ', '.join([term for term, freq in vocab_stats['australian_terms'][:10]])\n",
        "print(f\"   Top terms: {australian_terms_str}\")\n",
        "\n",
        "print(f\"\\n\ud83c\uddfb\ud83c\uddf3 Vietnamese terms found: {len(vocab_stats['vietnamese_terms'])}\")\n",
        "if vocab_stats['vietnamese_terms']:\n",
        "    vietnamese_terms_str = ', '.join([term for term, freq in vocab_stats['vietnamese_terms'][:5]])\n",
        "    print(f\"   Sample terms: {vietnamese_terms_str}\")\n",
        "\n",
        "# Show sample tokenized sentences\n",
        "print(f\"\\n\ud83d\udcdd Sample tokenized sentences:\")\n",
        "for i, sentence in enumerate(tokenized_corpus[:3]):\n",
        "    print(f\"   {i+1}. {sentence[:8]}... ({len(sentence)} words)\")\n",
        "\n",
        "print(f\"\\n\u2705 Preprocessing completed - ready for embedding training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AustralianWord2Vec(nn.Module):\n",
        "    \"\"\"\n",
        "    PyTorch implementation of Word2Vec Skip-gram model for Australian tourism corpus.\n",
        "    \n",
        "    TensorFlow equivalent:\n",
        "        embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_dim)\n",
        "        \n",
        "    This implementation uses:\n",
        "    - Skip-gram architecture for better rare word representation\n",
        "    - Negative sampling for efficient training\n",
        "    - Australian tourism vocabulary optimization\n",
        "    \n",
        "    Args:\n",
        "        vocab_size (int): Size of vocabulary\n",
        "        embed_dim (int): Embedding dimension (typically 100-300)\n",
        "        context_window (int): Context window size (typically 5-10)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, embed_dim=200, context_window=5):\n",
        "        super(AustralianWord2Vec, self).__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.context_window = context_window\n",
        "        \n",
        "        # Input embeddings (center words)\n",
        "        self.in_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
        "        \n",
        "        # Output embeddings (context words)\n",
        "        self.out_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
        "        \n",
        "        # Initialize embeddings with small random values\n",
        "        self._init_embeddings()\n",
        "        \n",
        "        # Store Australian cities for analysis\n",
        "        self.australian_cities = ['sydney', 'melbourne', 'brisbane', 'perth', \n",
        "                                'adelaide', 'darwin', 'hobart', 'canberra']\n",
        "    \n",
        "    def _init_embeddings(self):\n",
        "        \"\"\"Initialize embedding weights.\"\"\"\n",
        "        # Initialize with small random values\n",
        "        nn.init.uniform_(self.in_embeddings.weight, -0.5/self.embed_dim, 0.5/self.embed_dim)\n",
        "        nn.init.uniform_(self.out_embeddings.weight, -0.5/self.embed_dim, 0.5/self.embed_dim)\n",
        "    \n",
        "    def forward(self, center_words, context_words, negative_words=None):\n",
        "        \"\"\"\n",
        "        Forward pass for Word2Vec training.\n",
        "        \n",
        "        Args:\n",
        "            center_words (torch.Tensor): Center word indices [batch_size]\n",
        "            context_words (torch.Tensor): Context word indices [batch_size]\n",
        "            negative_words (torch.Tensor): Negative sample indices [batch_size, num_negative]\n",
        "            \n",
        "        Returns:\n",
        "            torch.Tensor: Loss value\n",
        "        \"\"\"\n",
        "        batch_size = center_words.size(0)\n",
        "        \n",
        "        # Get center word embeddings\n",
        "        center_embeds = self.in_embeddings(center_words)  # [batch_size, embed_dim]\n",
        "        \n",
        "        # Get context word embeddings  \n",
        "        context_embeds = self.out_embeddings(context_words)  # [batch_size, embed_dim]\n",
        "        \n",
        "        # Positive samples score\n",
        "        pos_score = torch.sum(center_embeds * context_embeds, dim=1)  # [batch_size]\n",
        "        pos_loss = -F.logsigmoid(pos_score).mean()\n",
        "        \n",
        "        # Negative sampling loss\n",
        "        neg_loss = 0\n",
        "        if negative_words is not None:\n",
        "            num_negative = negative_words.size(1)\n",
        "            \n",
        "            # Get negative word embeddings\n",
        "            neg_embeds = self.out_embeddings(negative_words)  # [batch_size, num_negative, embed_dim]\n",
        "            \n",
        "            # Compute negative scores\n",
        "            center_embeds_expanded = center_embeds.unsqueeze(1).expand(-1, num_negative, -1)\n",
        "            neg_scores = torch.sum(center_embeds_expanded * neg_embeds, dim=2)  # [batch_size, num_negative]\n",
        "            \n",
        "            neg_loss = -F.logsigmoid(-neg_scores).mean()\n",
        "        \n",
        "        return pos_loss + neg_loss\n",
        "    \n",
        "    def get_word_embeddings(self):\n",
        "        \"\"\"Get trained word embeddings.\"\"\"\n",
        "        return self.in_embeddings.weight.data\n",
        "    \n",
        "    def similarity(self, word1_idx, word2_idx):\n",
        "        \"\"\"Compute cosine similarity between two words.\"\"\"\n",
        "        embeddings = self.get_word_embeddings()\n",
        "        \n",
        "        emb1 = embeddings[word1_idx]\n",
        "        emb2 = embeddings[word2_idx]\n",
        "        \n",
        "        # Cosine similarity\n",
        "        cos_sim = F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0))\n",
        "        return cos_sim.item()\n",
        "    \n",
        "    def most_similar(self, word_idx, word_to_idx, idx_to_word, top_k=10):\n",
        "        \"\"\"Find most similar words to a given word.\"\"\"\n",
        "        embeddings = self.get_word_embeddings()\n",
        "        word_embed = embeddings[word_idx].unsqueeze(0)\n",
        "        \n",
        "        # Compute similarities with all words\n",
        "        similarities = F.cosine_similarity(word_embed, embeddings)\n",
        "        \n",
        "        # Get top-k most similar (excluding the word itself)\n",
        "        similarities[word_idx] = -1  # Exclude the word itself\n",
        "        top_indices = similarities.topk(top_k).indices\n",
        "        \n",
        "        similar_words = []\n",
        "        for idx in top_indices:\n",
        "            word = idx_to_word.get(idx.item(), '<UNK>')\n",
        "            similarity_score = similarities[idx].item()\n",
        "            similar_words.append((word, similarity_score))\n",
        "        \n",
        "        return similar_words\n",
        "\n",
        "print(\"\ud83d\udd24 Australian Word2Vec model class defined!\")\n",
        "print(\"   Architecture: Skip-gram with negative sampling\")\n",
        "print(\"   Optimized for: Australian tourism vocabulary\")\n",
        "print(\"   Features: Similarity computation, most similar words\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Word2VecDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for Word2Vec training with Australian tourism corpus.\n",
        "    \n",
        "    Generates (center_word, context_word) pairs for skip-gram training.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, tokenized_corpus, word_to_idx, context_window=5, num_negative=5):\n",
        "        self.tokenized_corpus = tokenized_corpus\n",
        "        self.word_to_idx = word_to_idx\n",
        "        self.idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "        self.context_window = context_window\n",
        "        self.num_negative = num_negative\n",
        "        self.vocab_size = len(word_to_idx)\n",
        "        \n",
        "        # Generate training pairs\n",
        "        self.training_pairs = self._generate_training_pairs()\n",
        "        \n",
        "        # Create word frequency table for negative sampling\n",
        "        self.word_freqs = self._build_frequency_table()\n",
        "        \n",
        "    def _generate_training_pairs(self):\n",
        "        \"\"\"Generate (center, context) word pairs.\"\"\"\n",
        "        pairs = []\n",
        "        \n",
        "        for sentence in self.tokenized_corpus:\n",
        "            # Convert words to indices\n",
        "            word_indices = [self.word_to_idx.get(word, self.word_to_idx.get('<UNK>', 0)) \n",
        "                           for word in sentence]\n",
        "            \n",
        "            # Generate context pairs\n",
        "            for center_idx, center_word_idx in enumerate(word_indices):\n",
        "                # Define context window\n",
        "                start = max(0, center_idx - self.context_window)\n",
        "                end = min(len(word_indices), center_idx + self.context_window + 1)\n",
        "                \n",
        "                # Generate pairs\n",
        "                for context_idx in range(start, end):\n",
        "                    if context_idx != center_idx:\n",
        "                        pairs.append((center_word_idx, word_indices[context_idx]))\n",
        "        \n",
        "        return pairs\n",
        "    \n",
        "    def _build_frequency_table(self):\n",
        "        \"\"\"Build word frequency table for negative sampling.\"\"\"\n",
        "        word_counts = Counter()\n",
        "        \n",
        "        for sentence in self.tokenized_corpus:\n",
        "            for word in sentence:\n",
        "                word_counts[word] += 1\n",
        "        \n",
        "        # Convert to frequency distribution\n",
        "        total_words = sum(word_counts.values())\n",
        "        word_freqs = np.zeros(self.vocab_size)\n",
        "        \n",
        "        for word, count in word_counts.items():\n",
        "            if word in self.word_to_idx:\n",
        "                idx = self.word_to_idx[word]\n",
        "                # Use subsampling for frequent words (power = 0.75)\n",
        "                word_freqs[idx] = (count / total_words) ** 0.75\n",
        "        \n",
        "        # Normalize\n",
        "        word_freqs = word_freqs / word_freqs.sum()\n",
        "        \n",
        "        return word_freqs\n",
        "    \n",
        "    def _negative_sampling(self, batch_size):\n",
        "        \"\"\"Generate negative samples.\"\"\"\n",
        "        negative_samples = np.random.choice(\n",
        "            self.vocab_size, \n",
        "            size=(batch_size, self.num_negative),\n",
        "            p=self.word_freqs\n",
        "        )\n",
        "        return torch.LongTensor(negative_samples)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.training_pairs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        center_word, context_word = self.training_pairs[idx]\n",
        "        return torch.LongTensor([center_word]), torch.LongTensor([context_word])\n",
        "\n",
        "# Build vocabulary from tokenized corpus\n",
        "def build_vocabulary(tokenized_corpus, min_count=2):\n",
        "    \"\"\"Build vocabulary with minimum word frequency threshold.\"\"\"\n",
        "    word_counts = Counter()\n",
        "    \n",
        "    # Count word frequencies\n",
        "    for sentence in tokenized_corpus:\n",
        "        for word in sentence:\n",
        "            word_counts[word] += 1\n",
        "    \n",
        "    # Filter by minimum count\n",
        "    filtered_words = {word: count for word, count in word_counts.items() \n",
        "                     if count >= min_count}\n",
        "    \n",
        "    # Create word-to-index mapping\n",
        "    word_to_idx = {'<UNK>': 0}  # Unknown words\n",
        "    idx_to_word = {0: '<UNK>'}\n",
        "    \n",
        "    for idx, word in enumerate(sorted(filtered_words.keys()), 1):\n",
        "        word_to_idx[word] = idx\n",
        "        idx_to_word[idx] = word\n",
        "    \n",
        "    return word_to_idx, idx_to_word, filtered_words\n",
        "\n",
        "# Build vocabulary for Australian tourism corpus\n",
        "word_to_idx, idx_to_word, word_counts = build_vocabulary(tokenized_corpus, min_count=2)\n",
        "vocab_size = len(word_to_idx)\n",
        "\n",
        "print(\"\ud83d\udcda Australian Tourism Vocabulary Built\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"   Vocabulary size: {vocab_size:,}\")\n",
        "print(f\"   Total training pairs: {len(tokenized_corpus)} sentences\")\n",
        "\n",
        "# Show sample vocabulary\n",
        "print(f\"\\n\ud83c\udde6\ud83c\uddfa Sample Australian words in vocabulary:\")\n",
        "australian_sample = [word for word in word_to_idx.keys() \n",
        "                    if word in ['sydney', 'melbourne', 'brisbane', 'perth', 'adelaide', \n",
        "                               'darwin', 'hobart', 'canberra', 'australia', 'australian']]\n",
        "print(f\"   Cities: {', '.join(australian_sample[:8])}\")\n",
        "\n",
        "# Show most frequent words\n",
        "most_frequent = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "print(f\"\\n\ud83d\udcca Most frequent words:\")\n",
        "for word, count in most_frequent:\n",
        "    print(f\"   {word}: {count}\")\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = Word2VecDataset(tokenized_corpus, word_to_idx, context_window=5, num_negative=5)\n",
        "dataloader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=0)\n",
        "\n",
        "print(f\"\\n\u26a1 Dataset created:\")\n",
        "print(f\"   Training pairs: {len(dataset):,}\")\n",
        "print(f\"   Batch size: 256\")\n",
        "print(f\"   Context window: 5\")\n",
        "print(f\"   Negative samples: 5\")\n",
        "\n",
        "print(f\"\\n\u2705 Ready for Word2Vec training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Word2Vec model\n",
        "embed_dim = 200 if DEVICE.type != 'cpu' else 100  # Adjust based on device\n",
        "model = AustralianWord2Vec(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=embed_dim,\n",
        "    context_window=5\n",
        ").to(DEVICE)\n",
        "\n",
        "# Training configuration\n",
        "learning_rate = 0.001 if DEVICE.type != 'cpu' else 0.002\n",
        "num_epochs = 10 if DEVICE.type != 'cpu' else 5\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# TensorBoard setup\n",
        "from datetime import datetime\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "log_dir = f\"runs/australian_word2vec_{timestamp}\"\n",
        "writer = SummaryWriter(log_dir)\n",
        "\n",
        "print(f\"\ud83c\udfcb\ufe0f Training Australian Tourism Word2Vec\")\n",
        "print(\"=\" * 45)\n",
        "print(f\"   Model: Skip-gram with negative sampling\")\n",
        "print(f\"   Embedding dimension: {embed_dim}\")\n",
        "print(f\"   Vocabulary size: {vocab_size:,}\")\n",
        "print(f\"   Learning rate: {learning_rate}\")\n",
        "print(f\"   Epochs: {num_epochs}\")\n",
        "print(f\"   Device: {DEVICE}\")\n",
        "print(f\"   TensorBoard logs: {log_dir}\")\n",
        "\n",
        "def train_word2vec(model, dataloader, optimizer, writer, num_epochs, device):\n",
        "    \"\"\"Train Word2Vec model with Australian tourism corpus.\"\"\"\n",
        "    \n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    step = 0\n",
        "    \n",
        "    print(f\"\\n\ud83d\ude80 Starting Word2Vec training...\")\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        epoch_steps = 0\n",
        "        \n",
        "        for batch_idx, (center_words, context_words) in enumerate(dataloader):\n",
        "            # Move to device\n",
        "            center_words = center_words.squeeze().to(device)\n",
        "            context_words = context_words.squeeze().to(device)\n",
        "            \n",
        "            # Generate negative samples\n",
        "            batch_size = center_words.size(0)\n",
        "            negative_words = dataset._negative_sampling(batch_size).to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss = model(center_words, context_words, negative_words)\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "            # Accumulate loss\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_steps += 1\n",
        "            step += 1\n",
        "            \n",
        "            # Log to TensorBoard\n",
        "            if batch_idx % 100 == 0:\n",
        "                writer.add_scalar('Loss/Batch', loss.item(), step)\n",
        "                \n",
        "                # Log some embedding norms\n",
        "                if batch_idx % 500 == 0:\n",
        "                    with torch.no_grad():\n",
        "                        embed_norms = torch.norm(model.in_embeddings.weight, dim=1).mean()\n",
        "                        writer.add_scalar('Embeddings/Average_Norm', embed_norms.item(), step)\n",
        "        \n",
        "        # Calculate average epoch loss\n",
        "        avg_epoch_loss = epoch_loss / epoch_steps\n",
        "        \n",
        "        # Log epoch metrics\n",
        "        writer.add_scalar('Loss/Epoch', avg_epoch_loss, epoch)\n",
        "        \n",
        "        print(f\"   Epoch {epoch+1:2d}/{num_epochs}: Loss = {avg_epoch_loss:.6f}\")\n",
        "        \n",
        "        # Log embedding samples for specific Australian words\n",
        "        if epoch % 2 == 0:  # Every 2 epochs\n",
        "            with torch.no_grad():\n",
        "                for city in ['sydney', 'melbourne', 'brisbane']:\n",
        "                    if city in word_to_idx:\n",
        "                        city_idx = word_to_idx[city]\n",
        "                        city_embedding = model.in_embeddings.weight[city_idx]\n",
        "                        writer.add_histogram(f'Embeddings/{city.title()}', city_embedding, epoch)\n",
        "    \n",
        "    writer.close()\n",
        "    print(f\"\\n\ud83c\udf89 Word2Vec training completed!\")\n",
        "    print(f\"   Final average loss: {avg_epoch_loss:.6f}\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "trained_model = train_word2vec(model, dataloader, optimizer, writer, num_epochs, DEVICE)\n",
        "\n",
        "# Save the trained model\n",
        "torch.save({\n",
        "    'model_state_dict': trained_model.state_dict(),\n",
        "    'word_to_idx': word_to_idx,\n",
        "    'idx_to_word': idx_to_word,\n",
        "    'embed_dim': embed_dim,\n",
        "    'vocab_size': vocab_size\n",
        "}, 'australian_word2vec_model.pth')\n",
        "\n",
        "print(f\"\\n\ud83d\udcbe Model saved as: australian_word2vec_model.pth\")\n",
        "print(f\"\ud83d\udcca TensorBoard logs available at: {log_dir}\")\n",
        "print(f\"   Run: tensorboard --logdir {log_dir}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}