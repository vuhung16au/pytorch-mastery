{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP From Scratch: Translation with a Sequence to Sequence Network and Attention üá¶üá∫\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/pytorch-mastery/blob/main/examples/pytorch-nlp/translation-seq2seq-network-attention.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/pytorch-mastery/blob/main/examples/pytorch-nlp/translation-seq2seq-network-attention.ipynb)\n",
    "\n",
    "Build a sequence-to-sequence neural machine translation model from scratch using PyTorch. Features advanced attention mechanisms for English-Vietnamese translation with Australian tourism context, comparing PyTorch implementation patterns with TensorFlow approaches.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "\n",
    "- üîÑ **Master Seq2Seq Architecture** - Understand encoder-decoder neural translation models\n",
    "- üéØ **Implement Attention Mechanisms** - Build attention layers for better translation quality\n",
    "- üá¶üá∫ **Handle Australian Tourism Data** - Process English-Vietnamese translation pairs\n",
    "- üìä **Visualize Attention Weights** - Understand what the model focuses on during translation\n",
    "- üîß **Compare with TensorFlow** - Learn PyTorch vs TensorFlow implementation differences\n",
    "- üìà **Monitor Training Progress** - Use TensorBoard for comprehensive training visualization\n",
    "- üåê **Evaluate Translation Quality** - Implement BLEU score and other translation metrics\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "1. **English-Vietnamese Seq2Seq Translator** - Neural machine translation for Australian tourism content\n",
    "2. **Attention Visualization** - See what parts of source sentences the model focuses on\n",
    "3. **Interactive Translation Demo** - Test translations with real Australian tourism examples\n",
    "4. **Training Monitoring** - Complete TensorBoard integration for loss and attention visualization\n",
    "5. **Performance Evaluation** - BLEU scores and translation quality assessment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Detection and Setup\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Detect the runtime environment\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules or \"kaggle\" in os.environ.get('KAGGLE_URL_BASE', '')\n",
    "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
    "\n",
    "print(f\"üîç Environment Detection:\")\n",
    "print(f\"   Local Development: {IS_LOCAL}\")\n",
    "print(f\"   Google Colab: {IS_COLAB}\")\n",
    "print(f\"   Kaggle Notebooks: {IS_KAGGLE}\")\n",
    "\n",
    "# Platform-specific system setup\n",
    "if IS_COLAB:\n",
    "    print(\"\\n‚öôÔ∏è  Setting up Google Colab environment...\")\n",
    "    !apt update -qq\n",
    "    !apt install -y -qq software-properties-common\n",
    "elif IS_KAGGLE:\n",
    "    print(\"\\n‚öôÔ∏è  Setting up Kaggle environment...\")\n",
    "else:\n",
    "    print(\"\\n‚öôÔ∏è  Setting up local environment...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for sequence-to-sequence translation\n",
    "required_packages = [\n",
    "    \"torch\",\n",
    "    \"pandas\",\n",
    "    \"seaborn\", \n",
    "    \"matplotlib\",\n",
    "    \"tensorboard\",\n",
    "    \"scikit-learn\",\n",
    "    \"nltk\",  # For BLEU score evaluation\n",
    "    \"plotly\",  # For attention visualization\n",
    "]\n",
    "\n",
    "print(\"üì¶ Installing packages for seq2seq translation...\")\n",
    "for package in required_packages:\n",
    "    if IS_COLAB or IS_KAGGLE:\n",
    "        !pip install -q {package}\n",
    "    else:\n",
    "        try:\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
    "                          capture_output=True, check=True)\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"   ‚ö†Ô∏è  {package} installation skipped (likely already installed)\")\n",
    "\n",
    "print(\"‚úÖ Package installation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Standard data science stack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Text processing and evaluation\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import unicodedata\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# NLTK for BLEU score evaluation\n",
    "try:\n",
    "    import nltk\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    print(\"‚úÖ NLTK imported successfully for BLEU evaluation\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  NLTK not available - BLEU scores will be computed manually\")\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"Set2\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"üìö Successfully imported all libraries for seq2seq translation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import platform\n",
    "\n",
    "def detect_device():\n",
    "    \"\"\"\n",
    "    Detect the best available PyTorch device with comprehensive hardware support.\n",
    "    \n",
    "    Priority order:\n",
    "    1. CUDA (NVIDIA GPUs) - Best performance for deep learning\n",
    "    2. MPS (Apple Silicon) - Optimized for M1/M2/M3 Macs  \n",
    "    3. CPU (Universal) - Always available fallback\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: The optimal device for PyTorch operations\n",
    "        str: Human-readable device description for logging\n",
    "    \"\"\"\n",
    "    # Check for CUDA (NVIDIA GPU)\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        device_info = f\"CUDA GPU: {gpu_name}\"\n",
    "        \n",
    "        # Additional CUDA info for optimization\n",
    "        cuda_version = torch.version.cuda\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        \n",
    "        print(f\"üöÄ Using CUDA acceleration\")\n",
    "        print(f\"   GPU: {gpu_name}\")\n",
    "        print(f\"   CUDA Version: {cuda_version}\")\n",
    "        print(f\"   GPU Memory: {gpu_memory:.1f} GB\")\n",
    "        \n",
    "        return device, device_info\n",
    "    \n",
    "    # Check for MPS (Apple Silicon)\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        device_info = \"Apple Silicon MPS\"\n",
    "        \n",
    "        # Get system info for Apple Silicon\n",
    "        system_info = platform.uname()\n",
    "        \n",
    "        print(f\"üçé Using Apple Silicon MPS acceleration\")\n",
    "        print(f\"   System: {system_info.system} {system_info.release}\")\n",
    "        print(f\"   Machine: {system_info.machine}\")\n",
    "        print(f\"   Processor: {system_info.processor}\")\n",
    "        \n",
    "        return device, device_info\n",
    "    \n",
    "    # Fallback to CPU\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        device_info = \"CPU (No GPU acceleration available)\"\n",
    "        \n",
    "        # Get CPU info for optimization guidance\n",
    "        cpu_count = torch.get_num_threads()\n",
    "        system_info = platform.uname()\n",
    "        \n",
    "        print(f\"üíª Using CPU (no GPU acceleration detected)\")\n",
    "        print(f\"   Processor: {system_info.processor}\")\n",
    "        print(f\"   PyTorch Threads: {cpu_count}\")\n",
    "        print(f\"   System: {system_info.system} {system_info.release}\")\n",
    "        \n",
    "        # Provide optimization suggestions for CPU-only setups\n",
    "        print(f\"\\nüí° CPU Optimization Tips:\")\n",
    "        print(f\"   ‚Ä¢ Reduce batch size to prevent memory issues\")\n",
    "        print(f\"   ‚Ä¢ Consider using smaller models for faster training\")\n",
    "        print(f\"   ‚Ä¢ Enable PyTorch optimizations: torch.set_num_threads({cpu_count})\")\n",
    "        \n",
    "        return device, device_info\n",
    "\n",
    "# Usage in the notebook\n",
    "device, device_info = detect_device()\n",
    "print(f\"\\n‚úÖ PyTorch device selected: {device}\")\n",
    "print(f\"üìä Device info: {device_info}\")\n",
    "\n",
    "# Set global device for the notebook\n",
    "DEVICE = device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÉÔ∏è Dataset: Australian Tourism English-Vietnamese Translation\n",
    "\n",
    "We'll create a comprehensive dataset of English-Vietnamese translation pairs focused on Australian tourism content. This dataset includes:\n",
    "\n",
    "- **Tourist attractions** in major Australian cities\n",
    "- **Travel experiences** and recommendations\n",
    "- **Cultural information** about Australian destinations\n",
    "- **Practical travel advice** for visitors\n",
    "\n",
    "This approach follows the repository's policy of using Australian context and Vietnamese as the secondary language for all multilingual examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Australian Tourism English-Vietnamese Translation Dataset\n",
    "class AustralianTourismTranslationDataset:\n",
    "    \"\"\"\n",
    "    English-Vietnamese translation pairs focused on Australian tourism content.\n",
    "    \n",
    "    This dataset follows repository guidelines:\n",
    "    - Australian context for all examples\n",
    "    - Vietnamese as the secondary language\n",
    "    - Tourism and cultural content focus\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.translation_pairs = [\n",
    "            # Sydney attractions and experiences\n",
    "            (\"The Sydney Opera House is a masterpiece of modern architecture.\", \n",
    "             \"Nh√† h√°t Opera Sydney l√† ki·ªát t√°c ki·∫øn tr√∫c hi·ªán ƒë·∫°i.\"),\n",
    "            (\"You can climb the Sydney Harbour Bridge for stunning city views.\", \n",
    "             \"B·∫°n c√≥ th·ªÉ leo c·∫ßu C·∫£ng Sydney ƒë·ªÉ ng·∫Øm nh√¨n th√†nh ph·ªë tuy·ªát ƒë·∫πp.\"),\n",
    "            (\"Bondi Beach is perfect for surfing and sunbathing.\", \n",
    "             \"B√£i bi·ªÉn Bondi ho√†n h·∫£o cho l∆∞·ªõt s√≥ng v√† t·∫Øm n·∫Øng.\"),\n",
    "            (\"The Royal Botanic Gardens offer peaceful walks near the harbor.\", \n",
    "             \"V∆∞·ªùn B√°ch th·∫£o Ho√†ng gia cung c·∫•p nh·ªØng con ƒë∆∞·ªùng y√™n tƒ©nh g·∫ßn c·∫£ng.\"),\n",
    "            \n",
    "            # Melbourne culture and attractions\n",
    "            (\"Melbourne is famous for its coffee culture and street art.\", \n",
    "             \"Melbourne n·ªïi ti·∫øng v·ªõi vƒÉn h√≥a c√† ph√™ v√† ngh·ªá thu·∫≠t ƒë∆∞·ªùng ph·ªë.\"),\n",
    "            (\"The laneways of Melbourne hide amazing cafes and galleries.\", \n",
    "             \"Nh·ªØng con h·∫ªm Melbourne ·∫©n ch·ª©a nh·ªØng qu√°n c√† ph√™ v√† ph√≤ng tranh tuy·ªát v·ªùi.\"),\n",
    "            (\"Queen Victoria Market is the largest open-air market in the Southern Hemisphere.\", \n",
    "             \"Ch·ª£ Queen Victoria l√† ch·ª£ tr·ªùi l·ªõn nh·∫•t ·ªü Nam b√°n c·∫ßu.\"),\n",
    "            (\"The Great Ocean Road starts from Melbourne and offers spectacular coastal views.\", \n",
    "             \"Con ƒë∆∞·ªùng Great Ocean b·∫Øt ƒë·∫ßu t·ª´ Melbourne v√† mang ƒë·∫øn t·∫ßm nh√¨n ven bi·ªÉn ngo·∫°n m·ª•c.\"),\n",
    "            \n",
    "            # Brisbane and Queensland\n",
    "            (\"Brisbane is the gateway to the Gold Coast and Sunshine Coast.\", \n",
    "             \"Brisbane l√† c·ª≠a ng√µ ƒë·∫øn Gold Coast v√† Sunshine Coast.\"),\n",
    "            (\"The Great Barrier Reef is accessible from Cairns in tropical Queensland.\", \n",
    "             \"R·∫°n san h√¥ Great Barrier c√≥ th·ªÉ ƒë·∫øn t·ª´ Cairns ·ªü Queensland nhi·ªát ƒë·ªõi.\"),\n",
    "            (\"Fraser Island is the world's largest sand island.\", \n",
    "             \"ƒê·∫£o Fraser l√† h√≤n ƒë·∫£o c√°t l·ªõn nh·∫•t th·∫ø gi·ªõi.\"),\n",
    "            (\"The Gold Coast theme parks offer thrilling rides and entertainment.\", \n",
    "             \"C√°c c√¥ng vi√™n gi·∫£i tr√≠ Gold Coast cung c·∫•p tr√≤ ch∆°i ly k·ª≥ v√† gi·∫£i tr√≠.\"),\n",
    "            \n",
    "            # Perth and Western Australia\n",
    "            (\"Perth has beautiful beaches and is one of the sunniest cities in the world.\", \n",
    "             \"Perth c√≥ nh·ªØng b√£i bi·ªÉn ƒë·∫πp v√† l√† m·ªôt trong nh·ªØng th√†nh ph·ªë n·∫Øng nh·∫•t th·∫ø gi·ªõi.\"),\n",
    "            (\"Rottnest Island near Perth is home to the friendly quokkas.\", \n",
    "             \"ƒê·∫£o Rottnest g·∫ßn Perth l√† nh√† c·ªßa nh·ªØng ch√∫ quokka th√¢n thi·ªán.\"),\n",
    "            (\"The Pinnacles Desert offers a unique landscape of limestone pillars.\", \n",
    "             \"Sa m·∫°c Pinnacles mang ƒë·∫øn c·∫£nh quan ƒë·ªôc ƒë√°o v·ªõi nh·ªØng c·ªôt ƒë√° v√¥i.\"),\n",
    "            \n",
    "            # Adelaide and South Australia\n",
    "            (\"Adelaide is surrounded by world-class wine regions.\", \n",
    "             \"Adelaide ƒë∆∞·ª£c bao quanh b·ªüi c√°c v√πng r∆∞·ª£u vang ƒë·∫≥ng c·∫•p th·∫ø gi·ªõi.\"),\n",
    "            (\"Kangaroo Island is famous for its wildlife and natural beauty.\", \n",
    "             \"ƒê·∫£o Kangaroo n·ªïi ti·∫øng v·ªõi ƒë·ªông v·∫≠t hoang d√£ v√† v·∫ª ƒë·∫πp t·ª± nhi√™n.\"),\n",
    "            (\"The Barossa Valley produces some of Australia's finest wines.\", \n",
    "             \"Thung l≈©ng Barossa s·∫£n xu·∫•t m·ªôt s·ªë lo·∫°i r∆∞·ª£u vang t·ªët nh·∫•t Australia.\"),\n",
    "            \n",
    "            # Tasmania (Hobart)\n",
    "            (\"Tasmania offers pristine wilderness and clean air.\", \n",
    "             \"Tasmania mang ƒë·∫øn thi√™n nhi√™n hoang s∆° v√† kh√¥ng kh√≠ trong l√†nh.\"),\n",
    "            (\"Cradle Mountain-Lake St Clair is perfect for hiking and nature photography.\", \n",
    "             \"N√∫i Cradle-H·ªì St Clair ho√†n h·∫£o cho ƒëi b·ªô ƒë∆∞·ªùng d√†i v√† ch·ª•p ·∫£nh thi√™n nhi√™n.\"),\n",
    "            (\"MONA in Hobart is one of the world's most provocative art museums.\", \n",
    "             \"MONA ·ªü Hobart l√† m·ªôt trong nh·ªØng b·∫£o t√†ng ngh·ªá thu·∫≠t khi√™u kh√≠ch nh·∫•t th·∫ø gi·ªõi.\"),\n",
    "            \n",
    "            # Darwin and Northern Territory\n",
    "            (\"Darwin is the gateway to Kakadu National Park.\", \n",
    "             \"Darwin l√† c·ª≠a ng√µ ƒë·∫øn C√¥ng vi√™n Qu·ªëc gia Kakadu.\"),\n",
    "            (\"Uluru is sacred to Aboriginal people and a UNESCO World Heritage site.\", \n",
    "             \"Uluru l√† thi√™ng li√™ng ƒë·ªëi v·ªõi ng∆∞·ªùi th·ªï d√¢n v√† l√† di s·∫£n th·∫ø gi·ªõi UNESCO.\"),\n",
    "            (\"Katherine Gorge offers spectacular boat cruises through ancient landscapes.\", \n",
    "             \"H·∫ªm n√∫i Katherine cung c·∫•p nh·ªØng chuy·∫øn du thuy·ªÅn ngo·∫°n m·ª•c qua c·∫£nh quan c·ªï x∆∞a.\"),\n",
    "            \n",
    "            # Canberra (Capital)\n",
    "            (\"Canberra is Australia's capital and home to important national institutions.\", \n",
    "             \"Canberra l√† th·ªß ƒë√¥ Australia v√† l√† n∆°i c√≥ c√°c t·ªï ch·ª©c qu·ªëc gia quan tr·ªçng.\"),\n",
    "            (\"The Australian War Memorial honors the nation's military history.\", \n",
    "             \"ƒê√†i t∆∞·ªüng ni·ªám Chi·∫øn tranh Australia t√¥n vinh l·ªãch s·ª≠ qu√¢n s·ª± c·ªßa ƒë·∫•t n∆∞·ªõc.\"),\n",
    "            \n",
    "            # General travel advice\n",
    "            (\"Australia uses the Australian dollar as its currency.\", \n",
    "             \"Australia s·ª≠ d·ª•ng ƒë√¥ la Australia l√†m ti·ªÅn t·ªá.\"),\n",
    "            (\"The best time to visit Australia is during the spring and autumn months.\", \n",
    "             \"Th·ªùi gian t·ªët nh·∫•t ƒë·ªÉ thƒÉm Australia l√† v√†o c√°c th√°ng m√πa xu√¢n v√† m√πa thu.\"),\n",
    "            (\"Tipping is not mandatory in Australia but is appreciated for good service.\", \n",
    "             \"Ti·ªÅn boa kh√¥ng b·∫Øt bu·ªôc ·ªü Australia nh∆∞ng ƒë∆∞·ª£c ƒë√°nh gi√° cao cho d·ªãch v·ª• t·ªët.\"),\n",
    "            (\"Public transport cards make it easy to travel around Australian cities.\", \n",
    "             \"Th·∫ª giao th√¥ng c√¥ng c·ªông gi√∫p di chuy·ªÉn d·ªÖ d√†ng quanh c√°c th√†nh ph·ªë Australia.\"),\n",
    "            \n",
    "            # Food and culture\n",
    "            (\"Try the famous Australian meat pies and sausage rolls.\", \n",
    "             \"H√£y th·ª≠ nh·ªØng chi·∫øc b√°nh th·ªãt v√† b√°nh cu·ªën x√∫c x√≠ch n·ªïi ti·∫øng c·ªßa Australia.\"),\n",
    "            (\"Vegemite is a unique Australian spread that locals love.\", \n",
    "             \"Vegemite l√† m·ªôt lo·∫°i m·ª©t ƒë·ªôc ƒë√°o c·ªßa Australia m√† ng∆∞·ªùi ƒë·ªãa ph∆∞∆°ng y√™u th√≠ch.\"),\n",
    "            (\"Fish and chips by the beach is a classic Australian experience.\", \n",
    "             \"C√° v√† khoai t√¢y chi√™n b√™n b√£i bi·ªÉn l√† tr·∫£i nghi·ªám Australia kinh ƒëi·ªÉn.\"),\n",
    "            \n",
    "            # Wildlife\n",
    "            (\"Kangaroos and koalas are Australia's most famous native animals.\", \n",
    "             \"Kangaroo v√† koala l√† nh·ªØng ƒë·ªông v·∫≠t b·∫£n ƒë·ªãa n·ªïi ti·∫øng nh·∫•t c·ªßa Australia.\"),\n",
    "            (\"The Tasmanian devil is found only in Tasmania.\", \n",
    "             \"Qu·ª∑ Tasmania ch·ªâ c√≥ th·ªÉ t√¨m th·∫•y ·ªü Tasmania.\"),\n",
    "            (\"Wombats are sturdy marsupials that dig extensive burrow systems.\", \n",
    "             \"Wombat l√† lo√†i th√∫ c√≥ t√∫i ch·∫Øc ch·∫Øn ƒë√†o h·ªá th·ªëng hang r·ªông l·ªõn.\")\n",
    "        ]\n",
    "        \n",
    "        print(f\"üìä Australian Tourism Translation Dataset Loaded\")\n",
    "        print(f\"   Total translation pairs: {len(self.translation_pairs)}\")\n",
    "        print(f\"   Languages: English ‚Üí Vietnamese\")\n",
    "        print(f\"   Context: Australian tourism and culture\")\n",
    "    \n",
    "    def get_pairs(self):\n",
    "        \"\"\"Return all translation pairs.\"\"\"\n",
    "        return self.translation_pairs\n",
    "    \n",
    "    def get_sample(self, n=5):\n",
    "        \"\"\"Get a random sample of translation pairs.\"\"\"\n",
    "        return random.sample(self.translation_pairs, min(n, len(self.translation_pairs)))\n",
    "    \n",
    "    def get_source_sentences(self):\n",
    "        \"\"\"Get all English source sentences.\"\"\"\n",
    "        return [pair[0] for pair in self.translation_pairs]\n",
    "    \n",
    "    def get_target_sentences(self):\n",
    "        \"\"\"Get all Vietnamese target sentences.\"\"\"\n",
    "        return [pair[1] for pair in self.translation_pairs]\n",
    "\n",
    "# Create the dataset\n",
    "dataset = AustralianTourismTranslationDataset()\n",
    "\n",
    "# Display some examples\n",
    "print(\"\\nüåè Sample Translation Pairs:\")\n",
    "print(\"=\" * 70)\n",
    "for i, (eng, vie) in enumerate(dataset.get_sample(5), 1):\n",
    "    print(f\"{i}. üá¨üáß English: {eng}\")\n",
    "    print(f\"   üáªüá≥ Vietnamese: {vie}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî§ Text Preprocessing and Vocabulary Building\n",
    "\n",
    "For sequence-to-sequence translation, we need to:\n",
    "\n",
    "1. **Normalize text** - Handle Unicode characters, punctuation, and case\n",
    "2. **Tokenize sentences** - Split into words/subwords\n",
    "3. **Build vocabularies** - Create word-to-index mappings for both languages\n",
    "4. **Add special tokens** - `<SOS>`, `<EOS>`, `<PAD>`, `<UNK>`\n",
    "5. **Convert to tensors** - Transform text into numerical sequences\n",
    "\n",
    "This preprocessing pipeline handles both English and Vietnamese text with proper Unicode support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Text preprocessing pipeline for English-Vietnamese translation.\n",
    "    \n",
    "    Handles Unicode normalization, tokenization, and vocabulary building\n",
    "    for both languages with special token support.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_vocab_size=10000, min_freq=1):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.min_freq = min_freq\n",
    "        \n",
    "        # Special tokens\n",
    "        self.SOS_TOKEN = 0  # Start of sequence\n",
    "        self.EOS_TOKEN = 1  # End of sequence\n",
    "        self.PAD_TOKEN = 2  # Padding\n",
    "        self.UNK_TOKEN = 3  # Unknown word\n",
    "        \n",
    "        self.special_tokens = {\n",
    "            '<SOS>': self.SOS_TOKEN,\n",
    "            '<EOS>': self.EOS_TOKEN,\n",
    "            '<PAD>': self.PAD_TOKEN,\n",
    "            '<UNK>': self.UNK_TOKEN\n",
    "        }\n",
    "        \n",
    "        # Vocabularies will be built from data\n",
    "        self.src_vocab = {}  # English\n",
    "        self.tgt_vocab = {}  # Vietnamese\n",
    "        self.src_vocab_inv = {}  # Index to word\n",
    "        self.tgt_vocab_inv = {}  # Index to word\n",
    "    \n",
    "    def normalize_string(self, s):\n",
    "        \"\"\"\n",
    "        Normalize text for translation preprocessing.\n",
    "        \n",
    "        Steps:\n",
    "        1. Unicode normalization\n",
    "        2. Lowercase conversion\n",
    "        3. Remove extra whitespace\n",
    "        4. Handle punctuation spacing\n",
    "        \"\"\"\n",
    "        # Unicode normalization\n",
    "        s = unicodedata.normalize('NFD', s)\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        s = s.lower().strip()\n",
    "        \n",
    "        # Add spaces around punctuation\n",
    "        s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "        s = re.sub(r\"[^a-zA-Z.!?√Ä√Å√Ç√É√à√â√ä√å√ç√í√ì√î√ï√ô√ö√ù√†√°√¢√£√®√©√™√¨√≠√≤√≥√¥√µ√π√∫√ΩƒÇƒÉƒêƒëƒ®ƒ©≈®≈©∆†∆°∆Ø∆∞·∫†-·ªπ]+\", r\" \", s)\n",
    "        \n",
    "        # Remove multiple spaces\n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Simple tokenization by splitting on whitespace.\n",
    "        \n",
    "        For production use, consider:\n",
    "        - Subword tokenization (BPE, SentencePiece)\n",
    "        - Language-specific tokenizers\n",
    "        - Handling of Vietnamese word segmentation\n",
    "        \"\"\"\n",
    "        return self.normalize_string(text).split()\n",
    "    \n",
    "    def build_vocabulary(self, sentences, is_source=True):\n",
    "        \"\"\"\n",
    "        Build vocabulary from list of sentences.\n",
    "        \n",
    "        Args:\n",
    "            sentences: List of sentences to process\n",
    "            is_source: True for source language (English), False for target (Vietnamese)\n",
    "        \"\"\"\n",
    "        print(f\"üìù Building {'source (English)' if is_source else 'target (Vietnamese)'} vocabulary...\")\n",
    "        \n",
    "        # Count word frequencies\n",
    "        word_counts = Counter()\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenize(sentence)\n",
    "            word_counts.update(tokens)\n",
    "        \n",
    "        # Filter by frequency and limit vocabulary size\n",
    "        filtered_words = [word for word, count in word_counts.items() if count >= self.min_freq]\n",
    "        most_common = sorted(filtered_words, key=lambda w: word_counts[w], reverse=True)[:self.max_vocab_size-4]\n",
    "        \n",
    "        # Create vocabulary dictionaries\n",
    "        vocab = dict(self.special_tokens)  # Start with special tokens\n",
    "        vocab_inv = {idx: token for token, idx in self.special_tokens.items()}\n",
    "        \n",
    "        for i, word in enumerate(most_common, start=4):  # Start after special tokens\n",
    "            vocab[word] = i\n",
    "            vocab_inv[i] = word\n",
    "        \n",
    "        if is_source:\n",
    "            self.src_vocab = vocab\n",
    "            self.src_vocab_inv = vocab_inv\n",
    "        else:\n",
    "            self.tgt_vocab = vocab\n",
    "            self.tgt_vocab_inv = vocab_inv\n",
    "        \n",
    "        vocab_name = \"source\" if is_source else \"target\"\n",
    "        print(f\"   {vocab_name} vocabulary size: {len(vocab)}\")\n",
    "        print(f\"   Most common words: {most_common[:10]}\")\n",
    "        \n",
    "        return vocab\n",
    "    \n",
    "    def encode_sentence(self, sentence, is_source=True, add_eos=True):\n",
    "        \"\"\"\n",
    "        Convert sentence to sequence of token indices.\n",
    "        \n",
    "        Args:\n",
    "            sentence: Input sentence string\n",
    "            is_source: True for source language, False for target\n",
    "            add_eos: Whether to add end-of-sequence token\n",
    "        \"\"\"\n",
    "        vocab = self.src_vocab if is_source else self.tgt_vocab\n",
    "        tokens = self.tokenize(sentence)\n",
    "        \n",
    "        # Convert tokens to indices\n",
    "        indices = [vocab.get(token, self.UNK_TOKEN) for token in tokens]\n",
    "        \n",
    "        # Add EOS token for target sequences\n",
    "        if add_eos:\n",
    "            indices.append(self.EOS_TOKEN)\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    def decode_sequence(self, indices, is_source=True):\n",
    "        \"\"\"\n",
    "        Convert sequence of indices back to sentence.\n",
    "        \"\"\"\n",
    "        vocab_inv = self.src_vocab_inv if is_source else self.tgt_vocab_inv\n",
    "        \n",
    "        # Convert indices to tokens, stop at EOS\n",
    "        tokens = []\n",
    "        for idx in indices:\n",
    "            if idx == self.EOS_TOKEN:\n",
    "                break\n",
    "            elif idx == self.PAD_TOKEN:\n",
    "                continue\n",
    "            else:\n",
    "                tokens.append(vocab_inv.get(idx, '<UNK>'))\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "\n",
    "# Initialize preprocessor and build vocabularies\n",
    "preprocessor = TextPreprocessor(max_vocab_size=8000, min_freq=1)\n",
    "\n",
    "# Get all sentences\n",
    "source_sentences = dataset.get_source_sentences()\n",
    "target_sentences = dataset.get_target_sentences()\n",
    "\n",
    "# Build vocabularies\n",
    "preprocessor.build_vocabulary(source_sentences, is_source=True)\n",
    "preprocessor.build_vocabulary(target_sentences, is_source=False)\n",
    "\n",
    "print(f\"\\nüìä Vocabulary Statistics:\")\n",
    "print(f\"   English vocabulary size: {len(preprocessor.src_vocab)}\")\n",
    "print(f\"   Vietnamese vocabulary size: {len(preprocessor.tgt_vocab)}\")\n",
    "\n",
    "# Test encoding/decoding\n",
    "test_sentence = \"The Sydney Opera House is beautiful.\"\n",
    "encoded = preprocessor.encode_sentence(test_sentence, is_source=True)\n",
    "decoded = preprocessor.decode_sequence(encoded, is_source=True)\n",
    "\n",
    "print(f\"\\nüîÑ Encoding/Decoding Test:\")\n",
    "print(f\"   Original: {test_sentence}\")\n",
    "print(f\"   Encoded: {encoded}\")\n",
    "print(f\"   Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Sequence-to-Sequence Dataset and DataLoader\n",
    "\n",
    "We need a PyTorch `Dataset` class that:\n",
    "\n",
    "1. **Handles variable-length sequences** with proper padding\n",
    "2. **Provides batch processing** with source and target sequences\n",
    "3. **Supports different sequence lengths** for encoder and decoder\n",
    "4. **Includes teacher forcing** setup for training\n",
    "\n",
    "The dataset will return source sequences, target input sequences (with SOS), and target output sequences (with EOS) for proper seq2seq training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for sequence-to-sequence translation.\n",
    "    \n",
    "    Returns:\n",
    "    - source_seq: Encoded source sentence (English)\n",
    "    - target_input_seq: Target sentence with SOS token (for decoder input)\n",
    "    - target_output_seq: Target sentence with EOS token (for loss calculation)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, translation_pairs, preprocessor, max_src_len=50, max_tgt_len=50):\n",
    "        self.pairs = translation_pairs\n",
    "        self.preprocessor = preprocessor\n",
    "        self.max_src_len = max_src_len\n",
    "        self.max_tgt_len = max_tgt_len\n",
    "        \n",
    "        # Pre-process all pairs\n",
    "        self.processed_pairs = []\n",
    "        for src_text, tgt_text in translation_pairs:\n",
    "            # Encode source sentence\n",
    "            src_indices = preprocessor.encode_sentence(src_text, is_source=True, add_eos=False)\n",
    "            \n",
    "            # Encode target sentence\n",
    "            tgt_indices = preprocessor.encode_sentence(tgt_text, is_source=False, add_eos=False)\n",
    "            \n",
    "            # Skip sequences that are too long\n",
    "            if len(src_indices) <= max_src_len - 1 and len(tgt_indices) <= max_tgt_len - 2:\n",
    "                self.processed_pairs.append((src_indices, tgt_indices))\n",
    "        \n",
    "        print(f\"üì¶ Seq2Seq Dataset Created:\")\n",
    "        print(f\"   Original pairs: {len(translation_pairs)}\")\n",
    "        print(f\"   Filtered pairs: {len(self.processed_pairs)}\")\n",
    "        print(f\"   Max source length: {max_src_len}\")\n",
    "        print(f\"   Max target length: {max_tgt_len}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.processed_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_indices, tgt_indices = self.processed_pairs[idx]\n",
    "        \n",
    "        # Pad source sequence and add EOS\n",
    "        src_seq = src_indices + [self.preprocessor.EOS_TOKEN]\n",
    "        src_seq += [self.preprocessor.PAD_TOKEN] * (self.max_src_len - len(src_seq))\n",
    "        \n",
    "        # Target input: SOS + target sequence (for decoder input)\n",
    "        tgt_input_seq = [self.preprocessor.SOS_TOKEN] + tgt_indices\n",
    "        tgt_input_seq += [self.preprocessor.PAD_TOKEN] * (self.max_tgt_len - len(tgt_input_seq))\n",
    "        \n",
    "        # Target output: target sequence + EOS (for loss calculation)\n",
    "        tgt_output_seq = tgt_indices + [self.preprocessor.EOS_TOKEN]\n",
    "        tgt_output_seq += [self.preprocessor.PAD_TOKEN] * (self.max_tgt_len - len(tgt_output_seq))\n",
    "        \n",
    "        return {\n",
    "            'source': torch.tensor(src_seq[:self.max_src_len], dtype=torch.long),\n",
    "            'target_input': torch.tensor(tgt_input_seq[:self.max_tgt_len], dtype=torch.long),\n",
    "            'target_output': torch.tensor(tgt_output_seq[:self.max_tgt_len], dtype=torch.long),\n",
    "            'src_len': len(src_indices) + 1,  # +1 for EOS\n",
    "            'tgt_len': len(tgt_indices) + 1   # +1 for EOS\n",
    "        }\n",
    "\n",
    "def create_data_loaders(translation_pairs, preprocessor, batch_size=16, train_split=0.8):\n",
    "    \"\"\"\n",
    "    Create train and validation DataLoaders for seq2seq training.\n",
    "    \"\"\"\n",
    "    # Split data\n",
    "    train_pairs, val_pairs = train_test_split(\n",
    "        translation_pairs, \n",
    "        train_size=train_split, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = Seq2SeqDataset(train_pairs, preprocessor, max_src_len=50, max_tgt_len=50)\n",
    "    val_dataset = Seq2SeqDataset(val_pairs, preprocessor, max_src_len=50, max_tgt_len=50)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, train_dataset, val_dataset\n",
    "\n",
    "# Create data loaders\n",
    "translation_pairs = dataset.get_pairs()\n",
    "train_loader, val_loader, train_dataset, val_dataset = create_data_loaders(\n",
    "    translation_pairs, \n",
    "    preprocessor, \n",
    "    batch_size=8  # Smaller batch size for CPU training\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Data Loader Statistics:\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")\n",
    "print(f\"   Batch size: {train_loader.batch_size}\")\n",
    "\n",
    "# Test data loader\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nüîç Sample Batch Structure:\")\n",
    "print(f\"   Source shape: {sample_batch['source'].shape}\")\n",
    "print(f\"   Target input shape: {sample_batch['target_input'].shape}\")\n",
    "print(f\"   Target output shape: {sample_batch['target_output'].shape}\")\n",
    "\n",
    "# Show example\n",
    "idx = 0\n",
    "src_text = preprocessor.decode_sequence(sample_batch['source'][idx], is_source=True)\n",
    "tgt_text = preprocessor.decode_sequence(sample_batch['target_output'][idx], is_source=False)\n",
    "print(f\"\\nüìù Example from batch:\")\n",
    "print(f\"   üá¨üáß Source: {src_text}\")\n",
    "print(f\"   üáªüá≥ Target: {tgt_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Sequence-to-Sequence Model Architecture\n",
    "\n",
    "We'll implement a complete seq2seq model with attention mechanism:\n",
    "\n",
    "### 1. **Encoder**\n",
    "- **Embedding layer** for source language tokens\n",
    "- **Bidirectional LSTM** to process source sequences\n",
    "- **Hidden state combination** from forward and backward passes\n",
    "\n",
    "### 2. **Attention Mechanism**\n",
    "- **Additive attention** (Bahdanau attention)\n",
    "- **Context vector computation** from encoder hidden states\n",
    "- **Attention weight visualization** capability\n",
    "\n",
    "### 3. **Decoder**\n",
    "- **Embedding layer** for target language tokens\n",
    "- **LSTM with attention context** for sequence generation\n",
    "- **Output projection** to target vocabulary\n",
    "\n",
    "This architecture is inspired by the original attention mechanism papers and optimized for our English-Vietnamese translation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder for sequence-to-sequence model.\n",
    "    \n",
    "    Uses bidirectional LSTM to encode source sequences.\n",
    "    Returns all hidden states for attention mechanism.\n",
    "    \n",
    "    TensorFlow equivalent:\n",
    "        encoder = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(hidden_size, return_sequences=True, return_state=True)\n",
    "        )\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=2)  # PAD_TOKEN = 2\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim, \n",
    "            hidden_dim, \n",
    "            num_layers, \n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Linear layer to combine bidirectional hidden states\n",
    "        self.hidden_projection = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.cell_projection = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "    \n",
    "    def forward(self, src_seq, src_lengths=None):\n",
    "        \"\"\"\n",
    "        Forward pass through encoder.\n",
    "        \n",
    "        Args:\n",
    "            src_seq: Source sequence tensor (batch_size, max_src_len)\n",
    "            src_lengths: Actual lengths of sequences (for packing)\n",
    "        \n",
    "        Returns:\n",
    "            encoder_outputs: All hidden states (batch_size, max_src_len, hidden_dim * 2)\n",
    "            final_hidden: Final hidden state (batch_size, hidden_dim)\n",
    "            final_cell: Final cell state (batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        batch_size = src_seq.size(0)\n",
    "        \n",
    "        # Embedding\n",
    "        embedded = self.embedding(src_seq)  # (batch_size, max_src_len, embed_dim)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # Pack sequences if lengths are provided (for efficiency)\n",
    "        if src_lengths is not None:\n",
    "            packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "                embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "            packed_outputs, (hidden, cell) = self.lstm(packed_embedded)\n",
    "            encoder_outputs, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_outputs, batch_first=True\n",
    "            )\n",
    "        else:\n",
    "            encoder_outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # encoder_outputs: (batch_size, max_src_len, hidden_dim * 2)\n",
    "        # hidden: (num_layers * 2, batch_size, hidden_dim)\n",
    "        # cell: (num_layers * 2, batch_size, hidden_dim)\n",
    "        \n",
    "        # Combine bidirectional hidden states\n",
    "        # Take the last layer's forward and backward hidden states\n",
    "        forward_hidden = hidden[-2]  # Forward direction\n",
    "        backward_hidden = hidden[-1]  # Backward direction\n",
    "        final_hidden = torch.cat([forward_hidden, backward_hidden], dim=1)\n",
    "        final_hidden = self.hidden_projection(final_hidden)  # (batch_size, hidden_dim)\n",
    "        \n",
    "        # Same for cell states\n",
    "        forward_cell = cell[-2]\n",
    "        backward_cell = cell[-1]\n",
    "        final_cell = torch.cat([forward_cell, backward_cell], dim=1)\n",
    "        final_cell = self.cell_projection(final_cell)  # (batch_size, hidden_dim)\n",
    "        \n",
    "        return encoder_outputs, final_hidden, final_cell\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Additive attention mechanism (Bahdanau attention).\n",
    "    \n",
    "    Computes attention weights between decoder hidden state and encoder outputs.\n",
    "    Returns context vector as weighted sum of encoder hidden states.\n",
    "    \n",
    "    TensorFlow equivalent would use tf.keras.layers.Attention or custom implementation\n",
    "    with tf.keras.layers.Dense layers for the same computations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, encoder_hidden_dim, decoder_hidden_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.encoder_hidden_dim = encoder_hidden_dim  # 2 * hidden_dim (bidirectional)\n",
    "        self.decoder_hidden_dim = decoder_hidden_dim  # hidden_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        \n",
    "        # Linear layers for attention computation\n",
    "        self.encoder_projection = nn.Linear(encoder_hidden_dim, attention_dim)\n",
    "        self.decoder_projection = nn.Linear(decoder_hidden_dim, attention_dim)\n",
    "        self.attention_vector = nn.Linear(attention_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, decoder_hidden, encoder_outputs, mask=None):\n",
    "        \"\"\"\n",
    "        Compute attention weights and context vector.\n",
    "        \n",
    "        Args:\n",
    "            decoder_hidden: Current decoder hidden state (batch_size, decoder_hidden_dim)\n",
    "            encoder_outputs: All encoder hidden states (batch_size, src_len, encoder_hidden_dim)\n",
    "            mask: Padding mask (batch_size, src_len)\n",
    "        \n",
    "        Returns:\n",
    "            context: Context vector (batch_size, encoder_hidden_dim)\n",
    "            attention_weights: Attention weights (batch_size, src_len)\n",
    "        \"\"\"\n",
    "        batch_size, src_len, _ = encoder_outputs.size()\n",
    "        \n",
    "        # Project encoder outputs\n",
    "        encoder_proj = self.encoder_projection(encoder_outputs)  # (batch_size, src_len, attention_dim)\n",
    "        \n",
    "        # Project decoder hidden state and expand\n",
    "        decoder_proj = self.decoder_projection(decoder_hidden).unsqueeze(1)  # (batch_size, 1, attention_dim)\n",
    "        decoder_proj = decoder_proj.expand(batch_size, src_len, -1)  # (batch_size, src_len, attention_dim)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attention_scores = self.attention_vector(torch.tanh(encoder_proj + decoder_proj))  # (batch_size, src_len, 1)\n",
    "        attention_scores = attention_scores.squeeze(2)  # (batch_size, src_len)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask == 0, -float('inf'))\n",
    "        \n",
    "        # Compute attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)  # (batch_size, src_len)\n",
    "        \n",
    "        # Compute context vector\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)  # (batch_size, 1, encoder_hidden_dim)\n",
    "        context = context.squeeze(1)  # (batch_size, encoder_hidden_dim)\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder with attention mechanism for sequence-to-sequence model.\n",
    "    \n",
    "    Uses attention to focus on relevant parts of the source sequence\n",
    "    during target sequence generation.\n",
    "    \n",
    "    TensorFlow equivalent would use tf.keras.layers.LSTM with\n",
    "    custom attention mechanism integration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, encoder_hidden_dim, \n",
    "                 attention_dim, num_layers=1, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=2)  # PAD_TOKEN = 2\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = Attention(encoder_hidden_dim, hidden_dim, attention_dim)\n",
    "        \n",
    "        # LSTM (input: embedding + context)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim + encoder_hidden_dim,  # embedding + context vector\n",
    "            hidden_dim,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(\n",
    "            hidden_dim + encoder_hidden_dim,  # hidden + context\n",
    "            vocab_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, target_token, hidden, cell, encoder_outputs, mask=None):\n",
    "        \"\"\"\n",
    "        Single step forward pass of decoder.\n",
    "        \n",
    "        Args:\n",
    "            target_token: Current target token (batch_size, 1)\n",
    "            hidden: Previous decoder hidden state (batch_size, hidden_dim)\n",
    "            cell: Previous decoder cell state (batch_size, hidden_dim)\n",
    "            encoder_outputs: All encoder hidden states (batch_size, src_len, encoder_hidden_dim)\n",
    "            mask: Source sequence mask (batch_size, src_len)\n",
    "        \n",
    "        Returns:\n",
    "            output: Output logits (batch_size, vocab_size)\n",
    "            hidden: New hidden state (batch_size, hidden_dim)\n",
    "            cell: New cell state (batch_size, hidden_dim)\n",
    "            attention_weights: Attention weights (batch_size, src_len)\n",
    "        \"\"\"\n",
    "        # Embedding\n",
    "        embedded = self.embedding(target_token)  # (batch_size, 1, embed_dim)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # Compute attention\n",
    "        context, attention_weights = self.attention(hidden, encoder_outputs, mask)\n",
    "        \n",
    "        # Concatenate embedding and context\n",
    "        lstm_input = torch.cat([embedded, context.unsqueeze(1)], dim=2)  # (batch_size, 1, embed_dim + encoder_hidden_dim)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        lstm_output, (new_hidden, new_cell) = self.lstm(lstm_input, (hidden.unsqueeze(0), cell.unsqueeze(0)))\n",
    "        \n",
    "        # Remove sequence dimension and layer dimension\n",
    "        lstm_output = lstm_output.squeeze(1)  # (batch_size, hidden_dim)\n",
    "        new_hidden = new_hidden.squeeze(0)  # (batch_size, hidden_dim)\n",
    "        new_cell = new_cell.squeeze(0)  # (batch_size, hidden_dim)\n",
    "        \n",
    "        # Concatenate LSTM output and context for final projection\n",
    "        output_input = torch.cat([lstm_output, context], dim=1)  # (batch_size, hidden_dim + encoder_hidden_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.output_projection(output_input)  # (batch_size, vocab_size)\n",
    "        \n",
    "        return output, new_hidden, new_cell, attention_weights\n",
    "\n",
    "\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete sequence-to-sequence model with attention.\n",
    "    \n",
    "    Combines encoder, decoder, and attention mechanism for neural machine translation.\n",
    "    Supports both training (teacher forcing) and inference modes.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Encoder processes source sequence\n",
    "    2. Decoder generates target sequence step-by-step with attention\n",
    "    3. Attention mechanism allows decoder to focus on relevant source positions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim=256, hidden_dim=512, \n",
    "                 attention_dim=256, num_layers=1, dropout=0.1):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        \n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            vocab_size=src_vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            vocab_size=tgt_vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            encoder_hidden_dim=hidden_dim * 2,  # Bidirectional encoder\n",
    "            attention_dim=attention_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "    \n",
    "    def create_mask(self, src_seq, src_lengths):\n",
    "        \"\"\"\n",
    "        Create mask for padding tokens in source sequence.\n",
    "        \"\"\"\n",
    "        batch_size, max_len = src_seq.size()\n",
    "        mask = torch.zeros(batch_size, max_len, dtype=torch.bool, device=src_seq.device)\n",
    "        \n",
    "        for i, length in enumerate(src_lengths):\n",
    "            mask[i, :length] = 1\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def forward(self, src_seq, tgt_seq, src_lengths=None, teacher_forcing_ratio=1.0):\n",
    "        \"\"\"\n",
    "        Forward pass for training with teacher forcing.\n",
    "        \n",
    "        Args:\n",
    "            src_seq: Source sequences (batch_size, src_len)\n",
    "            tgt_seq: Target sequences (batch_size, tgt_len)\n",
    "            src_lengths: Source sequence lengths\n",
    "            teacher_forcing_ratio: Probability of using teacher forcing\n",
    "        \n",
    "        Returns:\n",
    "            outputs: Decoder outputs (batch_size, tgt_len, vocab_size)\n",
    "            attention_weights: Attention weights (batch_size, tgt_len, src_len)\n",
    "        \"\"\"\n",
    "        batch_size = src_seq.size(0)\n",
    "        tgt_len = tgt_seq.size(1)\n",
    "        \n",
    "        # Encode source sequence\n",
    "        encoder_outputs, hidden, cell = self.encoder(src_seq, src_lengths)\n",
    "        \n",
    "        # Create source mask\n",
    "        mask = self.create_mask(src_seq, src_lengths) if src_lengths is not None else None\n",
    "        \n",
    "        # Initialize outputs and attention weights\n",
    "        outputs = torch.zeros(batch_size, tgt_len, self.tgt_vocab_size, device=src_seq.device)\n",
    "        attention_weights = torch.zeros(batch_size, tgt_len, encoder_outputs.size(1), device=src_seq.device)\n",
    "        \n",
    "        # First decoder input is SOS token\n",
    "        decoder_input = tgt_seq[:, 0:1]  # (batch_size, 1)\n",
    "        \n",
    "        # Decode step by step\n",
    "        for t in range(tgt_len):\n",
    "            # Decoder forward pass\n",
    "            output, hidden, cell, attn_weights = self.decoder(\n",
    "                decoder_input, hidden, cell, encoder_outputs, mask\n",
    "            )\n",
    "            \n",
    "            # Store output and attention weights\n",
    "            outputs[:, t, :] = output\n",
    "            attention_weights[:, t, :] = attn_weights\n",
    "            \n",
    "            # Decide next input (teacher forcing or previous prediction)\n",
    "            if t < tgt_len - 1:\n",
    "                if random.random() < teacher_forcing_ratio:\n",
    "                    # Use teacher forcing: next input is next target token\n",
    "                    decoder_input = tgt_seq[:, t+1:t+2]\n",
    "                else:\n",
    "                    # Use previous prediction\n",
    "                    decoder_input = output.argmax(dim=1).unsqueeze(1)\n",
    "        \n",
    "        return outputs, attention_weights\n",
    "    \n",
    "    def translate(self, src_seq, src_lengths=None, max_length=50, eos_token=1):\n",
    "        \"\"\"\n",
    "        Inference mode: translate source sequence to target sequence.\n",
    "        \n",
    "        Args:\n",
    "            src_seq: Source sequence (1, src_len) - single sequence\n",
    "            src_lengths: Source sequence length\n",
    "            max_length: Maximum target sequence length\n",
    "            eos_token: End-of-sequence token ID\n",
    "        \n",
    "        Returns:\n",
    "            translation: Generated target sequence\n",
    "            attention_weights: Attention weights for visualization\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_size = src_seq.size(0)\n",
    "            \n",
    "            # Encode source sequence\n",
    "            encoder_outputs, hidden, cell = self.encoder(src_seq, src_lengths)\n",
    "            \n",
    "            # Create source mask\n",
    "            mask = self.create_mask(src_seq, src_lengths) if src_lengths is not None else None\n",
    "            \n",
    "            # Initialize with SOS token\n",
    "            decoder_input = torch.tensor([[0]], device=src_seq.device)  # SOS_TOKEN = 0\n",
    "            \n",
    "            # Store results\n",
    "            translation = []\n",
    "            attention_weights = []\n",
    "            \n",
    "            # Generate tokens one by one\n",
    "            for _ in range(max_length):\n",
    "                output, hidden, cell, attn_weights = self.decoder(\n",
    "                    decoder_input, hidden, cell, encoder_outputs, mask\n",
    "                )\n",
    "                \n",
    "                # Get predicted token\n",
    "                predicted_token = output.argmax(dim=1)\n",
    "                translation.append(predicted_token.item())\n",
    "                attention_weights.append(attn_weights.squeeze(0).cpu().numpy())\n",
    "                \n",
    "                # Stop if EOS token is generated\n",
    "                if predicted_token.item() == eos_token:\n",
    "                    break\n",
    "                \n",
    "                # Use predicted token as next input\n",
    "                decoder_input = predicted_token.unsqueeze(1)\n",
    "        \n",
    "        return translation, np.array(attention_weights)\n",
    "\n",
    "# Create model instance\n",
    "model = Seq2SeqModel(\n",
    "    src_vocab_size=len(preprocessor.src_vocab),\n",
    "    tgt_vocab_size=len(preprocessor.tgt_vocab),\n",
    "    embed_dim=256,\n",
    "    hidden_dim=256,  # Smaller for CPU training\n",
    "    attention_dim=128,\n",
    "    num_layers=1,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "total_params = count_parameters(model)\n",
    "print(f\"\\nüèóÔ∏è  Seq2Seq Model Architecture:\")\n",
    "print(f\"   Source vocabulary size: {len(preprocessor.src_vocab)}\")\n",
    "print(f\"   Target vocabulary size: {len(preprocessor.tgt_vocab)}\")\n",
    "print(f\"   Embedding dimension: 256\")\n",
    "print(f\"   Hidden dimension: 256\")\n",
    "print(f\"   Attention dimension: 128\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Device: {DEVICE}\")\n",
    "\n",
    "# Test model with sample data\n",
    "sample_batch = next(iter(train_loader))\n",
    "src_seq = sample_batch['source'].to(DEVICE)\n",
    "tgt_input = sample_batch['target_input'].to(DEVICE)\n",
    "\n",
    "print(f\"\\nüß™ Model Forward Pass Test:\")\n",
    "print(f\"   Input shape: {src_seq.shape}\")\n",
    "print(f\"   Target shape: {tgt_input.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs, attention = model(src_seq, tgt_input, teacher_forcing_ratio=1.0)\n",
    "    print(f\"   Output shape: {outputs.shape}\")\n",
    "    print(f\"   Attention shape: {attention.shape}\")\n",
    "    print(f\"   ‚úÖ Model forward pass successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Training Loop with TensorBoard Integration\n",
    "\n",
    "We'll implement a comprehensive training loop that includes:\n",
    "\n",
    "- **Loss computation** with padding token masking\n",
    "- **Teacher forcing scheduling** with decreasing ratio over epochs\n",
    "- **TensorBoard logging** for loss, attention visualizations, and metrics\n",
    "- **Gradient clipping** to prevent exploding gradients\n",
    "- **Learning rate scheduling** for better convergence\n",
    "- **Model checkpointing** to save best model\n",
    "\n",
    "The training follows repository standards with device-aware implementation and comprehensive monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def get_run_logdir(run_name=\"seq2seq_translation\"):\n",
    "    \"\"\"Generate unique log directory for TensorBoard.\"\"\"\n",
    "    \n",
    "    # Platform-specific TensorBoard log directory setup\n",
    "    if IS_COLAB:\n",
    "        root_logdir = \"/content/tensorboard_logs\"\n",
    "    elif IS_KAGGLE:\n",
    "        root_logdir = \"./tensorboard_logs\"\n",
    "    else:\n",
    "        root_logdir = \"./tensorboard_logs\"\n",
    "    \n",
    "    # Create root directory if it doesn't exist\n",
    "    os.makedirs(root_logdir, exist_ok=True)\n",
    "    \n",
    "    # Generate unique run directory\n",
    "    timestamp = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "    run_logdir = os.path.join(root_logdir, f\"{run_name}_{timestamp}\")\n",
    "    \n",
    "    return run_logdir\n",
    "\n",
    "def masked_cross_entropy_loss(outputs, targets, pad_token=2):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss while ignoring padding tokens.\n",
    "    \n",
    "    Args:\n",
    "        outputs: Model predictions (batch_size, seq_len, vocab_size)\n",
    "        targets: Target sequences (batch_size, seq_len)\n",
    "        pad_token: Padding token ID to ignore\n",
    "    \n",
    "    Returns:\n",
    "        loss: Masked cross-entropy loss\n",
    "        num_tokens: Number of non-padding tokens\n",
    "    \"\"\"\n",
    "    # Flatten predictions and targets\n",
    "    outputs_flat = outputs.view(-1, outputs.size(-1))  # (batch_size * seq_len, vocab_size)\n",
    "    targets_flat = targets.view(-1)  # (batch_size * seq_len)\n",
    "    \n",
    "    # Create mask for non-padding tokens\n",
    "    mask = (targets_flat != pad_token)\n",
    "    \n",
    "    # Compute loss only for non-padding tokens\n",
    "    loss = F.cross_entropy(outputs_flat, targets_flat, reduction='none')\n",
    "    masked_loss = loss * mask.float()\n",
    "    \n",
    "    # Average over non-padding tokens\n",
    "    num_tokens = mask.sum().item()\n",
    "    if num_tokens > 0:\n",
    "        avg_loss = masked_loss.sum() / num_tokens\n",
    "    else:\n",
    "        avg_loss = masked_loss.sum()  # Should be 0\n",
    "    \n",
    "    return avg_loss, num_tokens\n",
    "\n",
    "def train_seq2seq_model(model, train_loader, val_loader, preprocessor, \n",
    "                       num_epochs=20, learning_rate=0.001, \n",
    "                       teacher_forcing_start=1.0, teacher_forcing_end=0.5,\n",
    "                       clip_grad_norm=1.0, save_best=True):\n",
    "    \"\"\"\n",
    "    Train the sequence-to-sequence model with comprehensive monitoring.\n",
    "    \n",
    "    Features:\n",
    "    - Teacher forcing ratio decay\n",
    "    - Gradient clipping\n",
    "    - TensorBoard logging\n",
    "    - Model checkpointing\n",
    "    - Translation examples during training\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup optimizer and scheduler\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "    \n",
    "    # TensorBoard setup\n",
    "    log_dir = get_run_logdir(\"australian_translation\")\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    \n",
    "    # Training state\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    print(f\"üöÄ Starting Australian Tourism Translation Training\")\n",
    "    print(f\"üìä Device: {DEVICE}\")\n",
    "    print(f\"üéØ Target: English ‚Üí Vietnamese translation\")\n",
    "    print(f\"üåè Context: Australian tourism content\")\n",
    "    print(f\"üìà Epochs: {num_epochs}\")\n",
    "    print(f\"üî¢ Learning Rate: {learning_rate}\")\n",
    "    print(f\"üìù TensorBoard logs: {log_dir}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Calculate teacher forcing ratio (decay over epochs)\n",
    "        teacher_forcing_ratio = teacher_forcing_start - (teacher_forcing_start - teacher_forcing_end) * (epoch / num_epochs)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_tokens = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Move data to device\n",
    "            src_seq = batch['source'].to(DEVICE)\n",
    "            tgt_input = batch['target_input'].to(DEVICE)\n",
    "            tgt_output = batch['target_output'].to(DEVICE)\n",
    "            src_lengths = batch['src_len'].to(DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs, attention_weights = model(src_seq, tgt_input, src_lengths, teacher_forcing_ratio)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss, num_tokens = masked_cross_entropy_loss(outputs, tgt_output)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if clip_grad_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            train_loss += loss.item() * num_tokens\n",
    "            train_tokens += num_tokens\n",
    "            \n",
    "            # Log batch metrics\n",
    "            if batch_idx % 10 == 0:\n",
    "                step = epoch * len(train_loader) + batch_idx\n",
    "                writer.add_scalar('Loss/Train_Batch', loss.item(), step)\n",
    "                writer.add_scalar('Meta/Teacher_Forcing_Ratio', teacher_forcing_ratio, step)\n",
    "                writer.add_scalar('Meta/Learning_Rate', optimizer.param_groups[0]['lr'], step)\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = train_loss / train_tokens if train_tokens > 0 else 0\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_tokens = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                src_seq = batch['source'].to(DEVICE)\n",
    "                tgt_input = batch['target_input'].to(DEVICE)\n",
    "                tgt_output = batch['target_output'].to(DEVICE)\n",
    "                src_lengths = batch['src_len'].to(DEVICE)\n",
    "                \n",
    "                outputs, attention_weights = model(src_seq, tgt_input, src_lengths, 1.0)\n",
    "                loss, num_tokens = masked_cross_entropy_loss(outputs, tgt_output)\n",
    "                \n",
    "                val_loss += loss.item() * num_tokens\n",
    "                val_tokens += num_tokens\n",
    "        \n",
    "        avg_val_loss = val_loss / val_tokens if val_tokens > 0 else 0\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Log epoch metrics\n",
    "        writer.add_scalar('Loss/Train_Epoch', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', avg_val_loss, epoch)\n",
    "        writer.add_scalar('Perplexity/Train', np.exp(avg_train_loss), epoch)\n",
    "        writer.add_scalar('Perplexity/Validation', np.exp(avg_val_loss), epoch)\n",
    "        \n",
    "        # Save best model\n",
    "        if save_best and avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss\n",
    "            }, 'best_seq2seq_model.pth')\n",
    "        \n",
    "        # Sample translation for monitoring\n",
    "        if epoch % 5 == 0:\n",
    "            sample_translation_logging(model, val_dataset, preprocessor, writer, epoch)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch+1:2d}/{num_epochs} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "              f\"TF Ratio: {teacher_forcing_ratio:.3f} | \"\n",
    "              f\"Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    print(f\"\\nüéØ Training completed!\")\n",
    "    print(f\"üìä Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"üìà TensorBoard logs saved to: {log_dir}\")\n",
    "    \n",
    "    return train_losses, val_losses, log_dir\n",
    "\n",
    "def sample_translation_logging(model, dataset, preprocessor, writer, epoch):\n",
    "    \"\"\"\n",
    "    Generate sample translations and log to TensorBoard.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a few samples\n",
    "    sample_indices = [0, 5, 10]\n",
    "    translations_text = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in sample_indices:\n",
    "            if idx < len(dataset):\n",
    "                sample = dataset[idx]\n",
    "                src_seq = sample['source'].unsqueeze(0).to(DEVICE)\n",
    "                src_len = torch.tensor([sample['src_len']], device=DEVICE)\n",
    "                \n",
    "                # Generate translation\n",
    "                translation, attention = model.translate(src_seq, src_len, max_length=50)\n",
    "                \n",
    "                # Decode sequences\n",
    "                src_text = preprocessor.decode_sequence(sample['source'], is_source=True)\n",
    "                tgt_text = preprocessor.decode_sequence(sample['target_output'], is_source=False)\n",
    "                pred_text = preprocessor.decode_sequence(translation, is_source=False)\n",
    "                \n",
    "                translation_example = f\"Source: {src_text}\\nTarget: {tgt_text}\\nPrediction: {pred_text}\\n\"\n",
    "                translations_text.append(translation_example)\n",
    "    \n",
    "    # Log translations as text\n",
    "    writer.add_text('Translations/Sample', '\\n'.join(translations_text), epoch)\n",
    "\n",
    "print(\"üèãÔ∏è Training setup complete! Ready to start training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training the model\n",
    "print(\"üé¨ Starting seq2seq model training...\")\n",
    "\n",
    "# Train the model\n",
    "train_losses, val_losses, log_directory = train_seq2seq_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    preprocessor=preprocessor,\n",
    "    num_epochs=15,  # Moderate number for demonstration\n",
    "    learning_rate=0.001,\n",
    "    teacher_forcing_start=1.0,\n",
    "    teacher_forcing_end=0.5,\n",
    "    clip_grad_norm=1.0,\n",
    "    save_best=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä TENSORBOARD VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Log directory: {log_directory}\")\n",
    "print(\"\\nüöÄ To view TensorBoard:\")\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"   In Google Colab:\")\n",
    "    print(\"   1. Run: %load_ext tensorboard\")\n",
    "    print(f\"   2. Run: %tensorboard --logdir {log_directory}\")\n",
    "    print(\"   3. TensorBoard will appear inline in the notebook\")\n",
    "elif IS_KAGGLE:\n",
    "    print(\"   In Kaggle:\")\n",
    "    print(f\"   1. Download logs from: {log_directory}\")\n",
    "    print(\"   2. Run locally: tensorboard --logdir ./tensorboard_logs\")\n",
    "    print(\"   3. Open http://localhost:6006 in browser\")\n",
    "else:\n",
    "    print(\"   Locally:\")\n",
    "    print(f\"   1. Run: tensorboard --logdir {log_directory}\")\n",
    "    print(\"   2. Open http://localhost:6006 in browser\")\n",
    "\n",
    "print(\"\\nüìà Available visualizations:\")\n",
    "print(\"   ‚Ä¢ Scalars: Training and validation loss, perplexity\")\n",
    "print(\"   ‚Ä¢ Text: Sample translations during training\")\n",
    "print(\"   ‚Ä¢ Meta: Teacher forcing ratio, learning rate\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Training Results Visualization\n",
    "\n",
    "Let's visualize the training progress using seaborn (following repository visualization standards) to understand how our model performed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "def plot_training_metrics(train_losses, val_losses):\n",
    "    \"\"\"\n",
    "    Plot training metrics with seaborn styling for Australian tourism model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create DataFrame for seaborn\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    # Combine losses for plotting\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Epoch': list(epochs) * 2,\n",
    "        'Loss': train_losses + val_losses,\n",
    "        'Phase': ['Training'] * len(epochs) + ['Validation'] * len(epochs)\n",
    "    })\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Loss plot\n",
    "    sns.lineplot(data=metrics_df, x='Epoch', y='Loss', hue='Phase', ax=ax1, marker='o')\n",
    "    ax1.set_title('Australian Tourism Translation - Training Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Cross-Entropy Loss')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend(title='Phase')\n",
    "    \n",
    "    # Perplexity plot\n",
    "    perplexity_df = metrics_df.copy()\n",
    "    perplexity_df['Perplexity'] = np.exp(perplexity_df['Loss'])\n",
    "    \n",
    "    sns.lineplot(data=perplexity_df, x='Epoch', y='Perplexity', hue='Phase', ax=ax2, marker='s')\n",
    "    ax2.set_title('Australian Tourism Translation - Perplexity', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Perplexity')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend(title='Phase')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final metrics\n",
    "    final_train_loss = train_losses[-1]\n",
    "    final_val_loss = val_losses[-1]\n",
    "    final_train_perplexity = np.exp(final_train_loss)\n",
    "    final_val_perplexity = np.exp(final_val_loss)\n",
    "    \n",
    "    print(f\"üìä Final Training Metrics:\")\n",
    "    print(f\"   Training Loss: {final_train_loss:.4f} (Perplexity: {final_train_perplexity:.2f})\")\n",
    "    print(f\"   Validation Loss: {final_val_loss:.4f} (Perplexity: {final_val_perplexity:.2f})\")\n",
    "    \n",
    "    # Convergence analysis\n",
    "    improvement = train_losses[0] - train_losses[-1]\n",
    "    print(f\"   Training improvement: {improvement:.4f}\")\n",
    "    \n",
    "    if final_val_loss < final_train_loss + 0.1:\n",
    "        print(\"   ‚úÖ Model shows good generalization (low overfitting)\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Model may be overfitting - consider regularization\")\n",
    "\n",
    "# Plot the training results\n",
    "plot_training_metrics(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Model Evaluation and BLEU Scores\n",
    "\n",
    "Let's evaluate our trained model using standard translation metrics:\n",
    "\n",
    "- **BLEU Score** - Measures n-gram overlap between predictions and references\n",
    "- **Translation Examples** - Qualitative analysis of translation quality\n",
    "- **Error Analysis** - Common translation patterns and mistakes\n",
    "\n",
    "We'll test on both our validation set and some new Australian tourism examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu_score(predictions, references, max_n=4):\n",
    "    \"\"\"\n",
    "    Compute BLEU score for translation evaluation.\n",
    "    \n",
    "    Simple implementation if NLTK is not available.\n",
    "    For production, use NLTK or sacrebleu for more accurate scoring.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use NLTK if available\n",
    "        if 'nltk' in globals():\n",
    "            # Convert to required format for NLTK\n",
    "            references_nltk = [[ref.split()] for ref in references]\n",
    "            predictions_nltk = [pred.split() for pred in predictions]\n",
    "            \n",
    "            bleu_scores = []\n",
    "            for pred, ref in zip(predictions_nltk, references_nltk):\n",
    "                score = sentence_bleu(ref, pred, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "                bleu_scores.append(score)\n",
    "            \n",
    "            return np.mean(bleu_scores)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Simple BLEU implementation\n",
    "    def get_ngrams(tokens, n):\n",
    "        return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "    \n",
    "    total_score = 0\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_tokens = pred.split()\n",
    "        ref_tokens = ref.split()\n",
    "        \n",
    "        scores = []\n",
    "        for n in range(1, max_n + 1):\n",
    "            pred_ngrams = get_ngrams(pred_tokens, n)\n",
    "            ref_ngrams = get_ngrams(ref_tokens, n)\n",
    "            \n",
    "            if len(pred_ngrams) == 0:\n",
    "                scores.append(0)\n",
    "            else:\n",
    "                matches = sum(1 for ng in pred_ngrams if ng in ref_ngrams)\n",
    "                scores.append(matches / len(pred_ngrams))\n",
    "        \n",
    "        # Geometric mean of n-gram scores\n",
    "        if all(s > 0 for s in scores):\n",
    "            bleu = np.exp(np.mean(np.log(scores)))\n",
    "        else:\n",
    "            bleu = 0\n",
    "        \n",
    "        # Brevity penalty\n",
    "        bp = min(1.0, len(pred_tokens) / len(ref_tokens)) if len(ref_tokens) > 0 else 0\n",
    "        total_score += bleu * bp\n",
    "    \n",
    "    return total_score / len(predictions) if predictions else 0\n",
    "\n",
    "def evaluate_model(model, dataset, preprocessor, num_samples=50, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Evaluate the trained model on validation data.\n",
    "    \n",
    "    Returns BLEU scores and example translations.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    examples = []\n",
    "    \n",
    "    print(f\"üîç Evaluating model on {num_samples} samples...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(min(num_samples, len(dataset))):\n",
    "            sample = dataset[i]\n",
    "            \n",
    "            # Prepare input\n",
    "            src_seq = sample['source'].unsqueeze(0).to(device)\n",
    "            src_len = torch.tensor([sample['src_len']], device=device)\n",
    "            \n",
    "            # Generate translation\n",
    "            translation, attention = model.translate(src_seq, src_len, max_length=50)\n",
    "            \n",
    "            # Decode sequences\n",
    "            src_text = preprocessor.decode_sequence(sample['source'], is_source=True)\n",
    "            ref_text = preprocessor.decode_sequence(sample['target_output'], is_source=False)\n",
    "            pred_text = preprocessor.decode_sequence(translation, is_source=False)\n",
    "            \n",
    "            predictions.append(pred_text)\n",
    "            references.append(ref_text)\n",
    "            \n",
    "            # Store examples for display\n",
    "            if len(examples) < 10:\n",
    "                examples.append({\n",
    "                    'source': src_text,\n",
    "                    'reference': ref_text,\n",
    "                    'prediction': pred_text,\n",
    "                    'attention': attention\n",
    "                })\n",
    "    \n",
    "    # Compute BLEU score\n",
    "    bleu_score = compute_bleu_score(predictions, references)\n",
    "    \n",
    "    return bleu_score, examples, predictions, references\n",
    "\n",
    "# Load best model if available\n",
    "try:\n",
    "    checkpoint = torch.load('best_seq2seq_model.pth', map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"‚úÖ Loaded best model from checkpoint\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ÑπÔ∏è  Using current model state (no checkpoint found)\")\n",
    "\n",
    "# Evaluate the model\n",
    "bleu_score, examples, predictions, references = evaluate_model(\n",
    "    model, val_dataset, preprocessor, num_samples=30\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Evaluation Results:\")\n",
    "print(f\"   BLEU Score: {bleu_score:.4f}\")\n",
    "print(f\"   Samples evaluated: {len(predictions)}\")\n",
    "\n",
    "# Interpret BLEU score\n",
    "if bleu_score > 0.3:\n",
    "    quality = \"Excellent\"\n",
    "elif bleu_score > 0.2:\n",
    "    quality = \"Good\"\n",
    "elif bleu_score > 0.1:\n",
    "    quality = \"Fair\"\n",
    "else:\n",
    "    quality = \"Needs Improvement\"\n",
    "\n",
    "print(f\"   Translation Quality: {quality}\")\n",
    "\n",
    "# Display example translations\n",
    "print(f\"\\nüåè Sample Translations:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, example in enumerate(examples[:5], 1):\n",
    "    print(f\"{i}. üá¨üáß Source:     {example['source']}\")\n",
    "    print(f\"   üáªüá≥ Reference:  {example['reference']}\")\n",
    "    print(f\"   ü§ñ Prediction: {example['prediction']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Attention Visualization\n",
    "\n",
    "One of the key advantages of attention mechanisms is interpretability. We can visualize which parts of the source sentence the model \"pays attention to\" when generating each word in the target sentence.\n",
    "\n",
    "This helps us understand:\n",
    "- **Alignment quality** between source and target languages\n",
    "- **Translation patterns** the model has learned\n",
    "- **Potential issues** like attention collapse or misalignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(source_text, target_text, attention_weights, preprocessor):\n",
    "    \"\"\"\n",
    "    Create attention heatmap using seaborn.\n",
    "    \n",
    "    Args:\n",
    "        source_text: Source sentence\n",
    "        target_text: Target sentence  \n",
    "        attention_weights: Attention weights array (target_len, source_len)\n",
    "        preprocessor: Text preprocessor for tokenization\n",
    "    \"\"\"\n",
    "    # Tokenize sentences\n",
    "    src_tokens = preprocessor.tokenize(source_text)\n",
    "    tgt_tokens = target_text.split()\n",
    "    \n",
    "    # Trim attention weights to match actual tokens\n",
    "    attention_trimmed = attention_weights[:len(tgt_tokens), :len(src_tokens)]\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Use seaborn for better aesthetics\n",
    "    sns.heatmap(\n",
    "        attention_trimmed,\n",
    "        xticklabels=src_tokens,\n",
    "        yticklabels=tgt_tokens,\n",
    "        cmap='Blues',\n",
    "        cbar_kws={'label': 'Attention Weight'},\n",
    "        square=False,\n",
    "        linewidths=0.5\n",
    "    )\n",
    "    \n",
    "    plt.title('Attention Weights: English ‚Üí Vietnamese Translation\\n' + \n",
    "              f'Source: {source_text[:50]}...\\n' +\n",
    "              f'Target: {target_text[:50]}...', \n",
    "              fontsize=12, pad=20)\n",
    "    plt.xlabel('Source Tokens (English)', fontsize=11)\n",
    "    plt.ylabel('Target Tokens (Vietnamese)', fontsize=11)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print attention analysis\n",
    "    print(f\"üîç Attention Analysis:\")\n",
    "    print(f\"   Source length: {len(src_tokens)} tokens\")\n",
    "    print(f\"   Target length: {len(tgt_tokens)} tokens\")\n",
    "    print(f\"   Attention matrix shape: {attention_trimmed.shape}\")\n",
    "    \n",
    "    # Find strongest attention weights\n",
    "    max_attention = np.max(attention_trimmed)\n",
    "    max_pos = np.unravel_index(np.argmax(attention_trimmed), attention_trimmed.shape)\n",
    "    print(f\"   Strongest attention: {max_attention:.3f} at position {max_pos}\")\n",
    "    print(f\"   Target '{tgt_tokens[max_pos[0]]}' ‚Üê Source '{src_tokens[max_pos[1]]}'\")\n",
    "\n",
    "def create_interactive_translation_demo(model, preprocessor, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Create an interactive translation demonstration.\n",
    "    \n",
    "    Shows translations for new Australian tourism sentences.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # New test sentences (not in training data)\n",
    "    test_sentences = [\n",
    "        \"Australia has unique wildlife like koalas and kangaroos.\",\n",
    "        \"The weather in Sydney is perfect for outdoor activities.\",\n",
    "        \"Melbourne's food scene is diverse and exciting.\",\n",
    "        \"The Great Ocean Road offers stunning coastal scenery.\",\n",
    "        \"Adelaide is known for its wine and festivals.\",\n",
    "        \"Tasmania's wilderness areas are pristine and beautiful.\",\n",
    "        \"Perth has some of the world's most beautiful beaches.\",\n",
    "        \"The Australian Outback is vast and mysterious.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üé≠ Interactive Australian Tourism Translation Demo\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    translations = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, sentence in enumerate(test_sentences, 1):\n",
    "            # Encode sentence\n",
    "            src_indices = preprocessor.encode_sentence(sentence, is_source=True, add_eos=True)\n",
    "            src_seq = torch.tensor([src_indices], dtype=torch.long, device=device)\n",
    "            src_len = torch.tensor([len(src_indices)], device=device)\n",
    "            \n",
    "            # Generate translation\n",
    "            translation, attention = model.translate(src_seq, src_len, max_length=50)\n",
    "            \n",
    "            # Decode translation\n",
    "            translated_text = preprocessor.decode_sequence(translation, is_source=False)\n",
    "            \n",
    "            # Store for visualization\n",
    "            translations.append({\n",
    "                'source': sentence,\n",
    "                'translation': translated_text,\n",
    "                'attention': attention\n",
    "            })\n",
    "            \n",
    "            print(f\"{i}. üá¨üáß English:    {sentence}\")\n",
    "            print(f\"   üáªüá≥ Vietnamese: {translated_text}\")\n",
    "            print()\n",
    "    \n",
    "    return translations\n",
    "\n",
    "# Run interactive demo\n",
    "demo_translations = create_interactive_translation_demo(model, preprocessor)\n",
    "\n",
    "# Visualize attention for a couple of examples\n",
    "print(\"\\nüé® Attention Visualizations:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show attention for first two examples\n",
    "for i, example in enumerate(demo_translations[:2]):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    visualize_attention(\n",
    "        example['source'], \n",
    "        example['translation'], \n",
    "        example['attention'], \n",
    "        preprocessor\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ TensorFlow vs PyTorch Implementation Comparison\n",
    "\n",
    "Let's compare key differences between implementing seq2seq models in TensorFlow and PyTorch:\n",
    "\n",
    "### **Key Architectural Differences**\n",
    "\n",
    "| Component | TensorFlow/Keras | PyTorch |\n",
    "|-----------|------------------|----------|\n",
    "| **Model Definition** | Functional/Sequential API | `nn.Module` subclassing |\n",
    "| **Training Loop** | `model.fit()` | Manual loop with optimizer steps |\n",
    "| **Attention** | `tf.keras.layers.Attention` | Custom implementation |\n",
    "| **Data Loading** | `tf.data.Dataset` | `torch.utils.data.DataLoader` |\n",
    "| **Device Management** | Mostly automatic | Explicit `.to(device)` calls |\n",
    "\n",
    "### **Code Comparison Examples**\n",
    "\n",
    "#### **Model Training:**\n",
    "```python\n",
    "# TensorFlow/Keras approach\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "history = model.fit(train_data, validation_data=val_data, epochs=20)\n",
    "\n",
    "# PyTorch approach (what we implemented)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "for epoch in range(20):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs, attention = model(src_seq, tgt_seq)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "#### **Attention Mechanism:**\n",
    "```python\n",
    "# TensorFlow/Keras\n",
    "attention_layer = tf.keras.layers.Attention()\n",
    "context = attention_layer([query, value, key])\n",
    "\n",
    "# PyTorch (our implementation)\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim, decoder_hidden_dim, attention_dim):\n",
    "        # Custom attention implementation with linear layers\n",
    "        self.attention_vector = nn.Linear(attention_dim, 1, bias=False)\n",
    "```\n",
    "\n",
    "### **Advantages of Each Framework**\n",
    "\n",
    "**TensorFlow/Keras Advantages:**\n",
    "- üöÄ **Faster prototyping** with high-level APIs\n",
    "- üîß **Built-in training loops** with callbacks and metrics\n",
    "- üìä **Integrated visualization** with TensorBoard\n",
    "- üè≠ **Production deployment** tools (TensorFlow Serving, TensorFlow Lite)\n",
    "\n",
    "**PyTorch Advantages:**\n",
    "- üî¨ **Research flexibility** with dynamic computation graphs\n",
    "- üõ†Ô∏è **Fine-grained control** over training process\n",
    "- üêç **Pythonic design** that feels more natural\n",
    "- üîç **Easier debugging** with standard Python debugging tools\n",
    "- üìö **Better for learning** underlying ML concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Summary and Next Steps\n",
    "\n",
    "Congratulations! You've successfully implemented a complete sequence-to-sequence neural machine translation model from scratch. Here's what we accomplished:\n",
    "\n",
    "### **‚úÖ What We Built**\n",
    "\n",
    "1. **üìö Australian Tourism Translation Dataset** - English-Vietnamese pairs with cultural context\n",
    "2. **üî§ Text Preprocessing Pipeline** - Unicode normalization, tokenization, vocabulary building\n",
    "3. **üèóÔ∏è Seq2Seq Architecture** - Bidirectional LSTM encoder with attention mechanism\n",
    "4. **üéØ Attention Mechanism** - Additive (Bahdanau) attention for better translations\n",
    "5. **üèãÔ∏è Training Pipeline** - Complete training loop with TensorBoard integration\n",
    "6. **üìä Evaluation Framework** - BLEU scores and qualitative analysis\n",
    "7. **üé® Attention Visualization** - Interpretable attention weight heatmaps\n",
    "8. **üé≠ Interactive Demo** - Real-time translation of Australian tourism content\n",
    "\n",
    "### **üîë Key Learning Points**\n",
    "\n",
    "- **Encoder-Decoder Architecture**: How to encode source sequences and decode target sequences\n",
    "- **Attention Mechanisms**: Why attention helps and how to implement it\n",
    "- **Teacher Forcing**: Training technique for sequence generation models\n",
    "- **PyTorch Training Loops**: Manual training vs TensorFlow's automated approach\n",
    "- **Translation Evaluation**: BLEU scores and qualitative assessment methods\n",
    "- **Attention Visualization**: Making neural networks more interpretable\n",
    "\n",
    "### **üöÄ Next Steps for Improvement**\n",
    "\n",
    "1. **üî§ Advanced Tokenization**\n",
    "   - Implement subword tokenization (BPE, SentencePiece)\n",
    "   - Handle out-of-vocabulary words better\n",
    "   - Add proper Vietnamese word segmentation\n",
    "\n",
    "2. **üß† Model Architecture**\n",
    "   - Try Transformer models (self-attention)\n",
    "   - Experiment with different attention mechanisms\n",
    "   - Add copy mechanisms for handling proper nouns\n",
    "\n",
    "3. **üìä Data and Evaluation**\n",
    "   - Collect larger, more diverse datasets\n",
    "   - Implement more evaluation metrics (METEOR, ROUGE-L)\n",
    "   - Add human evaluation studies\n",
    "\n",
    "4. **üè≠ Production Deployment**\n",
    "   - Model quantization for faster inference\n",
    "   - Beam search decoding for better translations\n",
    "   - REST API deployment with FastAPI/Flask\n",
    "\n",
    "5. **üî¨ Research Extensions**\n",
    "   - Multi-language support beyond English-Vietnamese\n",
    "   - Domain adaptation for different types of content\n",
    "   - Zero-shot translation capabilities\n",
    "\n",
    "### **üìö Additional Resources**\n",
    "\n",
    "- **Papers**: \"Neural Machine Translation by Jointly Learning to Align and Translate\" (Bahdanau et al.)\n",
    "- **Books**: \"Natural Language Processing with Python\" and \"Deep Learning\" (Goodfellow et al.)\n",
    "- **Courses**: Stanford CS224N, Fast.ai NLP course\n",
    "- **Datasets**: WMT translation shared tasks, OpenSubtitles corpus\n",
    "\n",
    "### **üéØ Repository Integration**\n",
    "\n",
    "This notebook demonstrates:\n",
    "- ‚úÖ **Australian context** in all examples and data\n",
    "- ‚úÖ **Vietnamese as secondary language** for multilingual tasks\n",
    "- ‚úÖ **PyTorch vs TensorFlow comparisons** for learning transition\n",
    "- ‚úÖ **Device-aware implementation** with CPU/GPU/MPS support\n",
    "- ‚úÖ **TensorBoard integration** following repository standards\n",
    "- ‚úÖ **Seaborn visualizations** for training metrics\n",
    "- ‚úÖ **Comprehensive documentation** with learning objectives\n",
    "\n",
    "You now have a solid foundation in sequence-to-sequence models and attention mechanisms. The skills you've learned here transfer directly to modern Transformer architectures and can be applied to many other sequence modeling tasks beyond translation!\n",
    "\n",
    "**Happy coding and keep exploring! üöÄüá¶üá∫üáªüá≥**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}