#!/usr/bin/env python3
"""
Create comprehensive Word Embeddings NLP notebook with Australian context
"""

import json

def create_word_embeddings_notebook():
    """Create comprehensive word embeddings notebook with Australian tourism focus."""
    
    # Notebook metadata
    notebook = {
        "cells": [],
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "codemirror_mode": {
                    "name": "ipython",
                    "version": 3
                },
                "file_extension": ".py",
                "mimetype": "text/x-python",
                "name": "python",
                "nbconvert_exporter": "python",
                "pygments_lexer": "ipython3",
                "version": "3.12.3"
            }
        },
        "nbformat": 4,
        "nbformat_minor": 4
    }
    
    # Cell 1: Title and Introduction
    notebook["cells"].append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "# Word Embeddings: Encoding Lexical Semantics üá¶üá∫\n",
            "\n",
            "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/pytorch-mastery/blob/main/examples/pytorch-nlp/word_embeddings_nllp.ipynb)\n",
            "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/pytorch-mastery/blob/main/examples/pytorch-nlp/word_embeddings_nllp.ipynb)\n",
            "\n",
            "A comprehensive guide to word embeddings using PyTorch, featuring Australian tourism examples and English-Vietnamese multilingual support. Learn how to encode lexical semantics and capture semantic relationships in Australian tourism vocabulary.\n",
            "\n",
            "## Learning Objectives\n",
            "\n",
            "By the end of this notebook, you will:\n",
            "\n",
            "- üî§ **Master word embedding techniques** including Word2Vec, GloVe, and FastText\n",
            "- üá¶üá∫ **Train custom embeddings** on Australian tourism corpus\n",
            "- üåè **Handle multilingual embeddings** for English-Vietnamese text\n",
            "- üìä **Visualize semantic relationships** between Australian cities and landmarks\n",
            "- üîÑ **Compare PyTorch vs TensorFlow** embedding implementations\n",
            "- üéØ **Apply embeddings** to real Australian NLP tasks\n",
            "\n",
            "## What You'll Build\n",
            "\n",
            "1. **Australian Tourism Word2Vec Model** - Capture semantic relationships in tourism vocabulary\n",
            "2. **Multilingual Embedding Space** - Align English and Vietnamese tourism terms\n",
            "3. **Semantic Similarity Engine** - Find similar Australian cities and attractions\n",
            "4. **Interactive Visualization** - Explore embedding space with t-SNE and PCA\n",
            "\n",
            "---"
        ]
    })
    
    # Cell 2: Environment Setup (reuse from deep_learning_nlp)
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Environment Detection and Setup\n",
            "import sys\n",
            "import subprocess\n",
            "import os\n",
            "import time\n",
            "\n",
            "# Detect the runtime environment\n",
            "IS_COLAB = \"google.colab\" in sys.modules\n",
            "IS_KAGGLE = \"kaggle_secrets\" in sys.modules or \"kaggle\" in os.environ.get('KAGGLE_URL_BASE', '')\n",
            "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
            "\n",
            "print(f\"üîç Environment Detection:\")\n",
            "print(f\"   Local Development: {IS_LOCAL}\")\n",
            "print(f\"   Google Colab: {IS_COLAB}\")\n",
            "print(f\"   Kaggle Notebooks: {IS_KAGGLE}\")\n",
            "\n",
            "# Platform-specific system setup\n",
            "if IS_COLAB:\n",
            "    print(\"\\n‚öôÔ∏è  Setting up Google Colab environment...\")\n",
            "    !apt update -qq\n",
            "    !apt install -y -qq software-properties-common\n",
            "elif IS_KAGGLE:\n",
            "    print(\"\\n‚öôÔ∏è  Setting up Kaggle environment...\")\n",
            "    # Kaggle usually has most packages pre-installed\n",
            "else:\n",
            "    print(\"\\n‚öôÔ∏è  Setting up local environment...\")"
        ]
    })
    
    # Cell 3: Package Installation
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Install required packages for word embeddings\n",
            "required_packages = [\n",
            "    \"torch\",\n",
            "    \"transformers\",\n",
            "    \"datasets\", \n",
            "    \"tokenizers\",\n",
            "    \"pandas\",\n",
            "    \"seaborn\",\n",
            "    \"matplotlib\",\n",
            "    \"scikit-learn\",\n",
            "    \"tensorboard\",\n",
            "    \"gensim\",  # For Word2Vec and GloVe implementations\n",
            "    \"plotly\",  # For interactive visualizations\n",
            "]\n",
            "\n",
            "print(\"üì¶ Installing packages for word embeddings...\")\n",
            "for package in required_packages:\n",
            "    if IS_COLAB or IS_KAGGLE:\n",
            "        !pip install -q {package}\n",
            "    else:\n",
            "        try:\n",
            "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
            "                          capture_output=True, check=True)\n",
            "        except subprocess.CalledProcessError:\n",
            "            print(f\"   ‚ö†Ô∏è  {package} installation skipped (likely already installed)\")\n",
            "            continue\n",
            "    print(f\"   ‚úÖ {package}\")\n",
            "\n",
            "print(\"\\nüéâ Package installation completed!\")"
        ]
    })
    
    # Cell 4: Import Libraries
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Import essential libraries for word embeddings\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import torch.optim as optim\n",
            "import torch.nn.functional as F\n",
            "from torch.utils.data import Dataset, DataLoader\n",
            "from torch.utils.tensorboard import SummaryWriter\n",
            "\n",
            "# Data handling and visualization\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as sns\n",
            "import matplotlib.pyplot as plt\n",
            "import plotly.express as px\n",
            "import plotly.graph_objects as go\n",
            "from plotly.subplots import make_subplots\n",
            "\n",
            "# Machine learning and embeddings\n",
            "from sklearn.manifold import TSNE\n",
            "from sklearn.decomposition import PCA\n",
            "from sklearn.metrics.pairwise import cosine_similarity\n",
            "from sklearn.cluster import KMeans\n",
            "\n",
            "# Text processing\n",
            "import re\n",
            "import string\n",
            "from collections import Counter, defaultdict\n",
            "import random\n",
            "from itertools import combinations\n",
            "\n",
            "# Gensim for pre-trained embeddings and Word2Vec\n",
            "try:\n",
            "    from gensim.models import Word2Vec, FastText\n",
            "    from gensim.models.keyedvectors import KeyedVectors\n",
            "    print(\"‚úÖ Gensim imported successfully\")\n",
            "except ImportError:\n",
            "    print(\"‚ö†Ô∏è  Gensim not available - will use PyTorch implementations only\")\n",
            "\n",
            "# Set style for better notebook aesthetics\n",
            "sns.set_style(\"whitegrid\")\n",
            "sns.set_palette(\"Set2\")\n",
            "plt.rcParams['figure.figsize'] = (14, 8)\n",
            "\n",
            "# Set random seeds for reproducibility\n",
            "torch.manual_seed(16)\n",
            "np.random.seed(16)\n",
            "random.seed(16)\n",
            "\n",
            "print(f\"üî§ Word Embeddings Environment Ready!\")\n",
            "print(f\"   PyTorch version: {torch.__version__}\")\n",
            "print(f\"   Libraries loaded successfully\")"
        ]
    })
    
    # Cell 5: Device Detection
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "import platform\n",
            "\n",
            "def detect_device():\n",
            "    \"\"\"Detect optimal device for word embeddings training.\"\"\"\n",
            "    if torch.cuda.is_available():\n",
            "        device = torch.device(\"cuda\")\n",
            "        gpu_name = torch.cuda.get_device_name(0)\n",
            "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
            "        \n",
            "        print(f\"üöÄ CUDA GPU detected: {gpu_name}\")\n",
            "        print(f\"   GPU Memory: {gpu_memory:.1f} GB\")\n",
            "        print(f\"   Optimal for large embedding training\")\n",
            "        \n",
            "        return device\n",
            "    \n",
            "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
            "        device = torch.device(\"mps\")\n",
            "        system_info = platform.uname()\n",
            "        \n",
            "        print(f\"üçé Apple Silicon MPS detected: {system_info.machine}\")\n",
            "        print(f\"   Optimized for M1/M2/M3 chips\")\n",
            "        print(f\"   Good performance for embedding training\")\n",
            "        \n",
            "        return device\n",
            "    \n",
            "    else:\n",
            "        device = torch.device(\"cpu\")\n",
            "        cpu_count = torch.get_num_threads()\n",
            "        \n",
            "        print(f\"üíª CPU mode: {platform.processor()}\")\n",
            "        print(f\"   Threads: {cpu_count}\")\n",
            "        print(f\"   üí° Tip: Use smaller embedding dimensions for faster training\")\n",
            "        \n",
            "        return device\n",
            "\n",
            "# Detect and set device\n",
            "DEVICE = detect_device()\n",
            "print(f\"\\n‚úÖ Device selected: {DEVICE}\")"
        ]
    })
    
    # Cell 6: Australian Tourism Corpus Creation
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Create comprehensive Australian tourism corpus for embedding training\n",
            "def create_australian_tourism_corpus():\n",
            "    \"\"\"\n",
            "    Create a rich corpus of Australian tourism content for training embeddings.\n",
            "    \n",
            "    Returns:\n",
            "        dict: Contains English and Vietnamese text with metadata\n",
            "    \"\"\"\n",
            "    \n",
            "    # English corpus - Australian tourism content\n",
            "    english_corpus = [\n",
            "        # Sydney content\n",
            "        \"Sydney Opera House is an iconic architectural masterpiece located on Bennelong Point in Sydney Harbour.\",\n",
            "        \"The Sydney Harbour Bridge offers spectacular views of the harbour and city skyline.\",\n",
            "        \"Bondi Beach is famous for surfing and hosts many international surfing competitions.\",\n",
            "        \"The Royal Botanic Gardens Sydney showcase native Australian flora and fauna.\",\n",
            "        \"Darling Harbour features world-class museums, restaurants, and entertainment venues.\",\n",
            "        \"The Rocks historic area preserves Sydney's convict heritage and colonial architecture.\",\n",
            "        \n",
            "        # Melbourne content\n",
            "        \"Melbourne is renowned for its vibrant coffee culture and laneway street art.\",\n",
            "        \"The Royal Exhibition Building in Carlton Gardens is a UNESCO World Heritage site.\",\n",
            "        \"Federation Square hosts cultural events and houses major galleries and museums.\",\n",
            "        \"Melbourne's tram network is the largest in the world and iconic to the city.\",\n",
            "        \"The Yarra River flows through Melbourne's central business district and parks.\",\n",
            "        \"Queen Victoria Market offers fresh produce, gourmet food, and unique souvenirs.\",\n",
            "        \n",
            "        # Queensland content\n",
            "        \"The Great Barrier Reef is the world's largest coral reef system and UNESCO World Heritage site.\",\n",
            "        \"Brisbane's South Bank features cultural institutions, restaurants, and riverside parks.\",\n",
            "        \"Gold Coast is famous for its theme parks, surfing beaches, and nightlife.\",\n",
            "        \"Cairns serves as the gateway to the Great Barrier Reef and Daintree Rainforest.\",\n",
            "        \"Fraser Island is the world's largest sand island with unique ecosystems.\",\n",
            "        \"Whitsunday Islands offer pristine beaches and excellent sailing conditions.\",\n",
            "        \n",
            "        # Western Australia content  \n",
            "        \"Perth is one of the most isolated major cities in the world.\",\n",
            "        \"Fremantle port city features well-preserved colonial architecture and maritime heritage.\",\n",
            "        \"Rottnest Island is home to quokkas and beautiful secluded beaches.\",\n",
            "        \"The Pinnacles Desert showcases thousands of limestone pillars in unique formations.\",\n",
            "        \"Margaret River region produces world-class wines and gourmet food.\",\n",
            "        \"Broome features Cable Beach with stunning sunsets and pearl diving history.\",\n",
            "        \n",
            "        # South Australia content\n",
            "        \"Adelaide is known as the Festival City with numerous cultural celebrations.\",\n",
            "        \"Barossa Valley produces premium wines and hosts international wine festivals.\",\n",
            "        \"Kangaroo Island wildlife sanctuary protects native Australian animals in natural habitat.\",\n",
            "        \"Adelaide Hills wine region offers cool climate varieties and scenic vineyards.\",\n",
            "        \"Flinders Ranges feature ancient mountain landscapes and Aboriginal cultural sites.\",\n",
            "        \n",
            "        # Northern Territory content\n",
            "        \"Uluru is a sacred Aboriginal site and iconic symbol of Australia.\",\n",
            "        \"Kata Tjuta rock formations complement Uluru in the heart of Australia.\",\n",
            "        \"Darwin serves as the gateway to Kakadu National Park and Top End wilderness.\",\n",
            "        \"Kakadu National Park preserves ancient Aboriginal rock art and diverse ecosystems.\",\n",
            "        \"Alice Springs is the heart of the Australian outback and Red Centre.\",\n",
            "        \n",
            "        # Tasmania content\n",
            "        \"Hobart's Museum of Old and New Art challenges visitors with provocative contemporary art.\",\n",
            "        \"Cradle Mountain-Lake St Clair National Park offers pristine wilderness hiking.\",\n",
            "        \"Salamanca Market in Hobart features local artisans and Tasmania's finest produce.\",\n",
            "        \"Devil's island Tasmania protects the endangered Tasmanian devil in natural habitat.\",\n",
            "        \n",
            "        # ACT content\n",
            "        \"Canberra houses Australia's national institutions including Parliament House and galleries.\",\n",
            "        \"Australian War Memorial commemorates the service of Australian armed forces.\",\n",
            "        \"National Gallery of Australia showcases the finest Australian and international art.\",\n",
            "        \"Lake Burley Griffin provides recreational activities in the heart of Canberra.\"\n",
            "    ]\n",
            "    \n",
            "    # Vietnamese corpus - translations and local content\n",
            "    vietnamese_corpus = [\n",
            "        # Sydney translations\n",
            "        \"Nh√† h√°t Opera Sydney l√† ki·ªát t√°c ki·∫øn tr√∫c bi·ªÉu t∆∞·ª£ng t·ªça l·∫°c t·∫°i Bennelong Point ·ªü C·∫£ng Sydney.\",\n",
            "        \"C·∫ßu C·∫£ng Sydney mang ƒë·∫øn t·∫ßm nh√¨n ngo·∫°n m·ª•c ra c·∫£ng v√† ƒë∆∞·ªùng ch√¢n tr·ªùi th√†nh ph·ªë.\",\n",
            "        \"B√£i bi·ªÉn Bondi n·ªïi ti·∫øng v·ªõi l∆∞·ªõt s√≥ng v√† t·ªï ch·ª©c nhi·ªÅu cu·ªôc thi l∆∞·ªõt s√≥ng qu·ªëc t·∫ø.\",\n",
            "        \"V∆∞·ªùn B√°ch th·∫£o Ho√†ng gia Sydney tr∆∞ng b√†y h·ªá ƒë·ªông th·ª±c v·∫≠t b·∫£n ƒë·ªãa Australia.\",\n",
            "        \n",
            "        # Melbourne translations\n",
            "        \"Melbourne n·ªïi ti·∫øng v·ªõi vƒÉn h√≥a c√† ph√™ s√¥i ƒë·ªông v√† ngh·ªá thu·∫≠t ƒë∆∞·ªùng ph·ªë trong c√°c con h·∫ªm.\",\n",
            "        \"T√≤a nh√† Tri·ªÉn l√£m Ho√†ng gia ·ªü Carlton Gardens l√† di s·∫£n th·∫ø gi·ªõi UNESCO.\",\n",
            "        \"Qu·∫£ng tr∆∞·ªùng Federation t·ªï ch·ª©c c√°c s·ª± ki·ªán vƒÉn h√≥a v√† c√≥ c√°c ph√≤ng tr∆∞ng b√†y l·ªõn.\",\n",
            "        \"M·∫°ng l∆∞·ªõi t√†u ƒëi·ªán Melbourne l√† l·ªõn nh·∫•t th·∫ø gi·ªõi v√† mang t√≠nh bi·ªÉu t∆∞·ª£ng c·ªßa th√†nh ph·ªë.\",\n",
            "        \n",
            "        # Queensland translations\n",
            "        \"R·∫°n san h√¥ Great Barrier l√† h·ªá th·ªëng r·∫°n san h√¥ l·ªõn nh·∫•t th·∫ø gi·ªõi v√† di s·∫£n UNESCO.\",\n",
            "        \"South Bank Brisbane c√≥ c√°c t·ªï ch·ª©c vƒÉn h√≥a, nh√† h√†ng v√† c√¥ng vi√™n ven s√¥ng.\",\n",
            "        \"Gold Coast n·ªïi ti·∫øng v·ªõi c√°c c√¥ng vi√™n gi·∫£i tr√≠, b√£i bi·ªÉn l∆∞·ªõt s√≥ng v√† cu·ªôc s·ªëng v·ªÅ ƒë√™m.\",\n",
            "        \"Cairns l√† c·ª≠a ng√µ ƒë·∫øn R·∫°n san h√¥ Great Barrier v√† R·ª´ng m∆∞a Daintree.\",\n",
            "        \n",
            "        # Other regions\n",
            "        \"Perth l√† m·ªôt trong nh·ªØng th√†nh ph·ªë l·ªõn bi·ªát l·∫≠p nh·∫•t tr√™n th·∫ø gi·ªõi.\",\n",
            "        \"Adelaide ƒë∆∞·ª£c bi·∫øt ƒë·∫øn l√† Th√†nh ph·ªë L·ªÖ h·ªôi v·ªõi nhi·ªÅu celebration vƒÉn h√≥a.\",\n",
            "        \"Uluru l√† ƒë·ªãa ƒëi·ªÉm thi√™ng li√™ng c·ªßa th·ªï d√¢n v√† bi·ªÉu t∆∞·ª£ng c·ªßa Australia.\",\n",
            "        \"Hobart c√≥ B·∫£o t√†ng Ngh·ªá thu·∫≠t C≈© v√† M·ªõi th√°ch th·ª©c du kh√°ch v·ªõi ngh·ªá thu·∫≠t ƒë∆∞∆°ng ƒë·∫°i.\",\n",
            "        \"Canberra ch·ª©a c√°c t·ªï ch·ª©c qu·ªëc gia c·ªßa Australia bao g·ªìm T√≤a nh√† Qu·ªëc h·ªôi.\"\n",
            "    ]\n",
            "    \n",
            "    return {\n",
            "        'english': english_corpus,\n",
            "        'vietnamese': vietnamese_corpus,\n",
            "        'combined': english_corpus + vietnamese_corpus\n",
            "    }\n",
            "\n",
            "# Create the corpus\n",
            "tourism_corpus = create_australian_tourism_corpus()\n",
            "\n",
            "print(\"üá¶üá∫ Australian Tourism Corpus Created\")\n",
            "print(\"=\" * 45)\n",
            "print(f\"   English texts: {len(tourism_corpus['english'])}\")\n",
            "print(f\"   Vietnamese texts: {len(tourism_corpus['vietnamese'])}\")\n",
            "print(f\"   Total corpus size: {len(tourism_corpus['combined'])}\")\n",
            "\n",
            "# Display sample texts\n",
            "print(f\"\\nüìù Sample English text:\")\n",
            "print(f\"   {tourism_corpus['english'][0]}\")\n",
            "print(f\"\\nüìù Sample Vietnamese text:\")\n",
            "print(f\"   {tourism_corpus['vietnamese'][0]}\")\n",
            "\n",
            "# Analyze vocabulary\n",
            "all_words = []\n",
            "for text in tourism_corpus['combined']:\n",
            "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
            "    all_words.extend(words)\n",
            "\n",
            "vocab_counter = Counter(all_words)\n",
            "unique_words = len(vocab_counter)\n",
            "total_words = len(all_words)\n",
            "\n",
            "print(f\"\\nüìä Corpus Statistics:\")\n",
            "print(f\"   Total words: {total_words:,}\")\n",
            "print(f\"   Unique words: {unique_words:,}\")\n",
            "print(f\"   Vocabulary richness: {unique_words/total_words:.3f}\")\n",
            "\n",
            "# Show most common Australian terms\n",
            "australian_terms = [word for word, count in vocab_counter.most_common(20) \n",
            "                   if word in ['sydney', 'melbourne', 'brisbane', 'perth', 'adelaide', \n",
            "                              'darwin', 'hobart', 'canberra', 'australia', 'australian',\n",
            "                              'beach', 'harbour', 'reef', 'park', 'island']]\n",
            "print(f\"\\nüèôÔ∏è  Top Australian terms: {', '.join(australian_terms[:10])}\")"
        ]
    })
    
    # Cell 7: Text Preprocessing for Embeddings
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "class AustralianEmbeddingPreprocessor:\n",
            "    \"\"\"\n",
            "    Specialized text preprocessor for Australian tourism embeddings.\n",
            "    \n",
            "    Handles both English and Vietnamese text while preserving important\n",
            "    Australian geographic and cultural terms.\n",
            "    \"\"\"\n",
            "    \n",
            "    def __init__(self):\n",
            "        # Protected Australian terms that should not be heavily modified\n",
            "        self.protected_terms = {\n",
            "            'cities': ['sydney', 'melbourne', 'brisbane', 'perth', 'adelaide', \n",
            "                      'darwin', 'hobart', 'canberra'],\n",
            "            'landmarks': ['uluru', 'kata', 'tjuta', 'kakadu', 'pinnacles', \n",
            "                         'cradle', 'mountain', 'fraser', 'rottnest'],\n",
            "            'features': ['harbour', 'reef', 'outback', 'rainforest', 'desert',\n",
            "                        'beach', 'island', 'river', 'park', 'gardens'],\n",
            "            'cultural': ['aboriginal', 'indigenous', 'heritage', 'colonial',\n",
            "                        'convict', 'federation', 'anzac']\n",
            "        }\n",
            "        \n",
            "        # Vietnamese-specific terms to preserve\n",
            "        self.vietnamese_terms = ['nh√†', 'h√°t', 'opera', 'c·∫ßu', 'c·∫£ng', 'b√£i', 'bi·ªÉn',\n",
            "                               'v∆∞·ªùn', 'b√°ch', 'th·∫£o', 'r·∫°n', 'san', 'h√¥', 'th√†nh', 'ph·ªë']\n",
            "    \n",
            "    def tokenize_sentence(self, text):\n",
            "        \"\"\"\n",
            "        Tokenize text into sentences, preserving Australian terms.\n",
            "        \n",
            "        Args:\n",
            "            text (str): Input text\n",
            "            \n",
            "        Returns:\n",
            "            list: List of tokenized words\n",
            "        \"\"\"\n",
            "        # Convert to lowercase\n",
            "        text = text.lower()\n",
            "        \n",
            "        # Remove punctuation but preserve apostrophes in contractions\n",
            "        text = re.sub(r\"[^\\w\\s']\", ' ', text)\n",
            "        \n",
            "        # Handle contractions\n",
            "        contractions = {\n",
            "            \"n't\": \" not\",\n",
            "            \"'re\": \" are\", \n",
            "            \"'s\": \" is\",\n",
            "            \"'ve\": \" have\",\n",
            "            \"'ll\": \" will\",\n",
            "            \"'d\": \" would\"\n",
            "        }\n",
            "        \n",
            "        for contraction, expansion in contractions.items():\n",
            "            text = text.replace(contraction, expansion)\n",
            "        \n",
            "        # Split into words\n",
            "        words = text.split()\n",
            "        \n",
            "        # Filter out very short words (except important ones)\n",
            "        important_short_words = {'wa', 'sa', 'nt', 'tas', 'act', 'nsw', 'vic', 'qld'}\n",
            "        words = [word for word in words \n",
            "                if len(word) > 2 or word in important_short_words]\n",
            "        \n",
            "        return words\n",
            "    \n",
            "    def prepare_training_data(self, corpus):\n",
            "        \"\"\"\n",
            "        Prepare corpus for embedding training.\n",
            "        \n",
            "        Args:\n",
            "            corpus (list): List of text documents\n",
            "            \n",
            "        Returns:\n",
            "            list: List of tokenized sentences\n",
            "        \"\"\"\n",
            "        tokenized_corpus = []\n",
            "        \n",
            "        for text in corpus:\n",
            "            # Split into sentences\n",
            "            sentences = re.split(r'[.!?]+', text)\n",
            "            \n",
            "            for sentence in sentences:\n",
            "                sentence = sentence.strip()\n",
            "                if len(sentence) > 10:  # Skip very short sentences\n",
            "                    tokens = self.tokenize_sentence(sentence)\n",
            "                    if len(tokens) >= 3:  # Minimum sentence length\n",
            "                        tokenized_corpus.append(tokens)\n",
            "        \n",
            "        return tokenized_corpus\n",
            "    \n",
            "    def analyze_vocabulary(self, tokenized_corpus):\n",
            "        \"\"\"\n",
            "        Analyze vocabulary statistics from tokenized corpus.\n",
            "        \n",
            "        Args:\n",
            "            tokenized_corpus (list): List of tokenized sentences\n",
            "            \n",
            "        Returns:\n",
            "            dict: Vocabulary statistics\n",
            "        \"\"\"\n",
            "        all_words = []\n",
            "        for sentence in tokenized_corpus:\n",
            "            all_words.extend(sentence)\n",
            "        \n",
            "        word_freq = Counter(all_words)\n",
            "        \n",
            "        # Find Australian-specific terms\n",
            "        australian_words = []\n",
            "        for word, freq in word_freq.items():\n",
            "            if any(word in terms for terms in self.protected_terms.values()):\n",
            "                australian_words.append((word, freq))\n",
            "        \n",
            "        # Find Vietnamese terms\n",
            "        vietnamese_words = [(word, freq) for word, freq in word_freq.items() \n",
            "                           if word in self.vietnamese_terms]\n",
            "        \n",
            "        return {\n",
            "            'total_words': len(all_words),\n",
            "            'unique_words': len(word_freq),\n",
            "            'word_frequencies': word_freq,\n",
            "            'australian_terms': australian_words,\n",
            "            'vietnamese_terms': vietnamese_words,\n",
            "            'avg_sentence_length': np.mean([len(s) for s in tokenized_corpus])\n",
            "        }\n",
            "\n",
            "# Initialize preprocessor and prepare data\n",
            "preprocessor = AustralianEmbeddingPreprocessor()\n",
            "\n",
            "# Prepare training data\n",
            "tokenized_corpus = preprocessor.prepare_training_data(tourism_corpus['combined'])\n",
            "vocab_stats = preprocessor.analyze_vocabulary(tokenized_corpus)\n",
            "\n",
            "print(\"üî§ Text Preprocessing for Australian Tourism Embeddings\")\n",
            "print(\"=\" * 55)\n",
            "print(f\"   Total sentences: {len(tokenized_corpus)}\")\n",
            "print(f\"   Total words: {vocab_stats['total_words']:,}\")\n",
            "print(f\"   Unique vocabulary: {vocab_stats['unique_words']:,}\")\n",
            "print(f\"   Average sentence length: {vocab_stats['avg_sentence_length']:.1f} words\")\n",
            "\n",
            "print(f\"\\nüá¶üá∫ Australian terms found: {len(vocab_stats['australian_terms'])}\")\n",
            "australian_terms_str = ', '.join([term for term, freq in vocab_stats['australian_terms'][:10]])\n",
            "print(f\"   Top terms: {australian_terms_str}\")\n",
            "\n",
            "print(f\"\\nüáªüá≥ Vietnamese terms found: {len(vocab_stats['vietnamese_terms'])}\")\n",
            "if vocab_stats['vietnamese_terms']:\n",
            "    vietnamese_terms_str = ', '.join([term for term, freq in vocab_stats['vietnamese_terms'][:5]])\n",
            "    print(f\"   Sample terms: {vietnamese_terms_str}\")\n",
            "\n",
            "# Show sample tokenized sentences\n",
            "print(f\"\\nüìù Sample tokenized sentences:\")\n",
            "for i, sentence in enumerate(tokenized_corpus[:3]):\n",
            "    print(f\"   {i+1}. {sentence[:8]}... ({len(sentence)} words)\")\n",
            "\n",
            "print(f\"\\n‚úÖ Preprocessing completed - ready for embedding training!\")"
        ]
    })
    
    # Save the initial notebook
    with open("word_embeddings_nllp.ipynb", "w") as f:
        json.dump(notebook, f, indent=2)
    
    return notebook

if __name__ == "__main__":
    notebook = create_word_embeddings_notebook()
    print("‚úÖ Word Embeddings notebook foundation created successfully!")