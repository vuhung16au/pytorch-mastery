#!/usr/bin/env python3
"""
Complete the Deep Learning NLP notebook with visualization and summary cells
"""

import json

def complete_deep_learning_notebook():
    """Add final cells for visualization, model testing, and summary."""
    
    # Load existing notebook
    with open("deep_learning_nlp.ipynb", "r") as f:
        notebook = json.load(f)
    
    # Cell 12: Training Visualization
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Training Results Visualization\n",
            "def plot_training_history(history):\n",
            "    \"\"\"\n",
            "    Plot training metrics with Australian tourism context.\n",
            "    \n",
            "    Uses seaborn for better aesthetics as per repository guidelines.\n",
            "    \"\"\"\n",
            "    \n",
            "    # Create training metrics DataFrame for seaborn\n",
            "    epochs = range(1, len(history['train_loss']) + 1)\n",
            "    \n",
            "    # Prepare data for seaborn\n",
            "    metrics_data = []\n",
            "    for epoch in epochs:\n",
            "        idx = epoch - 1\n",
            "        metrics_data.extend([\n",
            "            {'Epoch': epoch, 'Metric': 'Loss', 'Dataset': 'Train', 'Value': history['train_loss'][idx]},\n",
            "            {'Epoch': epoch, 'Metric': 'Loss', 'Dataset': 'Validation', 'Value': history['val_loss'][idx]},\n",
            "            {'Epoch': epoch, 'Metric': 'Accuracy', 'Dataset': 'Train', 'Value': history['train_acc'][idx]},\n",
            "            {'Epoch': epoch, 'Metric': 'Accuracy', 'Dataset': 'Validation', 'Value': history['val_acc'][idx]}\n",
            "        ])\n",
            "    \n",
            "    df_metrics = pd.DataFrame(metrics_data)\n",
            "    \n",
            "    # Create subplots with seaborn styling\n",
            "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
            "    \n",
            "    # Loss plot\n",
            "    loss_data = df_metrics[df_metrics['Metric'] == 'Loss']\n",
            "    sns.lineplot(data=loss_data, x='Epoch', y='Value', hue='Dataset', ax=axes[0])\n",
            "    axes[0].set_title('ğŸ‡¦ğŸ‡º Australian Tourism Sentiment Analysis - Training Loss', fontsize=14, fontweight='bold')\n",
            "    axes[0].set_ylabel('Loss')\n",
            "    axes[0].grid(True, alpha=0.3)\n",
            "    \n",
            "    # Accuracy plot\n",
            "    acc_data = df_metrics[df_metrics['Metric'] == 'Accuracy']\n",
            "    sns.lineplot(data=acc_data, x='Epoch', y='Value', hue='Dataset', ax=axes[1])\n",
            "    axes[1].set_title('ğŸ¯ Model Accuracy Progress', fontsize=14, fontweight='bold')\n",
            "    axes[1].set_ylabel('Accuracy')\n",
            "    axes[1].grid(True, alpha=0.3)\n",
            "    \n",
            "    # Learning rate plot\n",
            "    axes[2].plot(epochs, history['learning_rates'], 'o-', color='orange', linewidth=2, markersize=4)\n",
            "    axes[2].set_title('ğŸ“‰ Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
            "    axes[2].set_xlabel('Epoch')\n",
            "    axes[2].set_ylabel('Learning Rate')\n",
            "    axes[2].grid(True, alpha=0.3)\n",
            "    axes[2].set_yscale('log')\n",
            "    \n",
            "    plt.tight_layout()\n",
            "    plt.show()\n",
            "    \n",
            "    # Print final metrics\n",
            "    final_train_acc = history['train_acc'][-1]\n",
            "    final_val_acc = history['val_acc'][-1]\n",
            "    best_val_acc = max(history['val_acc'])\n",
            "    \n",
            "    print(f\"\\nğŸ“Š Final Training Results:\")\n",
            "    print(f\"   Final Training Accuracy: {final_train_acc:.4f} ({final_train_acc*100:.2f}%)\")\n",
            "    print(f\"   Final Validation Accuracy: {final_val_acc:.4f} ({final_val_acc*100:.2f}%)\")\n",
            "    print(f\"   Best Validation Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
            "    \n",
            "    # Model performance assessment\n",
            "    if best_val_acc > 0.8:\n",
            "        print(f\"   ğŸ‰ Excellent performance for Australian tourism sentiment analysis!\")\n",
            "    elif best_val_acc > 0.7:\n",
            "        print(f\"   âœ… Good performance, ready for Australian tourism applications!\")\n",
            "    else:\n",
            "        print(f\"   âš ï¸  Model may need more training or data for optimal performance.\")\n",
            "\n",
            "# Plot training results\n",
            "plot_training_history(training_history)"
        ]
    })
    
    # Cell 13: Model Testing with Australian Examples
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Test the trained model with new Australian tourism examples\n",
            "def test_australian_sentiment_model():\n",
            "    \"\"\"\n",
            "    Test the trained model with fresh Australian tourism examples.\n",
            "    \"\"\"\n",
            "    \n",
            "    # Load the best model\n",
            "    model.load_state_dict(torch.load('best_australian_sentiment_model.pth', map_location=DEVICE))\n",
            "    model.eval()\n",
            "    \n",
            "    # New test examples (not seen during training)\n",
            "    test_examples = [\n",
            "        # English examples\n",
            "        {\n",
            "            'text': \"The Sydney Harbour Bridge climb was absolutely incredible! Spectacular views of the entire city!\",\n",
            "            'language': 'English',\n",
            "            'expected': 'positive'\n",
            "        },\n",
            "        {\n",
            "            'text': \"Melbourne's weather ruined our entire vacation. Constantly raining and cold.\",\n",
            "            'language': 'English', \n",
            "            'expected': 'negative'\n",
            "        },\n",
            "        {\n",
            "            'text': \"Perth has some nice beaches but the city center is quite basic.\",\n",
            "            'language': 'English',\n",
            "            'expected': 'neutral'\n",
            "        },\n",
            "        \n",
            "        # Vietnamese examples\n",
            "        {\n",
            "            'text': \"ThÃ nh phá»‘ Brisbane ráº¥t sáº¡ch sáº½ vÃ  thÃ¢n thiá»‡n vá»›i du khÃ¡ch!\",\n",
            "            'language': 'Vietnamese',\n",
            "            'expected': 'positive'\n",
            "        },\n",
            "        {\n",
            "            'text': \"GiÃ¡ cáº£ á»Ÿ Adelaide quÃ¡ Ä‘áº¯t Ä‘á», khÃ´ng Ä‘Ã¡ng vá»›i cháº¥t lÆ°á»£ng dá»‹ch vá»¥.\",\n",
            "            'language': 'Vietnamese',\n",
            "            'expected': 'negative'\n",
            "        },\n",
            "        {\n",
            "            'text': \"Darwin cÃ³ khÃ­ háº­u áº¥m Ã¡p nhÆ°ng khÃ´ng cÃ³ nhiá»u hoáº¡t Ä‘á»™ng giáº£i trÃ­.\",\n",
            "            'language': 'Vietnamese',\n",
            "            'expected': 'neutral'\n",
            "        }\n",
            "    ]\n",
            "    \n",
            "    print(\"ğŸ§ª Testing Australian Tourism Sentiment Classifier\")\n",
            "    print(\"=\" * 65)\n",
            "    \n",
            "    correct_predictions = 0\n",
            "    total_predictions = len(test_examples)\n",
            "    \n",
            "    for i, example in enumerate(test_examples):\n",
            "        # Encode the text\n",
            "        encoded_text = preprocessor.encode_text(example['text'])\n",
            "        text_tensor = torch.tensor([encoded_text], dtype=torch.long)\n",
            "        \n",
            "        # Get prediction\n",
            "        prediction = model.predict_sentiment(text_tensor, preprocessor)\n",
            "        \n",
            "        # Check if prediction matches expected\n",
            "        is_correct = prediction['sentiment'] == example['expected']\n",
            "        if is_correct:\n",
            "            correct_predictions += 1\n",
            "        \n",
            "        # Display results\n",
            "        status_emoji = \"âœ…\" if is_correct else \"âŒ\"\n",
            "        confidence_bar = \"â–ˆ\" * int(prediction['confidence'] * 10)\n",
            "        \n",
            "        print(f\"\\n{i+1}. {status_emoji} {example['language']} Text Analysis:\")\n",
            "        print(f\"   ğŸ“ Text: {example['text'][:60]}{'...' if len(example['text']) > 60 else ''}\")\n",
            "        print(f\"   ğŸ¯ Expected: {example['expected'].capitalize()}\")\n",
            "        print(f\"   ğŸ¤– Predicted: {prediction['sentiment'].capitalize()} ({prediction['confidence']:.3f})\")\n",
            "        print(f\"   ğŸ“Š Confidence: {confidence_bar} {prediction['confidence']*100:.1f}%\")\n",
            "        \n",
            "        # Show all probabilities\n",
            "        print(f\"   ğŸ“ˆ All Probabilities:\")\n",
            "        for sentiment, prob in prediction['probabilities'].items():\n",
            "            print(f\"      {sentiment.capitalize()}: {prob:.3f}\")\n",
            "    \n",
            "    # Final test accuracy\n",
            "    test_accuracy = correct_predictions / total_predictions\n",
            "    print(f\"\\nğŸ¯ Test Accuracy: {correct_predictions}/{total_predictions} = {test_accuracy:.3f} ({test_accuracy*100:.1f}%)\")\n",
            "    \n",
            "    if test_accuracy >= 0.8:\n",
            "        print(f\"ğŸŒŸ Excellent! Model performs very well on Australian tourism sentiment analysis.\")\n",
            "    elif test_accuracy >= 0.6:\n",
            "        print(f\"ğŸ‘ Good performance! Model shows solid understanding of Australian tourism sentiment.\")\n",
            "    else:\n",
            "        print(f\"âš ï¸  Model needs improvement for reliable Australian tourism sentiment classification.\")\n",
            "    \n",
            "    return test_accuracy\n",
            "\n",
            "# Run model testing\n",
            "test_accuracy = test_australian_sentiment_model()"
        ]
    })
    
    # Cell 14: TensorBoard Visualization Instructions
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# TensorBoard Visualization Instructions\n",
            "print(\"ğŸ“Š TENSORBOARD VISUALIZATION\")\n",
            "print(\"=\" * 60)\n",
            "print(f\"Log directory: {run_logdir}\")\n",
            "print(\"\\nğŸš€ To view TensorBoard:\")\n",
            "\n",
            "if IS_COLAB:\n",
            "    print(\"   In Google Colab:\")\n",
            "    print(\"   1. Run: %load_ext tensorboard\")\n",
            "    print(f\"   2. Run: %tensorboard --logdir {run_logdir}\")\n",
            "    print(\"   3. TensorBoard will appear inline in the notebook\")\n",
            "    \n",
            "    # Auto-load TensorBoard in Colab\n",
            "    try:\n",
            "        %load_ext tensorboard\n",
            "        %tensorboard --logdir {run_logdir}\n",
            "    except:\n",
            "        print(\"   Note: Run the commands above manually in Colab\")\n",
            "        \n",
            "elif IS_KAGGLE:\n",
            "    print(\"   In Kaggle:\")\n",
            "    print(f\"   1. Download logs from: {run_logdir}\")\n",
            "    print(\"   2. Run locally: tensorboard --logdir ./tensorboard_logs\")\n",
            "    print(\"   3. Open http://localhost:6006 in browser\")\n",
            "else:\n",
            "    print(\"   Locally:\")\n",
            "    print(f\"   1. Run: tensorboard --logdir {run_logdir}\")\n",
            "    print(\"   2. Open http://localhost:6006 in browser\")\n",
            "\n",
            "print(\"\\nğŸ“ˆ Available visualizations:\")\n",
            "print(\"   â€¢ Scalars: Loss, accuracy, learning rate over time\")\n",
            "print(\"   â€¢ Histograms: Model parameter distributions\")\n",
            "print(\"   â€¢ Training Progress: Batch-level and epoch-level metrics\")\n",
            "print(\"   â€¢ Memory Usage: GPU memory utilization (if available)\")\n",
            "print(\"   â€¢ Australian Context: Tourism sentiment analysis progress\")\n",
            "\n",
            "print(\"\\nğŸ” Key metrics to examine:\")\n",
            "print(\"   ğŸ“‰ Loss curves: Should decrease over time\")\n",
            "print(\"   ğŸ“ˆ Accuracy curves: Should increase and converge\")\n",
            "print(\"   ğŸ›ï¸  Learning rate: Should adapt based on validation performance\")\n",
            "print(\"   ğŸ“Š Parameter histograms: Should show healthy distributions\")\n",
            "\n",
            "print(\"=\" * 60)"
        ]
    })
    
    # Cell 15: TensorFlow vs PyTorch Comparison Summary
    notebook["cells"].append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## ğŸ”„ TensorFlow vs PyTorch: Key Differences Summary\n",
            "\n",
            "This section summarizes the key differences between TensorFlow and PyTorch approaches demonstrated in this notebook, helping with the transition from TensorFlow to PyTorch for NLP applications.\n",
            "\n",
            "### Model Definition\n",
            "\n",
            "**TensorFlow (Keras)**:\n",
            "```python\n",
            "model = tf.keras.Sequential([\n",
            "    tf.keras.layers.Embedding(vocab_size, embed_dim),\n",
            "    tf.keras.layers.LSTM(128, return_sequences=False),\n",
            "    tf.keras.layers.Dropout(0.3),\n",
            "    tf.keras.layers.Dense(64, activation='relu'),\n",
            "    tf.keras.layers.Dense(3, activation='softmax')\n",
            "])\n",
            "```\n",
            "\n",
            "**PyTorch**:\n",
            "```python\n",
            "class SentimentClassifier(nn.Module):\n",
            "    def __init__(self, vocab_size, embed_dim):\n",
            "        super().__init__()\n",
            "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
            "        self.lstm = nn.LSTM(embed_dim, 128, batch_first=True)\n",
            "        self.dropout = nn.Dropout(0.3)\n",
            "        self.classifier = nn.Linear(128, 3)\n",
            "    \n",
            "    def forward(self, x):\n",
            "        embedded = self.embedding(x)\n",
            "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
            "        output = self.classifier(self.dropout(hidden[-1]))\n",
            "        return output\n",
            "```\n",
            "\n",
            "### Training Loop\n",
            "\n",
            "**TensorFlow**:\n",
            "```python\n",
            "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
            "history = model.fit(train_data, epochs=10, validation_data=val_data)\n",
            "```\n",
            "\n",
            "**PyTorch**:\n",
            "```python\n",
            "criterion = nn.CrossEntropyLoss()\n",
            "optimizer = optim.Adam(model.parameters())\n",
            "\n",
            "for epoch in range(epochs):\n",
            "    for batch in train_loader:\n",
            "        optimizer.zero_grad()  # Clear gradients\n",
            "        outputs = model(batch.data)\n",
            "        loss = criterion(outputs, batch.targets)\n",
            "        loss.backward()  # Compute gradients\n",
            "        optimizer.step()  # Update parameters\n",
            "```\n",
            "\n",
            "### Key Differences Table\n",
            "\n",
            "| Aspect | TensorFlow | PyTorch |\n",
            "|--------|------------|----------|\n",
            "| **Model Definition** | Sequential/Functional API | Object-oriented with `nn.Module` |\n",
            "| **Training** | `model.fit()` (automatic) | Manual training loop |\n",
            "| **Gradients** | Automatic with `GradientTape` | Manual with `loss.backward()` |\n",
            "| **Graph Execution** | Static graph (TF 1.x) / Eager (TF 2.x) | Dynamic graph (always eager) |\n",
            "| **Device Management** | `with tf.device()` context | `.to(device)` method calls |\n",
            "| **Debugging** | More challenging (especially TF 1.x) | Easier with standard Python debugging |\n",
            "| **Flexibility** | High-level API focus | More explicit control |\n",
            "| **Learning Curve** | Steeper initially, easier for simple models | Gentler, more intuitive for Python developers |\n",
            "\n",
            "### Advantages of Each Framework\n",
            "\n",
            "**TensorFlow Advantages**:\n",
            "- ğŸš€ **Production Ready**: TensorFlow Serving, TensorFlow Lite\n",
            "- ğŸ“± **Mobile/Edge**: Better mobile and edge deployment\n",
            "- ğŸ”§ **High-level APIs**: Keras makes simple models very easy\n",
            "- ğŸ“Š **Visualization**: Native TensorBoard integration\n",
            "- ğŸ­ **Ecosystem**: Mature production ecosystem\n",
            "\n",
            "**PyTorch Advantages**:\n",
            "- ğŸ **Pythonic**: More intuitive for Python developers\n",
            "- ğŸ”¬ **Research**: Preferred for research and experimentation\n",
            "- ğŸ› **Debugging**: Easier debugging with standard Python tools\n",
            "- âš¡ **Dynamic**: Dynamic computational graphs\n",
            "- ğŸ¯ **Control**: More explicit control over training process\n",
            "\n",
            "### When to Choose PyTorch\n",
            "\n",
            "Choose PyTorch for Australian NLP projects when:\n",
            "- ğŸ”¬ **Research & Experimentation**: Trying new architectures or techniques\n",
            "- ğŸ“ **Learning**: Understanding deep learning fundamentals\n",
            "- ğŸ¤— **NLP Focus**: Working with Hugging Face transformers ecosystem\n",
            "- ğŸ **Python Preference**: Team prefers explicit, Pythonic code\n",
            "- ğŸ”§ **Custom Models**: Building complex, custom neural architectures\n",
            "\n",
            "### Transition Tips\n",
            "\n",
            "For TensorFlow developers moving to PyTorch:\n",
            "\n",
            "1. **Start with `nn.Module`**: Think of it as a more explicit version of Keras layers\n",
            "2. **Manual Training Loops**: Initially more code, but more control and transparency\n",
            "3. **Device Management**: Explicitly move tensors and models to GPU/CPU\n",
            "4. **Gradient Management**: Always call `optimizer.zero_grad()` before `loss.backward()`\n",
            "5. **Debugging**: Use standard Python debugging tools and `print()` statements\n",
            "6. **Hugging Face**: PyTorch integrates seamlessly with modern NLP libraries\n"
        ]
    })
    
    # Cell 16: Summary and Next Steps
    notebook["cells"].append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## ğŸ‰ Congratulations! Deep Learning NLP with PyTorch Complete\n",
            "\n",
            "You've successfully completed a comprehensive introduction to deep learning for NLP using PyTorch with Australian context examples and English-Vietnamese multilingual support!\n",
            "\n",
            "### ğŸ† What You've Accomplished\n",
            "\n",
            "âœ… **PyTorch Fundamentals**: Mastered tensors, models, and training loops for NLP\n",
            "\n",
            "âœ… **Australian Context**: Built a sentiment classifier for Australian tourism reviews\n",
            "\n",
            "âœ… **Multilingual NLP**: Handled both English and Vietnamese text processing\n",
            "\n",
            "âœ… **Neural Networks**: Implemented LSTM-based architecture from scratch\n",
            "\n",
            "âœ… **Training Pipeline**: Created complete training loop with validation and early stopping\n",
            "\n",
            "âœ… **Visualization**: Integrated TensorBoard for comprehensive training monitoring\n",
            "\n",
            "âœ… **Device Optimization**: Implemented device-aware training (CUDA/MPS/CPU)\n",
            "\n",
            "âœ… **TensorFlow Transition**: Learned key differences and migration strategies\n",
            "\n",
            "### ğŸ“Š Model Performance\n",
            "\n",
            "Your Australian Tourism Sentiment Classifier can now:\n",
            "- ğŸ‡¦ğŸ‡º Analyze sentiment in Australian tourism reviews\n",
            "- ğŸ‡»ğŸ‡³ Process Vietnamese translations and reviews\n",
            "- ğŸ¯ Classify text as positive, neutral, or negative sentiment\n",
            "- ğŸ“± Run efficiently on various devices (GPU, Apple Silicon, CPU)\n",
            "- ğŸ“Š Provide confidence scores and probability distributions\n",
            "\n",
            "### ğŸš€ Next Steps in Your PyTorch NLP Journey\n",
            "\n",
            "Continue your learning with the remaining notebooks in this series:\n",
            "\n",
            "#### 1. ğŸ”¤ Word Embeddings (`word_embeddings_nllp.ipynb`)\n",
            "- **Focus**: Deep dive into word representation techniques\n",
            "- **Australian Context**: Train embeddings on Australian tourism corpus\n",
            "- **Multilingual**: English-Vietnamese word alignment and cross-lingual embeddings\n",
            "- **Techniques**: Word2Vec, GloVe, FastText, and visualization\n",
            "\n",
            "#### 2. ğŸ”„ Sequence Models (`sequence_models_nlp.ipynb`)\n",
            "- **Focus**: Advanced LSTM, GRU, and attention mechanisms\n",
            "- **Australian Context**: Part-of-speech tagging and named entity recognition\n",
            "- **Multilingual**: Sequence-to-sequence translation models\n",
            "- **Techniques**: Bidirectional RNNs, attention, and seq2seq architectures\n",
            "\n",
            "#### 3. ğŸš€ Advanced NLP (`advanced_nlp.ipynb`)\n",
            "- **Focus**: Bi-LSTM CRF and state-of-the-art techniques\n",
            "- **Australian Context**: Named entity recognition for Australian locations\n",
            "- **Integration**: Bridge to Hugging Face transformers\n",
            "- **Techniques**: CRF layers, dynamic computation graphs, and modern architectures\n",
            "\n",
            "### ğŸ› ï¸ Practical Applications\n",
            "\n",
            "Apply your new skills to real Australian NLP projects:\n",
            "\n",
            "- **Tourism Analysis**: Analyze TripAdvisor reviews for Australian destinations\n",
            "- **Social Media**: Monitor sentiment about Australian events and locations\n",
            "- **Customer Service**: Build multilingual chatbots for Australian businesses\n",
            "- **News Analysis**: Process Australian news articles in multiple languages\n",
            "- **E-commerce**: Analyze product reviews for Australian retailers\n",
            "\n",
            "### ğŸ“š Additional Resources\n",
            "\n",
            "Expand your PyTorch NLP knowledge:\n",
            "\n",
            "- ğŸ”— [PyTorch NLP Tutorials](https://pytorch.org/tutorials/beginner/nlp/)\n",
            "- ğŸ¤— [Hugging Face Transformers](https://huggingface.co/transformers/)\n",
            "- ğŸ“– [Natural Language Processing with PyTorch](https://www.oreilly.com/library/view/natural-language-processing/9781491978221/)\n",
            "- ğŸ“ [Stanford CS224N: NLP with Deep Learning](http://web.stanford.edu/class/cs224n/)\n",
            "- ğŸ‡¦ğŸ‡º [Australian Text Analytics Platform](https://www.atap.edu.au/)\n",
            "\n",
            "### ğŸ¤ Community and Support\n",
            "\n",
            "Join the PyTorch and NLP community:\n",
            "\n",
            "- ğŸ’¬ [PyTorch Forums](https://discuss.pytorch.org/)\n",
            "- ğŸ¦ [PyTorch Twitter](https://twitter.com/pytorch)\n",
            "- ğŸ“§ [Hugging Face Discord](https://discord.gg/JfAtkvEtRb)\n",
            "- ğŸ“± [r/MachineLearning](https://www.reddit.com/r/MachineLearning/)\n",
            "\n",
            "### ğŸŒŸ Keep Experimenting!\n",
            "\n",
            "The best way to master PyTorch NLP is through hands-on practice:\n",
            "\n",
            "1. **Modify the Model**: Try different architectures, hyperparameters, and optimizers\n",
            "2. **Expand the Dataset**: Add more Australian tourism data or other domains\n",
            "3. **Add Languages**: Incorporate other languages beyond English and Vietnamese\n",
            "4. **Deploy Models**: Create APIs and web applications with your trained models\n",
            "5. **Contribute**: Share your Australian NLP models and datasets with the community\n",
            "\n",
            "---\n",
            "\n",
            "**ğŸŠ Congratulations on completing your first PyTorch NLP project with Australian flair! You're now ready to tackle real-world natural language processing challenges with confidence. ğŸ‡¦ğŸ‡ºğŸš€**"
        ]
    })
    
    # Save the completed notebook
    with open("deep_learning_nlp.ipynb", "w") as f:
        json.dump(notebook, f, indent=2)
    
    return notebook

if __name__ == "__main__":
    complete_deep_learning_notebook()
    print("âœ… Deep Learning NLP notebook completed with visualization and summary!")