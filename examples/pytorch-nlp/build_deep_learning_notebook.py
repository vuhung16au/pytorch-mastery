#!/usr/bin/env python3
"""
Build the Deep Learning NLP notebook with comprehensive PyTorch examples
"""

import json

def create_deep_learning_nlp_notebook():
    """Create comprehensive deep learning NLP notebook with Australian context."""
    
    # Notebook metadata
    notebook = {
        "cells": [],
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "codemirror_mode": {
                    "name": "ipython",
                    "version": 3
                },
                "file_extension": ".py",
                "mimetype": "text/x-python",
                "name": "python",
                "nbconvert_exporter": "python",
                "pygments_lexer": "ipython3",
                "version": "3.12.3"
            }
        },
        "nbformat": 4,
        "nbformat_minor": 4
    }
    
    # Cell 1: Title and Header
    notebook["cells"].append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "# Deep Learning for NLP with PyTorch üá¶üá∫\n",
            "\n",
            "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/pytorch-mastery/blob/main/examples/pytorch-nlp/deep_learning_nlp.ipynb)\n",
            "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/pytorch-mastery/blob/main/examples/pytorch-nlp/deep_learning_nlp.ipynb)\n",
            "\n",
            "A comprehensive introduction to deep learning concepts for Natural Language Processing using PyTorch, featuring Australian tourism examples and English-Vietnamese multilingual support.\n",
            "\n",
            "## Learning Objectives\n",
            "\n",
            "By the end of this notebook, you will:\n",
            "\n",
            "- üèóÔ∏è **Master PyTorch fundamentals** for NLP applications\n",
            "- üìä **Build neural networks** for Australian text classification\n",
            "- üá¶üá∫ **Process tourism data** with practical Australian examples\n",
            "- üåè **Handle multilingual text** with English-Vietnamese examples\n",
            "- üîÑ **Compare with TensorFlow** to ease the transition\n",
            "- üìà **Implement TensorBoard logging** for training visualization\n",
            "\n",
            "## What You'll Build\n",
            "\n",
            "1. **Australian Tourism Sentiment Classifier** - Analyze reviews of Sydney Opera House, Melbourne coffee, Perth beaches\n",
            "2. **Multilingual Text Processor** - Handle English and Vietnamese tourism content\n",
            "3. **Neural Network from Scratch** - Build and train your first PyTorch NLP model\n",
            "4. **Cross-Platform Solution** - Code that works on Local, Colab, and Kaggle\n",
            "\n",
            "---"
        ]
    })
    
    # Cell 2: Environment Detection and Setup
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Environment Detection and Setup\n",
            "import sys\n",
            "import subprocess\n",
            "import os\n",
            "import time\n",
            "\n",
            "# Detect the runtime environment\n",
            "IS_COLAB = \"google.colab\" in sys.modules\n",
            "IS_KAGGLE = \"kaggle_secrets\" in sys.modules or \"kaggle\" in os.environ.get('KAGGLE_URL_BASE', '')\n",
            "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
            "\n",
            "print(f\"Environment detected:\")\n",
            "print(f\"  - Local: {IS_LOCAL}\")\n",
            "print(f\"  - Google Colab: {IS_COLAB}\")\n",
            "print(f\"  - Kaggle: {IS_KAGGLE}\")\n",
            "\n",
            "# Platform-specific system setup\n",
            "if IS_COLAB:\n",
            "    print(\"\\nSetting up Google Colab environment...\")\n",
            "    !apt update -qq\n",
            "    !apt install -y -qq software-properties-common\n",
            "elif IS_KAGGLE:\n",
            "    print(\"\\nSetting up Kaggle environment...\")\n",
            "    # Kaggle usually has most packages pre-installed\n",
            "else:\n",
            "    print(\"\\nSetting up local environment...\")"
        ]
    })
    
    # Cell 3: Package Installation
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Install required packages for this notebook\n",
            "required_packages = [\n",
            "    \"torch\",\n",
            "    \"transformers\",\n",
            "    \"datasets\", \n",
            "    \"tokenizers\",\n",
            "    \"pandas\",\n",
            "    \"seaborn\",\n",
            "    \"matplotlib\",\n",
            "    \"tensorboard\",\n",
            "    \"scikit-learn\"\n",
            "]\n",
            "\n",
            "print(\"Installing required packages...\")\n",
            "for package in required_packages:\n",
            "    if IS_COLAB or IS_KAGGLE:\n",
            "        !pip install -q {package}\n",
            "    else:\n",
            "        try:\n",
            "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
            "                          capture_output=True, check=True)\n",
            "        except subprocess.CalledProcessError:\n",
            "            print(f\"Note: {package} installation skipped (likely already installed)\")\n",
            "    print(f\"‚úì {package}\")\n",
            "\n",
            "print(\"\\nüì¶ Package installation completed!\")"
        ]
    })
    
    # Cell 4: Import Libraries and Device Detection
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Import essential libraries\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import torch.optim as optim\n",
            "import torch.nn.functional as F\n",
            "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
            "from torch.utils.tensorboard import SummaryWriter\n",
            "\n",
            "# Data handling and visualization\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as sns\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.preprocessing import LabelEncoder\n",
            "from sklearn.metrics import classification_report, confusion_matrix\n",
            "\n",
            "# Text processing\n",
            "import re\n",
            "import string\n",
            "from collections import Counter, defaultdict\n",
            "import random\n",
            "\n",
            "# Set style for better notebook aesthetics\n",
            "sns.set_style(\"whitegrid\")\n",
            "sns.set_palette(\"husl\")\n",
            "plt.rcParams['figure.figsize'] = (12, 8)\n",
            "\n",
            "# Set random seeds for reproducibility\n",
            "torch.manual_seed(42)\n",
            "np.random.seed(42)\n",
            "random.seed(42)\n",
            "\n",
            "print(f\"‚úÖ PyTorch {torch.__version__} ready!\")\n",
            "print(f\"üìä Libraries imported successfully!\")"
        ]
    })
    
    # Cell 5: Intelligent Device Detection
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "import platform\n",
            "\n",
            "def detect_device():\n",
            "    \"\"\"\n",
            "    Detect the best available PyTorch device with comprehensive hardware support.\n",
            "    \n",
            "    Priority order:\n",
            "    1. CUDA (NVIDIA GPUs) - Best performance for deep learning\n",
            "    2. MPS (Apple Silicon) - Optimized for M1/M2/M3 Macs  \n",
            "    3. CPU (Universal) - Always available fallback\n",
            "    \n",
            "    Returns:\n",
            "        torch.device: The optimal device for PyTorch operations\n",
            "        str: Human-readable device description for logging\n",
            "    \"\"\"\n",
            "    # Check for CUDA (NVIDIA GPU)\n",
            "    if torch.cuda.is_available():\n",
            "        device = torch.device(\"cuda\")\n",
            "        gpu_name = torch.cuda.get_device_name(0)\n",
            "        device_info = f\"CUDA GPU: {gpu_name}\"\n",
            "        \n",
            "        # Additional CUDA info for optimization\n",
            "        cuda_version = torch.version.cuda\n",
            "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
            "        \n",
            "        print(f\"üöÄ Using CUDA acceleration\")\n",
            "        print(f\"   GPU: {gpu_name}\")\n",
            "        print(f\"   CUDA Version: {cuda_version}\")\n",
            "        print(f\"   GPU Memory: {gpu_memory:.1f} GB\")\n",
            "        \n",
            "        return device, device_info\n",
            "    \n",
            "    # Check for MPS (Apple Silicon)\n",
            "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
            "        device = torch.device(\"mps\")\n",
            "        device_info = \"Apple Silicon MPS\"\n",
            "        \n",
            "        # Get system info for Apple Silicon\n",
            "        system_info = platform.uname()\n",
            "        \n",
            "        print(f\"üçé Using Apple Silicon MPS acceleration\")\n",
            "        print(f\"   System: {system_info.system} {system_info.release}\")\n",
            "        print(f\"   Machine: {system_info.machine}\")\n",
            "        print(f\"   Processor: {system_info.processor}\")\n",
            "        \n",
            "        return device, device_info\n",
            "    \n",
            "    # Fallback to CPU\n",
            "    else:\n",
            "        device = torch.device(\"cpu\")\n",
            "        device_info = \"CPU (No GPU acceleration available)\"\n",
            "        \n",
            "        # Get CPU info for optimization guidance\n",
            "        cpu_count = torch.get_num_threads()\n",
            "        system_info = platform.uname()\n",
            "        \n",
            "        print(f\"üíª Using CPU (no GPU acceleration detected)\")\n",
            "        print(f\"   Processor: {system_info.processor}\")\n",
            "        print(f\"   PyTorch Threads: {cpu_count}\")\n",
            "        print(f\"   System: {system_info.system} {system_info.release}\")\n",
            "        \n",
            "        # Provide optimization suggestions for CPU-only setups\n",
            "        print(f\"\\nüí° CPU Optimization Tips:\")\n",
            "        print(f\"   ‚Ä¢ Reduce batch size to prevent memory issues\")\n",
            "        print(f\"   ‚Ä¢ Consider using smaller models for faster training\")\n",
            "        print(f\"   ‚Ä¢ Enable PyTorch optimizations: torch.set_num_threads({cpu_count})\")\n",
            "        \n",
            "        return device, device_info\n",
            "\n",
            "# Usage in all PyTorch notebooks\n",
            "device, device_info = detect_device()\n",
            "print(f\"\\n‚úÖ PyTorch device selected: {device}\")\n",
            "print(f\"üìä Device info: {device_info}\")\n",
            "\n",
            "# Set global device for the notebook\n",
            "DEVICE = device"
        ]
    })
    
    # Cell 6: Australian Tourism Dataset Creation
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Australian Tourism Dataset with English-Vietnamese Examples\n",
            "print(\"üá¶üá∫ Creating Australian Tourism Dataset with Multilingual Support\")\n",
            "print(\"=\" * 70)\n",
            "\n",
            "# Australian tourism reviews with English-Vietnamese pairs\n",
            "australian_tourism_data = {\n",
            "    'english': [\n",
            "        # Positive reviews\n",
            "        \"The Sydney Opera House is absolutely breathtaking! Worth every dollar.\",\n",
            "        \"Melbourne's coffee culture is world-class. Amazing baristas everywhere!\",\n",
            "        \"Uluru at sunset was a spiritual experience I'll never forget.\",\n",
            "        \"Perth beaches are perfect for families with young children.\",\n",
            "        \"Brisbane's South Bank is a fantastic place for weekend activities.\",\n",
            "        \"Adelaide's food scene exceeded all my expectations. Incredible wines!\",\n",
            "        \"Darwin's tropical climate and laid-back vibe are absolutely perfect.\",\n",
            "        \"Hobart's MONA museum is mind-blowing and thought-provoking.\",\n",
            "        \"Canberra's national galleries showcase Australia's rich cultural heritage.\",\n",
            "        \"The Great Barrier Reef snorkeling was the highlight of my trip.\",\n",
            "        \"Blue Mountains scenic railway offers spectacular mountain views.\",\n",
            "        \"Gold Coast theme parks provide endless fun for the whole family.\",\n",
            "        \n",
            "        # Neutral reviews\n",
            "        \"Sydney Harbour Bridge climb was okay, but quite expensive for what it is.\",\n",
            "        \"Melbourne weather is unpredictable, pack clothes for all seasons.\",\n",
            "        \"Perth is very isolated but has decent shopping and dining options.\",\n",
            "        \"Brisbane can be quite humid during summer months, plan accordingly.\",\n",
            "        \"Adelaide is smaller than expected but has its own unique charm.\",\n",
            "        \"Darwin has limited attractions but the markets are interesting.\",\n",
            "        \n",
            "        # Negative reviews\n",
            "        \"Sydney accommodation prices are absolutely outrageous and unreasonable.\",\n",
            "        \"Melbourne trams are constantly delayed and overcrowded during peak hours.\",\n",
            "        \"Perth nightlife is disappointing and closes way too early.\",\n",
            "        \"Brisbane's public transport system needs significant improvements.\",\n",
            "        \"Adelaide becomes very quiet after 6 PM, limited evening entertainment.\",\n",
            "        \"Darwin is extremely expensive for basic necessities and groceries.\"\n",
            "    ],\n",
            "    \n",
            "    'vietnamese': [\n",
            "        # Positive reviews (Vietnamese)\n",
            "        \"Nh√† h√°t Opera Sydney th·∫≠t ngo·∫°n m·ª•c! X·ª©ng ƒë√°ng t·ª´ng ƒë·ªìng ti·ªÅn.\",\n",
            "        \"VƒÉn h√≥a c√† ph√™ Melbourne ƒë·∫≥ng c·∫•p th·∫ø gi·ªõi. Barista tuy·ªát v·ªùi ·ªü kh·∫Øp n∆°i!\",\n",
            "        \"Uluru l√∫c ho√†ng h√¥n l√† tr·∫£i nghi·ªám t√¢m linh t√¥i s·∫Ω kh√¥ng bao gi·ªù qu√™n.\",\n",
            "        \"B√£i bi·ªÉn Perth ho√†n h·∫£o cho c√°c gia ƒë√¨nh c√≥ con nh·ªè.\",\n",
            "        \"South Bank Brisbane l√† n∆°i tuy·ªát v·ªùi cho ho·∫°t ƒë·ªông cu·ªëi tu·∫ßn.\",\n",
            "        \"·∫®m th·ª±c Adelaide v∆∞·ª£t xa mong ƒë·ª£i c·ªßa t√¥i. R∆∞·ª£u vang tuy·ªát v·ªùi!\",\n",
            "        \"Kh√≠ h·∫≠u nhi·ªát ƒë·ªõi v√† kh√¥ng kh√≠ th∆∞ gi√£n ·ªü Darwin th·∫≠t ho√†n h·∫£o.\",\n",
            "        \"B·∫£o t√†ng MONA Hobart th·∫≠t ·∫•n t∆∞·ª£ng v√† k√≠ch th√≠ch t∆∞ duy.\",\n",
            "        \"C√°c ph√≤ng tr∆∞ng b√†y qu·ªëc gia Canberra th·ªÉ hi·ªán di s·∫£n vƒÉn h√≥a √öc.\",\n",
            "        \"L·∫∑n ng·∫Øm san h√¥ Great Barrier Reef l√† ƒëi·ªÉm nh·∫•n chuy·∫øn ƒëi.\",\n",
            "        \"ƒê∆∞·ªùng s·∫Øt Blue Mountains mang ƒë·∫øn t·∫ßm nh√¨n n√∫i non tuy·ªát ƒë·∫πp.\",\n",
            "        \"C√¥ng vi√™n gi·∫£i tr√≠ Gold Coast mang l·∫°i ni·ªÅm vui cho c·∫£ gia ƒë√¨nh.\",\n",
            "        \n",
            "        # Neutral reviews (Vietnamese)\n",
            "        \"Leo c·∫ßu Sydney Harbour Bridge c≈©ng ƒë∆∞·ª£c, nh∆∞ng kh√° ƒë·∫Øt so v·ªõi gi√° tr·ªã.\",\n",
            "        \"Th·ªùi ti·∫øt Melbourne kh√≥ ƒëo√°n, h√£y mang qu·∫ßn √°o cho m·ªçi m√πa.\",\n",
            "        \"Perth r·∫•t bi·ªát l·∫≠p nh∆∞ng c√≥ l·ª±a ch·ªçn mua s·∫Øm v√† ƒÉn u·ªëng t·ªët.\",\n",
            "        \"Brisbane c√≥ th·ªÉ kh√° ·∫©m ∆∞·ªõt v√†o m√πa h√®, h√£y l√™n k·∫ø ho·∫°ch ph√π h·ª£p.\",\n",
            "        \"Adelaide nh·ªè h∆°n mong ƒë·ª£i nh∆∞ng c√≥ n√©t quy·∫øn r≈© ri√™ng.\",\n",
            "        \"Darwin c√≥ √≠t ƒëi·ªÉm tham quan nh∆∞ng c√°c khu ch·ª£ kh√° th√∫ v·ªã.\",\n",
            "        \n",
            "        # Negative reviews (Vietnamese)\n",
            "        \"Gi√° ch·ªó ·ªü Sydney th·∫≠t phi l√Ω v√† qu√° ƒë·∫Øt ƒë·ªè.\",\n",
            "        \"T√†u ƒëi·ªán Melbourne li√™n t·ª•c ch·∫≠m tr·ªÖ v√† qu√° t·∫£i gi·ªù cao ƒëi·ªÉm.\",\n",
            "        \"Cu·ªôc s·ªëng v·ªÅ ƒë√™m Perth th·∫•t v·ªçng v√† ƒë√≥ng c·ª≠a qu√° s·ªõm.\",\n",
            "        \"H·ªá th·ªëng giao th√¥ng c√¥ng c·ªông Brisbane c·∫ßn c·∫£i thi·ªán ƒë√°ng k·ªÉ.\",\n",
            "        \"Adelaide tr·ªü n√™n r·∫•t y√™n tƒ©nh sau 6 gi·ªù chi·ªÅu, √≠t gi·∫£i tr√≠ bu·ªïi t·ªëi.\",\n",
            "        \"Darwin c·ª±c k·ª≥ ƒë·∫Øt ƒë·ªè cho nh·ªØng nhu c·∫ßu c∆° b·∫£n v√† th·ª±c ph·∫©m.\"\n",
            "    ],\n",
            "    \n",
            "    'labels': [\n",
            "        # Positive labels (1)\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        # Neutral labels (0) \n",
            "        0, 0, 0, 0, 0, 0,\n",
            "        # Negative labels (-1)\n",
            "        -1, -1, -1, -1, -1, -1\n",
            "    ]\n",
            "}\n",
            "\n",
            "# Create DataFrame for easy manipulation\n",
            "tourism_df = pd.DataFrame({\n",
            "    'english_text': australian_tourism_data['english'],\n",
            "    'vietnamese_text': australian_tourism_data['vietnamese'], \n",
            "    'sentiment': australian_tourism_data['labels']\n",
            "})\n",
            "\n",
            "# Add sentiment labels\n",
            "sentiment_map = {1: 'positive', 0: 'neutral', -1: 'negative'}\n",
            "tourism_df['sentiment_label'] = tourism_df['sentiment'].map(sentiment_map)\n",
            "\n",
            "print(f\"üìä Dataset Statistics:\")\n",
            "print(f\"   Total reviews: {len(tourism_df)}\")\n",
            "print(f\"   Languages: English, Vietnamese\")\n",
            "print(f\"   Sentiment distribution:\")\n",
            "print(tourism_df['sentiment_label'].value_counts().to_string())\n",
            "\n",
            "# Display sample data\n",
            "print(f\"\\nüåü Sample Australian Tourism Reviews:\")\n",
            "for i in range(3):\n",
            "    row = tourism_df.iloc[i]\n",
            "    print(f\"\\n{i+1}. Sentiment: {row['sentiment_label'].upper()}\")\n",
            "    print(f\"   üá¨üáß English: {row['english_text']}\")\n",
            "    print(f\"   üáªüá≥ Vietnamese: {row['vietnamese_text']}\")\n",
            "\n",
            "tourism_df.head()"
        ]
    })
    
    # Cell 7: Text Preprocessing Pipeline
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "class AustralianTextPreprocessor:\n",
            "    \"\"\"\n",
            "    Text preprocessing pipeline for Australian tourism content with multilingual support.\n",
            "    \n",
            "    Handles both English and Vietnamese text with Australian-specific considerations:\n",
            "    - Australian English spelling and terminology\n",
            "    - Vietnamese diacritics and tone marks\n",
            "    - Tourism-specific vocabulary (Sydney, Melbourne, etc.)\n",
            "    \n",
            "    TensorFlow equivalent:\n",
            "        tf.keras.preprocessing.text.Tokenizer with custom filters\n",
            "    \"\"\"\n",
            "    \n",
            "    def __init__(self, max_vocab_size=10000, max_length=128):\n",
            "        self.max_vocab_size = max_vocab_size\n",
            "        self.max_length = max_length\n",
            "        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1, '<START>': 2, '<END>': 3}\n",
            "        self.idx_to_word = {0: '<PAD>', 1: '<UNK>', 2: '<START>', 3: '<END>'}\n",
            "        self.vocab_size = 4\n",
            "        \n",
            "        # Australian cities and landmarks for special handling\n",
            "        self.australian_entities = {\n",
            "            'cities': ['sydney', 'melbourne', 'brisbane', 'perth', 'adelaide', 'darwin', 'hobart', 'canberra'],\n",
            "            'landmarks': ['opera', 'harbour', 'bridge', 'uluru', 'reef', 'mountains'],\n",
            "            'states': ['nsw', 'vic', 'qld', 'wa', 'sa', 'nt', 'tas', 'act']\n",
            "        }\n",
            "    \n",
            "    def clean_text(self, text):\n",
            "        \"\"\"\n",
            "        Clean and normalize text while preserving Australian and Vietnamese characteristics.\n",
            "        \"\"\"\n",
            "        if not isinstance(text, str):\n",
            "            return \"\"\n",
            "        \n",
            "        # Convert to lowercase\n",
            "        text = text.lower()\n",
            "        \n",
            "        # Handle Australian-specific contractions\n",
            "        australian_contractions = {\n",
            "            \"can't\": \"cannot\",\n",
            "            \"won't\": \"will not\",\n",
            "            \"i'm\": \"i am\",\n",
            "            \"you're\": \"you are\",\n",
            "            \"it's\": \"it is\",\n",
            "            \"that's\": \"that is\",\n",
            "            \"there's\": \"there is\",\n",
            "            \"we're\": \"we are\",\n",
            "            \"they're\": \"they are\"\n",
            "        }\n",
            "        \n",
            "        for contraction, expansion in australian_contractions.items():\n",
            "            text = text.replace(contraction, expansion)\n",
            "        \n",
            "        # Remove excessive punctuation but keep sentence structure\n",
            "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
            "        \n",
            "        # Handle multiple spaces\n",
            "        text = re.sub(r'\\s+', ' ', text)\n",
            "        \n",
            "        return text.strip()\n",
            "    \n",
            "    def build_vocabulary(self, texts):\n",
            "        \"\"\"\n",
            "        Build vocabulary from Australian tourism texts.\n",
            "        \"\"\"\n",
            "        word_freq = Counter()\n",
            "        \n",
            "        print(\"üî§ Building vocabulary from Australian tourism corpus...\")\n",
            "        \n",
            "        for text in texts:\n",
            "            cleaned = self.clean_text(text)\n",
            "            words = cleaned.split()\n",
            "            word_freq.update(words)\n",
            "        \n",
            "        # Get most common words, excluding special tokens\n",
            "        most_common = word_freq.most_common(self.max_vocab_size - 4)\n",
            "        \n",
            "        # Add words to vocabulary\n",
            "        for word, freq in most_common:\n",
            "            if word not in self.word_to_idx:\n",
            "                self.word_to_idx[word] = self.vocab_size\n",
            "                self.idx_to_word[self.vocab_size] = word\n",
            "                self.vocab_size += 1\n",
            "        \n",
            "        print(f\"   üìä Vocabulary size: {self.vocab_size}\")\n",
            "        print(f\"   üá¶üá∫ Australian entities found: {len([w for w in self.word_to_idx if any(ent in w for entities in self.australian_entities.values() for ent in entities)])}\")\n",
            "        \n",
            "        # Show most common Australian tourism words\n",
            "        tourism_words = [word for word, freq in most_common[:20]]\n",
            "        print(f\"   üåü Top tourism words: {', '.join(tourism_words[:10])}\")\n",
            "    \n",
            "    def encode_text(self, text):\n",
            "        \"\"\"\n",
            "        Convert text to sequence of token IDs.\n",
            "        \"\"\"\n",
            "        cleaned = self.clean_text(text)\n",
            "        words = cleaned.split()\n",
            "        \n",
            "        # Convert words to indices\n",
            "        indices = [self.word_to_idx.get(word, self.word_to_idx['<UNK>']) for word in words]\n",
            "        \n",
            "        # Add start and end tokens\n",
            "        indices = [self.word_to_idx['<START>']] + indices + [self.word_to_idx['<END>']]\n",
            "        \n",
            "        # Pad or truncate to max_length\n",
            "        if len(indices) > self.max_length:\n",
            "            indices = indices[:self.max_length]\n",
            "        else:\n",
            "            indices.extend([self.word_to_idx['<PAD>']] * (self.max_length - len(indices)))\n",
            "        \n",
            "        return indices\n",
            "    \n",
            "    def decode_text(self, indices):\n",
            "        \"\"\"\n",
            "        Convert sequence of token IDs back to text.\n",
            "        \"\"\"\n",
            "        words = []\n",
            "        for idx in indices:\n",
            "            word = self.idx_to_word.get(idx, '<UNK>')\n",
            "            if word not in ['<PAD>', '<START>', '<END>']:\n",
            "                words.append(word)\n",
            "        return ' '.join(words)\n",
            "\n",
            "# Initialize preprocessor\n",
            "preprocessor = AustralianTextPreprocessor(max_vocab_size=5000, max_length=64)\n",
            "\n",
            "# Build vocabulary from both English and Vietnamese texts\n",
            "all_texts = tourism_df['english_text'].tolist() + tourism_df['vietnamese_text'].tolist()\n",
            "preprocessor.build_vocabulary(all_texts)\n",
            "\n",
            "print(f\"\\nüîß Text Preprocessing Pipeline Ready!\")\n",
            "print(f\"   Max vocabulary size: {preprocessor.max_vocab_size}\")\n",
            "print(f\"   Max sequence length: {preprocessor.max_length}\")\n",
            "print(f\"   Actual vocabulary size: {preprocessor.vocab_size}\")"
        ]
    })
    
    # Cell 8: Data Preparation and Encoding
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Prepare training data with both English and Vietnamese texts\n",
            "print(\"üìä Preparing Australian Tourism Training Data\")\n",
            "print(\"=\" * 50)\n",
            "\n",
            "# Encode texts\n",
            "english_encoded = [preprocessor.encode_text(text) for text in tourism_df['english_text']]\n",
            "vietnamese_encoded = [preprocessor.encode_text(text) for text in tourism_df['vietnamese_text']]\n",
            "\n",
            "# Combine English and Vietnamese data for multilingual training\n",
            "all_encoded_texts = english_encoded + vietnamese_encoded\n",
            "all_labels = tourism_df['sentiment'].tolist() * 2  # Duplicate labels for both languages\n",
            "\n",
            "# Convert to tensors\n",
            "X = torch.tensor(all_encoded_texts, dtype=torch.long)\n",
            "y = torch.tensor(all_labels, dtype=torch.long)\n",
            "\n",
            "# Adjust labels for classification (0, 1, 2 instead of -1, 0, 1)\n",
            "y = y + 1  # Now: 0=negative, 1=neutral, 2=positive\n",
            "\n",
            "print(f\"üìà Dataset prepared:\")\n",
            "print(f\"   Input shape: {X.shape}\")\n",
            "print(f\"   Labels shape: {y.shape}\")\n",
            "print(f\"   Vocabulary size: {preprocessor.vocab_size}\")\n",
            "print(f\"   Sequence length: {preprocessor.max_length}\")\n",
            "\n",
            "# Split data for training and validation\n",
            "X_train, X_val, y_train, y_val = train_test_split(\n",
            "    X, y, test_size=0.2, random_state=42, stratify=y\n",
            ")\n",
            "\n",
            "print(f\"\\nüîÑ Train/Validation Split:\")\n",
            "print(f\"   Training samples: {len(X_train)}\")\n",
            "print(f\"   Validation samples: {len(X_val)}\")\n",
            "\n",
            "# Create data loaders for efficient training\n",
            "train_dataset = TensorDataset(X_train, y_train)\n",
            "val_dataset = TensorDataset(X_val, y_val)\n",
            "\n",
            "batch_size = 16 if DEVICE.type == 'cpu' else 32  # Adjust batch size based on device\n",
            "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
            "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
            "\n",
            "print(f\"\\n‚ö° Data loaders created:\")\n",
            "print(f\"   Batch size: {batch_size} (optimized for {DEVICE.type.upper()})\")\n",
            "print(f\"   Training batches: {len(train_loader)}\")\n",
            "print(f\"   Validation batches: {len(val_loader)}\")\n",
            "\n",
            "# Show example of encoded text\n",
            "sample_idx = 0\n",
            "sample_text = tourism_df['english_text'].iloc[sample_idx]\n",
            "sample_encoded = english_encoded[sample_idx]\n",
            "sample_decoded = preprocessor.decode_text(sample_encoded)\n",
            "\n",
            "print(f\"\\nüîç Encoding Example:\")\n",
            "print(f\"   Original: {sample_text}\")\n",
            "print(f\"   Encoded: {sample_encoded[:10]}... (showing first 10 tokens)\")\n",
            "print(f\"   Decoded: {sample_decoded}\")"
        ]
    })
    
    # Continue building the notebook...
    # I'll create the remaining cells in the next part to keep the response manageable
    
    return notebook

if __name__ == "__main__":
    notebook = create_deep_learning_nlp_notebook()
    with open("deep_learning_nlp.ipynb", "w") as f:
        json.dump(notebook, f, indent=2)
    print("‚úÖ Deep Learning NLP notebook created successfully!")