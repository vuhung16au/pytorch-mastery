{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Interpreting Text Models with Captum \ud83d\udd0d\ud83c\udde6\ud83c\uddfa\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/pytorch-mastery/blob/main/examples/pytorch-nlp/interpreting_text_models.ipynb)\n",
        "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/pytorch-mastery/blob/main/examples/pytorch-nlp/interpreting_text_models.ipynb)\n",
        "\n",
        "Learn how to interpret and understand PyTorch text models using **Captum**, PyTorch's model interpretability library. This notebook demonstrates various interpretation techniques using a pre-trained IMDB sentiment analysis model with Australian tourism examples and English-Vietnamese multilingual content.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "- \ud83d\udd0d **Master Captum fundamentals** for text model interpretation\n",
        "- \ud83d\udcca **Use Integrated Gradients** to understand feature importance\n",
        "- \ud83c\udfaf **Apply Layer Conductance** for layer-wise attribution analysis\n",
        "- \ud83d\udd0d **Implement Input X Gradient** for simple gradient-based interpretation\n",
        "- \ud83c\udfa8 **Visualize attributions** with Australian tourism sentiment examples\n",
        "- \ud83c\udf0f **Handle multilingual interpretation** with English-Vietnamese text\n",
        "- \ud83d\udcc8 **Compare interpretation methods** for comprehensive understanding\n",
        "\n",
        "## What You'll Build\n",
        "\n",
        "1. **Pre-trained Model Loading** - Use IMDB CNN model from Captum tutorials\n",
        "2. **Australian Sentiment Analyzer** - Interpret sentiment predictions for tourism reviews\n",
        "3. **Feature Attribution Visualizer** - Show which words drive model predictions\n",
        "4. **Multilingual Interpreter** - Compare interpretations across English-Vietnamese text\n",
        "5. **Interactive Attribution Dashboard** - Explore model behavior interactively\n",
        "\n",
        "## Australian Context Examples\n",
        "\n",
        "We'll interpret model predictions for:\n",
        "- \ud83c\udfdb\ufe0f Sydney Opera House reviews and tourism experiences\n",
        "- \u2615 Melbourne coffee culture sentiment analysis\n",
        "- \ud83c\udfd6\ufe0f Gold Coast beach and tourism feedback\n",
        "- \ud83d\udde3\ufe0f English-Vietnamese tourism review translations\n",
        "\n",
        "**Resources**: [Captum Documentation](https://captum.ai) | [IMDB Model](https://github.com/pytorch/captum/raw/refs/heads/master/tutorials/models/imdb-model-cnn-large.pt)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment Detection and Setup\n",
        "import sys\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Detect the runtime environment\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules or \"kaggle\" in os.environ.get('KAGGLE_URL_BASE', '')\n",
        "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
        "\n",
        "print(f\"Environment detected:\")\n",
        "print(f\"  - Local: {IS_LOCAL}\")\n",
        "print(f\"  - Google Colab: {IS_COLAB}\")\n",
        "print(f\"  - Kaggle: {IS_KAGGLE}\")\n",
        "\n",
        "# Platform-specific system setup\n",
        "if IS_COLAB:\n",
        "    print(\"\\nSetting up Google Colab environment...\")\n",
        "    !apt update -qq\n",
        "    !apt install -y -qq software-properties-common\n",
        "elif IS_KAGGLE:\n",
        "    print(\"\\nSetting up Kaggle environment...\")\n",
        "    # Kaggle usually has most packages pre-installed\n",
        "else:\n",
        "    print(\"\\nSetting up local environment...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages for text model interpretation\n",
        "required_packages = [\n",
        "    \"torch\",\n",
        "    \"captum\",           # Model interpretability library\n",
        "    \"transformers\",\n",
        "    \"datasets\", \n",
        "    \"tokenizers\",\n",
        "    \"pandas\",\n",
        "    \"seaborn\",\n",
        "    \"matplotlib\",\n",
        "    \"scikit-learn\",\n",
        "    \"tensorboard\",\n",
        "    \"numpy\",\n",
        "    \"wordcloud\",        # For text visualization\n",
        "    \"plotly\"            # Interactive visualizations\n",
        "]\n",
        "\n",
        "print(\"Installing required packages for text model interpretation...\")\n",
        "for package in required_packages:\n",
        "    if IS_COLAB or IS_KAGGLE:\n",
        "        !pip install -q {package}\n",
        "    else:\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
        "                      capture_output=True)\n",
        "    print(f\"\u2713 {package}\")\n",
        "\n",
        "print(\"\\n\ud83c\udf89 Package installation completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core PyTorch and Captum imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Captum for model interpretation\n",
        "from captum.attr import (\n",
        "    IntegratedGradients,\n",
        "    LayerConductance,\n",
        "    InputXGradient,\n",
        "    GradientShap,\n",
        "    DeepLift,\n",
        "    Saliency\n",
        ")\n",
        "from captum.attr import visualization as viz\n",
        "\n",
        "# Data processing and visualization\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Text processing\n",
        "import re\n",
        "import string\n",
        "from collections import Counter, defaultdict\n",
        "import random\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Utility imports\n",
        "import urllib.request\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better notebook aesthetics\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(f\"\u2705 PyTorch {torch.__version__} ready!\")\n",
        "print(f\"\ud83d\udd0d Captum library imported successfully!\")\n",
        "print(f\"\ud83d\udcca Visualization libraries ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import platform\n",
        "\n",
        "def detect_device():\n",
        "    \"\"\"\n",
        "    Detect the best available PyTorch device with comprehensive hardware support.\n",
        "    \n",
        "    Priority order:\n",
        "    1. CUDA (NVIDIA GPUs) - Best performance for deep learning\n",
        "    2. MPS (Apple Silicon) - Optimized for M1/M2/M3 Macs  \n",
        "    3. CPU (Universal) - Always available fallback\n",
        "    \n",
        "    Returns:\n",
        "        torch.device: The optimal device for PyTorch operations\n",
        "        str: Human-readable device description for logging\n",
        "    \"\"\"\n",
        "    # Check for CUDA (NVIDIA GPU)\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        device_info = f\"CUDA GPU: {gpu_name}\"\n",
        "        \n",
        "        # Additional CUDA info for optimization\n",
        "        cuda_version = torch.version.cuda\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "        \n",
        "        print(f\"\ud83d\ude80 Using CUDA acceleration\")\n",
        "        print(f\"   GPU: {gpu_name}\")\n",
        "        print(f\"   CUDA Version: {cuda_version}\")\n",
        "        print(f\"   GPU Memory: {gpu_memory:.1f} GB\")\n",
        "        \n",
        "        return device, device_info\n",
        "    \n",
        "    # Check for MPS (Apple Silicon)\n",
        "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "        device_info = \"Apple Silicon MPS\"\n",
        "        \n",
        "        # Get system info for Apple Silicon\n",
        "        system_info = platform.uname()\n",
        "        \n",
        "        print(f\"\ud83c\udf4e Using Apple Silicon MPS acceleration\")\n",
        "        print(f\"   System: {system_info.system} {system_info.release}\")\n",
        "        print(f\"   Machine: {system_info.machine}\")\n",
        "        print(f\"   Processor: {system_info.processor}\")\n",
        "        \n",
        "        return device, device_info\n",
        "    \n",
        "    # Fallback to CPU\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        device_info = \"CPU (No GPU acceleration available)\"\n",
        "        \n",
        "        # Get CPU info for optimization guidance\n",
        "        cpu_count = torch.get_num_threads()\n",
        "        system_info = platform.uname()\n",
        "        \n",
        "        print(f\"\ud83d\udcbb Using CPU (no GPU acceleration detected)\")\n",
        "        print(f\"   Processor: {system_info.processor}\")\n",
        "        print(f\"   PyTorch Threads: {cpu_count}\")\n",
        "        print(f\"   System: {system_info.system} {system_info.release}\")\n",
        "        \n",
        "        return device, device_info\n",
        "\n",
        "# Detect and set global device\n",
        "DEVICE, DEVICE_INFO = detect_device()\n",
        "print(f\"\\n\u2705 Device selected: {DEVICE}\")\n",
        "print(f\"\ud83d\udcca Device info: {DEVICE_INFO}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download pre-trained IMDB CNN model from Captum tutorials\n",
        "print(\"\ud83d\udce5 Downloading pre-trained IMDB sentiment analysis model...\")\n",
        "print(\"Model: CNN for IMDB sentiment classification (~20MB)\")\n",
        "print(\"Source: https://github.com/pytorch/captum/raw/refs/heads/master/tutorials/models/imdb-model-cnn-large.pt\")\n",
        "\n",
        "# Create models directory if it doesn't exist\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Download the model\n",
        "model_url = \"https://github.com/pytorch/captum/raw/refs/heads/master/tutorials/models/imdb-model-cnn-large.pt\"\n",
        "model_path = \"models/imdb-model-cnn-large.pt\"\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    print(\"Downloading model file...\")\n",
        "    urllib.request.urlretrieve(model_url, model_path)\n",
        "    print(f\"\u2705 Model downloaded successfully: {model_path}\")\n",
        "else:\n",
        "    print(f\"\u2705 Model already exists: {model_path}\")\n",
        "\n",
        "# Check file size\n",
        "file_size = os.path.getsize(model_path) / (1024 * 1024)  # Convert to MB\n",
        "print(f\"\ud83d\udcca Model file size: {file_size:.1f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the CNN model architecture for IMDB sentiment analysis\n",
        "# This matches the architecture of the pre-trained model\n",
        "\n",
        "class IMDBConvNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutional Neural Network for IMDB sentiment analysis.\n",
        "    \n",
        "    This architecture matches the pre-trained model from Captum tutorials.\n",
        "    We'll use it for interpreting sentiment predictions on Australian tourism reviews.\n",
        "    \n",
        "    Architecture:\n",
        "    - Embedding layer (vocab_size=1002, embed_dim=128)\n",
        "    - 1D Convolution layers with different kernel sizes\n",
        "    - Global max pooling\n",
        "    - Fully connected layers for classification\n",
        "    \n",
        "    Args:\n",
        "        vocab_size (int): Size of vocabulary (default: 1002 for IMDB)\n",
        "        embed_dim (int): Embedding dimension (default: 128)\n",
        "        num_classes (int): Number of output classes (default: 2 for binary sentiment)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size=1002, embed_dim=128, num_classes=2):\n",
        "        super(IMDBConvNet, self).__init__()\n",
        "        \n",
        "        # Store model parameters\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        \n",
        "        # Convolutional layers with different kernel sizes\n",
        "        self.conv1 = nn.Conv1d(embed_dim, 100, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(embed_dim, 100, kernel_size=4, padding=2)\n",
        "        self.conv3 = nn.Conv1d(embed_dim, 100, kernel_size=5, padding=2)\n",
        "        \n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        \n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(300, 128)  # 300 = 100 * 3 conv outputs\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Input shape: (batch_size, sequence_length)\n",
        "        \n",
        "        # Embedding lookup\n",
        "        embedded = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
        "        \n",
        "        # Transpose for Conv1d: (batch_size, embed_dim, seq_len)\n",
        "        embedded = embedded.transpose(1, 2)\n",
        "        \n",
        "        # Apply convolutions with different kernel sizes\n",
        "        conv1_out = F.relu(self.conv1(embedded))  # (batch_size, 100, seq_len)\n",
        "        conv2_out = F.relu(self.conv2(embedded))  # (batch_size, 100, seq_len)\n",
        "        conv3_out = F.relu(self.conv3(embedded))  # (batch_size, 100, seq_len)\n",
        "        \n",
        "        # Global max pooling\n",
        "        pool1 = F.max_pool1d(conv1_out, kernel_size=conv1_out.size(2))  # (batch_size, 100, 1)\n",
        "        pool2 = F.max_pool1d(conv2_out, kernel_size=conv2_out.size(2))  # (batch_size, 100, 1)\n",
        "        pool3 = F.max_pool1d(conv3_out, kernel_size=conv3_out.size(2))  # (batch_size, 100, 1)\n",
        "        \n",
        "        # Concatenate pooled features\n",
        "        pooled = torch.cat([pool1, pool2, pool3], dim=1)  # (batch_size, 300, 1)\n",
        "        pooled = pooled.squeeze(2)  # (batch_size, 300)\n",
        "        \n",
        "        # Apply dropout\n",
        "        pooled = self.dropout(pooled)\n",
        "        \n",
        "        # Fully connected layers\n",
        "        fc1_out = F.relu(self.fc1(pooled))  # (batch_size, 128)\n",
        "        fc1_out = self.dropout(fc1_out)\n",
        "        \n",
        "        # Final classification layer\n",
        "        logits = self.fc2(fc1_out)  # (batch_size, num_classes)\n",
        "        \n",
        "        return logits\n",
        "    \n",
        "    def predict_sentiment(self, x):\n",
        "        \"\"\"Predict sentiment with probabilities.\"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = self.forward(x)\n",
        "            probabilities = F.softmax(logits, dim=1)\n",
        "            predictions = torch.argmax(probabilities, dim=1)\n",
        "            \n",
        "        return predictions, probabilities\n",
        "\n",
        "# Create model instance\n",
        "model = IMDBConvNet(vocab_size=1002, embed_dim=128, num_classes=2)\n",
        "print(f\"\ud83d\udcca Model created: {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "print(f\"\ud83c\udfd7\ufe0f Architecture: CNN with embedding -> conv1d -> max_pool -> fc\")\n",
        "print(f\"\ud83c\udfaf Task: Binary sentiment classification (positive/negative)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pre-trained weights and move model to device\n",
        "print(\"\ud83d\udd04 Loading pre-trained model weights...\")\n",
        "\n",
        "try:\n",
        "    # Load the pre-trained state dict\n",
        "    state_dict = torch.load(model_path, map_location=DEVICE)\n",
        "    model.load_state_dict(state_dict)\n",
        "    print(\"\u2705 Pre-trained weights loaded successfully!\")\n",
        "    \n",
        "    # Move model to the detected device\n",
        "    model = model.to(DEVICE)\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    \n",
        "    print(f\"\ud83d\udcf1 Model moved to device: {DEVICE}\")\n",
        "    print(f\"\u2699\ufe0f Model set to evaluation mode\")\n",
        "    \n",
        "    # Verify model is on correct device\n",
        "    model_device = next(model.parameters()).device\n",
        "    print(f\"\ud83d\udd27 Model parameters on device: {model_device}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error loading model: {e}\")\n",
        "    print(\"\ud83d\udca1 Please ensure the model file is downloaded correctly\")\n",
        "    raise e\n",
        "\n",
        "print(\"\\n\ud83c\udf89 Pre-trained IMDB sentiment model ready for interpretation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple vocabulary and tokenizer for the IMDB model\n",
        "# Note: This is a simplified version for demonstration\n",
        "# In practice, you'd use the exact vocabulary from the original training\n",
        "\n",
        "class SimpleTokenizer:\n",
        "    \"\"\"\n",
        "    Simple tokenizer for IMDB sentiment analysis with Australian context.\n",
        "    \n",
        "    This tokenizer maps words to indices for the pre-trained CNN model.\n",
        "    We'll include common words and Australian-specific terms.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Create basic vocabulary (simplified for demo)\n",
        "        # Index 0: padding, Index 1: unknown token\n",
        "        self.word_to_idx = {\n",
        "            '<pad>': 0,\n",
        "            '<unk>': 1,\n",
        "            # Common sentiment words\n",
        "            'good': 2, 'great': 3, 'excellent': 4, 'amazing': 5, 'wonderful': 6,\n",
        "            'bad': 7, 'terrible': 8, 'awful': 9, 'horrible': 10, 'disappointing': 11,\n",
        "            'love': 12, 'like': 13, 'enjoy': 14, 'hate': 15, 'dislike': 16,\n",
        "            'best': 17, 'worst': 18, 'perfect': 19, 'beautiful': 20, 'ugly': 21,\n",
        "            \n",
        "            # Australian-specific terms\n",
        "            'sydney': 22, 'melbourne': 23, 'brisbane': 24, 'perth': 25, 'adelaide': 26,\n",
        "            'opera': 27, 'house': 28, 'harbour': 29, 'bridge': 30, 'beach': 31,\n",
        "            'coffee': 32, 'cafe': 33, 'restaurant': 34, 'food': 35, 'tourism': 36,\n",
        "            'tourist': 37, 'vacation': 38, 'holiday': 39, 'trip': 40, 'visit': 41,\n",
        "            'australia': 42, 'australian': 43, 'aussie': 44, 'kangaroo': 45, 'koala': 46,\n",
        "            \n",
        "            # Common words\n",
        "            'the': 47, 'and': 48, 'or': 49, 'but': 50, 'is': 51, 'was': 52, 'are': 53,\n",
        "            'very': 54, 'really': 55, 'so': 56, 'too': 57, 'not': 58, 'no': 59, 'yes': 60,\n",
        "            'this': 61, 'that': 62, 'it': 63, 'i': 64, 'you': 65, 'we': 66, 'they': 67,\n",
        "            'place': 68, 'location': 69, 'service': 70, 'staff': 71, 'experience': 72,\n",
        "            'recommend': 73, 'worth': 74, 'money': 75, 'price': 76, 'expensive': 77,\n",
        "            'cheap': 78, 'quality': 79, 'clean': 80, 'dirty': 81, 'friendly': 82,\n",
        "            'rude': 83, 'helpful': 84, 'slow': 85, 'fast': 86, 'crowded': 87,\n",
        "            \n",
        "            # Vietnamese words (for multilingual examples)\n",
        "            'tuy\u1ec7t': 88, 'v\u1eddi': 89, '\u0111\u1eb9p': 90, 'x\u1ea5u': 91, 't\u1ed1t': 92, 't\u1ec7': 93,\n",
        "            'y\u00eau': 94, 'th\u00edch': 95, 'gh\u00e9t': 96, 'nh\u00e0': 97, 'h\u00e1t': 98, 'c\u00e0': 99, 'ph\u00ea': 100\n",
        "        }\n",
        "        \n",
        "        # Fill remaining vocabulary slots with dummy tokens\n",
        "        for i in range(101, 1002):\n",
        "            self.word_to_idx[f'word_{i}'] = i\n",
        "        \n",
        "        # Create reverse mapping\n",
        "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
        "        \n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Simple tokenization by splitting on whitespace and punctuation.\"\"\"\n",
        "        # Convert to lowercase and remove punctuation\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "        tokens = text.split()\n",
        "        return tokens\n",
        "    \n",
        "    def encode(self, text, max_length=256):\n",
        "        \"\"\"Convert text to tensor of token indices.\"\"\"\n",
        "        tokens = self.tokenize(text)\n",
        "        \n",
        "        # Convert tokens to indices\n",
        "        indices = []\n",
        "        for token in tokens[:max_length]:\n",
        "            if token in self.word_to_idx:\n",
        "                indices.append(self.word_to_idx[token])\n",
        "            else:\n",
        "                indices.append(self.word_to_idx['<unk>'])  # Unknown token\n",
        "        \n",
        "        # Pad sequence to max_length\n",
        "        while len(indices) < max_length:\n",
        "            indices.append(self.word_to_idx['<pad>'])\n",
        "        \n",
        "        return torch.tensor(indices[:max_length], dtype=torch.long)\n",
        "    \n",
        "    def decode(self, indices):\n",
        "        \"\"\"Convert tensor of indices back to text.\"\"\"\n",
        "        if isinstance(indices, torch.Tensor):\n",
        "            indices = indices.tolist()\n",
        "        \n",
        "        tokens = []\n",
        "        for idx in indices:\n",
        "            if idx == 0:  # Stop at padding\n",
        "                break\n",
        "            tokens.append(self.idx_to_word.get(idx, '<unk>'))\n",
        "        \n",
        "        return ' '.join(tokens)\n",
        "\n",
        "# Create tokenizer instance\n",
        "tokenizer = SimpleTokenizer()\n",
        "print(f\"\ud83d\udcdd Tokenizer created with vocabulary size: {len(tokenizer.word_to_idx)}\")\n",
        "print(f\"\ud83d\udd24 Sample words: {list(tokenizer.word_to_idx.keys())[:20]}\")\n",
        "\n",
        "# Test tokenizer with Australian example\n",
        "test_text = \"The Sydney Opera House is absolutely beautiful and amazing!\"\n",
        "encoded = tokenizer.encode(test_text, max_length=20)\n",
        "decoded = tokenizer.decode(encoded)\n",
        "\n",
        "print(f\"\\n\ud83e\uddea Tokenizer Test:\")\n",
        "print(f\"Original: {test_text}\")\n",
        "print(f\"Encoded: {encoded[:10]}...\")  # Show first 10 tokens\n",
        "print(f\"Decoded: {decoded}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Australian tourism examples for interpretation\n",
        "# These will be our test cases for understanding model behavior\n",
        "\n",
        "australian_tourism_examples = {\n",
        "    'positive': [\n",
        "        \"The Sydney Opera House is absolutely breathtaking and worth every dollar!\",\n",
        "        \"Melbourne coffee culture is amazing, best cafe experience in Australia!\",\n",
        "        \"Bondi Beach has perfect waves for surfing, beautiful and clean!\",\n",
        "        \"Perth beaches are pristine and peaceful, wonderful for families!\",\n",
        "        \"Great Barrier Reef snorkeling was an incredible experience, highly recommend!\",\n",
        "        \"Brisbane weather is fantastic year round, great place to visit!\",\n",
        "        \"Adelaide wine tours exceeded expectations, excellent service and quality!\",\n",
        "        \"Darwin wildlife parks are educational and fun, perfect for tourists!\"\n",
        "    ],\n",
        "    'negative': [\n",
        "        \"Sydney Opera House tours are overpriced and disappointing, waste of money!\",\n",
        "        \"Melbourne weather is terrible, cold and rainy, ruined our vacation!\",\n",
        "        \"Gold Coast beaches are crowded and dirty, awful tourist trap!\",\n",
        "        \"Perth is boring and isolated, nothing interesting to do here!\",\n",
        "        \"Brisbane food scene is terrible, expensive restaurants with bad service!\",\n",
        "        \"Adelaide is too quiet and boring, worst vacation destination ever!\",\n",
        "        \"Cairns accommodation was dirty and overpriced, horrible experience!\",\n",
        "        \"Hobart weather ruined our trip, cold and miserable the entire time!\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Vietnamese translations for multilingual interpretation\n",
        "vietnamese_examples = {\n",
        "    'positive': [\n",
        "        \"Nh\u00e0 h\u00e1t Opera Sydney th\u1eadt ngo\u1ea1n m\u1ee5c v\u00e0 x\u1ee9ng \u0111\u00e1ng t\u1eebng \u0111\u1ed3ng!\",\n",
        "        \"V\u0103n h\u00f3a c\u00e0 ph\u00ea Melbourne tuy\u1ec7t v\u1eddi, tr\u1ea3i nghi\u1ec7m qu\u00e1n c\u00e0 ph\u00ea t\u1ed1t nh\u1ea5t \u00dac!\",\n",
        "        \"B\u00e3i bi\u1ec3n Bondi c\u00f3 s\u00f3ng ho\u00e0n h\u1ea3o \u0111\u1ec3 l\u01b0\u1edbt s\u00f3ng, \u0111\u1eb9p v\u00e0 s\u1ea1ch s\u1ebd!\",\n",
        "        \"B\u00e3i bi\u1ec3n Perth nguy\u00ean s\u01a1 v\u00e0 y\u00ean b\u00ecnh, tuy\u1ec7t v\u1eddi cho gia \u0111\u00ecnh!\"\n",
        "    ],\n",
        "    'negative': [\n",
        "        \"Tour Nh\u00e0 h\u00e1t Opera Sydney \u0111\u1eaft \u0111\u1ecf v\u00e0 th\u1ea5t v\u1ecdng, l\u00e3ng ph\u00ed ti\u1ec1n!\",\n",
        "        \"Th\u1eddi ti\u1ebft Melbourne kh\u1ee7ng khi\u1ebfp, l\u1ea1nh v\u00e0 m\u01b0a, h\u1ee7y ho\u1ea1i k\u1ef3 ngh\u1ec9!\",\n",
        "        \"B\u00e3i bi\u1ec3n Gold Coast \u0111\u00f4ng \u0111\u00fac v\u00e0 b\u1ea9n th\u1ec9u, b\u1eaby kh\u00e1ch du l\u1ecbch kh\u1ee7ng khi\u1ebfp!\",\n",
        "        \"Perth nh\u00e0m ch\u00e1n v\u00e0 bi\u1ec7t l\u1eadp, kh\u00f4ng c\u00f3 g\u00ec th\u00fa v\u1ecb \u0111\u1ec3 l\u00e0m!\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Combine all examples\n",
        "all_examples = []\n",
        "all_labels = []\n",
        "\n",
        "# Add English examples\n",
        "for text in australian_tourism_examples['positive']:\n",
        "    all_examples.append(text)\n",
        "    all_labels.append(1)  # Positive\n",
        "\n",
        "for text in australian_tourism_examples['negative']:\n",
        "    all_examples.append(text)\n",
        "    all_labels.append(0)  # Negative\n",
        "\n",
        "# Add Vietnamese examples\n",
        "for text in vietnamese_examples['positive']:\n",
        "    all_examples.append(text)\n",
        "    all_labels.append(1)  # Positive\n",
        "\n",
        "for text in vietnamese_examples['negative']:\n",
        "    all_examples.append(text)\n",
        "    all_labels.append(0)  # Negative\n",
        "\n",
        "print(f\"\ud83c\udde6\ud83c\uddfa Australian Tourism Examples Created:\")\n",
        "print(f\"   \ud83d\udcca Total examples: {len(all_examples)}\")\n",
        "print(f\"   \u2705 Positive examples: {sum(all_labels)}\")\n",
        "print(f\"   \u274c Negative examples: {len(all_labels) - sum(all_labels)}\")\n",
        "print(f\"   \ud83c\udf0f Languages: English + Vietnamese\")\n",
        "\n",
        "# Show sample examples\n",
        "print(f\"\\n\ud83d\udcdd Sample Examples:\")\n",
        "print(f\"   Positive (EN): {all_examples[0]}\")\n",
        "print(f\"   Negative (EN): {all_examples[8]}\")\n",
        "print(f\"   Positive (VI): {all_examples[16]}\")\n",
        "print(f\"   Negative (VI): {all_examples[20]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test model predictions on our Australian examples\n",
        "print(\"\ud83e\uddea Testing Model Predictions on Australian Tourism Examples\\n\")\n",
        "\n",
        "def test_model_predictions(examples, labels, sample_size=8):\n",
        "    \"\"\"Test model predictions and show results.\"\"\"\n",
        "    \n",
        "    correct_predictions = 0\n",
        "    results = []\n",
        "    \n",
        "    for i, (text, true_label) in enumerate(zip(examples[:sample_size], labels[:sample_size])):\n",
        "        # Encode text\n",
        "        encoded = tokenizer.encode(text, max_length=256).unsqueeze(0).to(DEVICE)\n",
        "        \n",
        "        # Get prediction\n",
        "        pred_label, probabilities = model.predict_sentiment(encoded)\n",
        "        pred_label = pred_label.item()\n",
        "        prob_neg, prob_pos = probabilities[0].cpu().numpy()\n",
        "        \n",
        "        # Calculate accuracy\n",
        "        is_correct = (pred_label == true_label)\n",
        "        if is_correct:\n",
        "            correct_predictions += 1\n",
        "        \n",
        "        # Store result\n",
        "        results.append({\n",
        "            'text': text[:80] + '...' if len(text) > 80 else text,\n",
        "            'true_label': 'Positive' if true_label == 1 else 'Negative',\n",
        "            'predicted_label': 'Positive' if pred_label == 1 else 'Negative',\n",
        "            'confidence': prob_pos if pred_label == 1 else prob_neg,\n",
        "            'correct': '\u2705' if is_correct else '\u274c'\n",
        "        })\n",
        "        \n",
        "        # Print result\n",
        "        print(f\"Example {i+1}: {results[-1]['correct']}\")\n",
        "        print(f\"  Text: {results[-1]['text']}\")\n",
        "        print(f\"  True: {results[-1]['true_label']} | Predicted: {results[-1]['predicted_label']} ({results[-1]['confidence']:.3f})\")\n",
        "        print()\n",
        "    \n",
        "    accuracy = correct_predictions / sample_size\n",
        "    print(f\"\ud83d\udcca Prediction Accuracy: {accuracy:.1%} ({correct_predictions}/{sample_size})\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Test on sample examples\n",
        "prediction_results = test_model_predictions(all_examples, all_labels, sample_size=8)\n",
        "\n",
        "print(\"\\n\ud83c\udfaf Model Performance Summary:\")\n",
        "print(f\"   \u2022 The pre-trained IMDB model can handle Australian tourism text\")\n",
        "print(f\"   \u2022 Some vocabulary might be out-of-domain (tourism vs movie reviews)\")\n",
        "print(f\"   \u2022 Interpretation will help us understand what drives predictions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implement Integrated Gradients for text interpretation\n",
        "print(\"\ud83d\udd0d Implementing Integrated Gradients for Text Interpretation\\n\")\n",
        "\n",
        "# Initialize Integrated Gradients\n",
        "integrated_gradients = IntegratedGradients(model)\n",
        "\n",
        "def interpret_with_integrated_gradients(text, target_class=None, n_steps=50):\n",
        "    \"\"\"\n",
        "    Use Integrated Gradients to interpret model predictions for text.\n",
        "    \n",
        "    Args:\n",
        "        text (str): Input text to interpret\n",
        "        target_class (int): Target class for attribution (None for predicted class)\n",
        "        n_steps (int): Number of steps for integration\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (attributions, prediction, probabilities, tokens)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Encode text\n",
        "    encoded = tokenizer.encode(text, max_length=256).unsqueeze(0).to(DEVICE)\n",
        "    \n",
        "    # Get baseline (all padding tokens)\n",
        "    baseline = torch.zeros_like(encoded).to(DEVICE)\n",
        "    \n",
        "    # Get model prediction\n",
        "    pred_label, probabilities = model.predict_sentiment(encoded)\n",
        "    pred_class = pred_label.item()\n",
        "    \n",
        "    # Use predicted class if target not specified\n",
        "    if target_class is None:\n",
        "        target_class = pred_class\n",
        "    \n",
        "    # Compute attributions using Integrated Gradients\n",
        "    model.eval()\n",
        "    attributions = integrated_gradients.attribute(\n",
        "        encoded,\n",
        "        baselines=baseline,\n",
        "        target=target_class,\n",
        "        n_steps=n_steps\n",
        "    )\n",
        "    \n",
        "    # Get tokens for visualization\n",
        "    tokens = []\n",
        "    for idx in encoded[0].cpu().numpy():\n",
        "        if idx == 0:  # Stop at padding\n",
        "            break\n",
        "        tokens.append(tokenizer.idx_to_word.get(idx, '<unk>'))\n",
        "    \n",
        "    return attributions[0].cpu().numpy()[:len(tokens)], pred_class, probabilities[0].cpu().numpy(), tokens\n",
        "\n",
        "# Test Integrated Gradients on a positive Australian example\n",
        "example_text = \"The Sydney Opera House is absolutely beautiful and amazing!\"\n",
        "print(f\"\ud83c\udfad Analyzing: '{example_text}'\\n\")\n",
        "\n",
        "attributions, prediction, probs, tokens = interpret_with_integrated_gradients(example_text)\n",
        "\n",
        "print(f\"\ud83d\udcca Model Prediction:\")\n",
        "print(f\"   Predicted Class: {'Positive' if prediction == 1 else 'Negative'}\")\n",
        "print(f\"   Confidence: {probs[prediction]:.3f}\")\n",
        "print(f\"   Probabilities: Negative={probs[0]:.3f}, Positive={probs[1]:.3f}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udd0d Token Attribution Analysis:\")\n",
        "for token, attr in zip(tokens, attributions):\n",
        "    sentiment = \"\u2192 Positive\" if attr > 0 else \"\u2192 Negative\" if attr < 0 else \"\u2192 Neutral\"\n",
        "    print(f\"   '{token}': {attr:.4f} {sentiment}\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Interpretation Guide:\")\n",
        "print(\"   \u2022 Positive attributions push toward positive sentiment\")\n",
        "print(\"   \u2022 Negative attributions push toward negative sentiment\")\n",
        "print(\"   \u2022 Larger absolute values indicate stronger influence\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implement Input X Gradient for faster interpretation\n",
        "print(\"\u26a1 Implementing Input X Gradient for Fast Text Interpretation\\n\")\n",
        "\n",
        "# Initialize Input X Gradient\n",
        "input_x_gradient = InputXGradient(model)\n",
        "\n",
        "def interpret_with_input_x_gradient(text, target_class=None):\n",
        "    \"\"\"\n",
        "    Use Input X Gradient for fast text interpretation.\n",
        "    \n",
        "    This method is computationally cheaper than Integrated Gradients\n",
        "    but may be less accurate for some models.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Encode text\n",
        "    encoded = tokenizer.encode(text, max_length=256).unsqueeze(0).to(DEVICE)\n",
        "    \n",
        "    # Get model prediction\n",
        "    pred_label, probabilities = model.predict_sentiment(encoded)\n",
        "    pred_class = pred_label.item()\n",
        "    \n",
        "    # Use predicted class if target not specified\n",
        "    if target_class is None:\n",
        "        target_class = pred_class\n",
        "    \n",
        "    # Compute attributions using Input X Gradient\n",
        "    model.eval()\n",
        "    attributions = input_x_gradient.attribute(\n",
        "        encoded,\n",
        "        target=target_class\n",
        "    )\n",
        "    \n",
        "    # Get tokens for visualization\n",
        "    tokens = []\n",
        "    for idx in encoded[0].cpu().numpy():\n",
        "        if idx == 0:  # Stop at padding\n",
        "            break\n",
        "        tokens.append(tokenizer.idx_to_word.get(idx, '<unk>'))\n",
        "    \n",
        "    return attributions[0].cpu().numpy()[:len(tokens)], pred_class, probabilities[0].cpu().numpy(), tokens\n",
        "\n",
        "# Compare Input X Gradient with Integrated Gradients\n",
        "print(f\"\ud83d\udd2c Comparing Interpretation Methods:\\n\")\n",
        "\n",
        "# Test on negative Australian example\n",
        "negative_example = \"Sydney Opera House tours are overpriced and disappointing!\"\n",
        "print(f\"\ud83d\udcdd Analyzing: '{negative_example}'\\n\")\n",
        "\n",
        "# Get attributions from both methods\n",
        "ig_attr, ig_pred, ig_probs, ig_tokens = interpret_with_integrated_gradients(negative_example)\n",
        "ixg_attr, ixg_pred, ixg_probs, ixg_tokens = interpret_with_input_x_gradient(negative_example)\n",
        "\n",
        "print(f\"\ud83d\udcca Method Comparison:\")\n",
        "print(f\"\\n\ud83d\udd0d Integrated Gradients:\")\n",
        "print(f\"   Prediction: {'Positive' if ig_pred == 1 else 'Negative'} ({ig_probs[ig_pred]:.3f})\")\n",
        "for token, attr in zip(ig_tokens[:8], ig_attr[:8]):  # Show first 8 tokens\n",
        "    print(f\"   '{token}': {attr:.4f}\")\n",
        "\n",
        "print(f\"\\n\u26a1 Input X Gradient:\")\n",
        "print(f\"   Prediction: {'Positive' if ixg_pred == 1 else 'Negative'} ({ixg_probs[ixg_pred]:.3f})\")\n",
        "for token, attr in zip(ixg_tokens[:8], ixg_attr[:8]):  # Show first 8 tokens\n",
        "    print(f\"   '{token}': {attr:.4f}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udcad Key Differences:\")\n",
        "print(f\"   \u2022 Integrated Gradients: More accurate, computationally expensive\")\n",
        "print(f\"   \u2022 Input X Gradient: Faster, may have noise in attributions\")\n",
        "print(f\"   \u2022 Both help identify important words for sentiment classification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implement Layer Conductance for understanding internal representations\n",
        "print(\"\ud83e\udde0 Implementing Layer Conductance for Internal Layer Analysis\\n\")\n",
        "\n",
        "# Initialize Layer Conductance for embedding layer\n",
        "layer_conductance = LayerConductance(model, model.embedding)\n",
        "\n",
        "def analyze_layer_conductance(text, target_class=None):\n",
        "    \"\"\"\n",
        "    Use Layer Conductance to understand how embedding layer contributes to predictions.\n",
        "    \n",
        "    This helps us understand what the model learns about word representations.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Encode text\n",
        "    encoded = tokenizer.encode(text, max_length=256).unsqueeze(0).to(DEVICE)\n",
        "    \n",
        "    # Get baseline (all padding tokens)\n",
        "    baseline = torch.zeros_like(encoded).to(DEVICE)\n",
        "    \n",
        "    # Get model prediction\n",
        "    pred_label, probabilities = model.predict_sentiment(encoded)\n",
        "    pred_class = pred_label.item()\n",
        "    \n",
        "    # Use predicted class if target not specified\n",
        "    if target_class is None:\n",
        "        target_class = pred_class\n",
        "    \n",
        "    # Compute layer conductance\n",
        "    model.eval()\n",
        "    conductance = layer_conductance.attribute(\n",
        "        encoded,\n",
        "        baselines=baseline,\n",
        "        target=target_class\n",
        "    )\n",
        "    \n",
        "    # Sum across embedding dimensions to get token-level importance\n",
        "    token_conductance = conductance.sum(dim=2)[0].cpu().numpy()\n",
        "    \n",
        "    # Get tokens\n",
        "    tokens = []\n",
        "    for idx in encoded[0].cpu().numpy():\n",
        "        if idx == 0:  # Stop at padding\n",
        "            break\n",
        "        tokens.append(tokenizer.idx_to_word.get(idx, '<unk>'))\n",
        "    \n",
        "    return token_conductance[:len(tokens)], pred_class, probabilities[0].cpu().numpy(), tokens\n",
        "\n",
        "# Test Layer Conductance on Australian examples\n",
        "australian_examples_for_analysis = [\n",
        "    \"Melbourne coffee culture is amazing and wonderful!\",  # Positive\n",
        "    \"Perth is boring and terrible, worst vacation ever!\"    # Negative\n",
        "]\n",
        "\n",
        "for i, example in enumerate(australian_examples_for_analysis):\n",
        "    print(f\"\ud83d\udd2c Analysis {i+1}: '{example}'\\n\")\n",
        "    \n",
        "    conductance, prediction, probs, tokens = analyze_layer_conductance(example)\n",
        "    \n",
        "    print(f\"\ud83d\udcca Prediction: {'Positive' if prediction == 1 else 'Negative'} ({probs[prediction]:.3f})\")\n",
        "    print(f\"\ud83e\udde0 Embedding Layer Conductance:\")\n",
        "    \n",
        "    # Sort tokens by conductance magnitude\n",
        "    token_conductance_pairs = list(zip(tokens, conductance))\n",
        "    token_conductance_pairs.sort(key=lambda x: abs(x[1]), reverse=True)\n",
        "    \n",
        "    print(f\"   Most Influential Tokens:\")\n",
        "    for token, cond in token_conductance_pairs[:5]:  # Top 5\n",
        "        influence = \"Strong Positive\" if cond > 0.1 else \"Positive\" if cond > 0 else \"Negative\" if cond > -0.1 else \"Strong Negative\"\n",
        "        print(f\"     '{token}': {cond:.4f} ({influence})\")\n",
        "    \n",
        "    print()\n",
        "\n",
        "print(\"\ud83d\udca1 Layer Conductance Insights:\")\n",
        "print(\"   \u2022 Shows how embedding representations contribute to final prediction\")\n",
        "print(\"   \u2022 Higher absolute values indicate more influential embeddings\")\n",
        "print(\"   \u2022 Helps understand what the model learned about word semantics\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualization functions for attribution analysis\n",
        "print(\"\ud83c\udfa8 Creating Visualization Functions for Attribution Analysis\\n\")\n",
        "\n",
        "def visualize_attribution_heatmap(tokens, attributions, title=\"Token Attribution Heatmap\"):\n",
        "    \"\"\"\n",
        "    Create a heatmap visualization of token attributions.\n",
        "    \"\"\"\n",
        "    import matplotlib.patches as patches\n",
        "    \n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(figsize=(15, 3))\n",
        "    \n",
        "    # Normalize attributions for color mapping\n",
        "    max_attr = max(abs(min(attributions)), abs(max(attributions)))\n",
        "    normalized_attr = [attr / max_attr for attr in attributions]\n",
        "    \n",
        "    # Create color map (red for negative, blue for positive)\n",
        "    colors = []\n",
        "    for attr in normalized_attr:\n",
        "        if attr > 0:\n",
        "            colors.append(plt.cm.Blues(abs(attr)))\n",
        "        else:\n",
        "            colors.append(plt.cm.Reds(abs(attr)))\n",
        "    \n",
        "    # Plot tokens with background colors\n",
        "    for i, (token, attr, color) in enumerate(zip(tokens, attributions, colors)):\n",
        "        # Create rectangle for background\n",
        "        rect = patches.Rectangle((i-0.4, -0.4), 0.8, 0.8, \n",
        "                               facecolor=color, alpha=0.7)\n",
        "        ax.add_patch(rect)\n",
        "        \n",
        "        # Add token text\n",
        "        ax.text(i, 0, token, ha='center', va='center', \n",
        "               fontsize=10, weight='bold')\n",
        "        \n",
        "        # Add attribution value below\n",
        "        ax.text(i, -0.7, f'{attr:.3f}', ha='center', va='center', \n",
        "               fontsize=8, style='italic')\n",
        "    \n",
        "    # Set plot properties\n",
        "    ax.set_xlim(-0.5, len(tokens)-0.5)\n",
        "    ax.set_ylim(-1, 0.5)\n",
        "    ax.set_title(title, fontsize=14, weight='bold')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    \n",
        "    # Add legend\n",
        "    from matplotlib.lines import Line2D\n",
        "    legend_elements = [\n",
        "        Line2D([0], [0], marker='s', color='w', markerfacecolor='blue', \n",
        "               markersize=10, label='Positive Attribution'),\n",
        "        Line2D([0], [0], marker='s', color='w', markerfacecolor='red', \n",
        "               markersize=10, label='Negative Attribution')\n",
        "    ]\n",
        "    ax.legend(handles=legend_elements, loc='upper right')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def create_attribution_bar_chart(tokens, attributions, title=\"Token Attribution Analysis\"):\n",
        "    \"\"\"\n",
        "    Create a bar chart showing token attributions.\n",
        "    \"\"\"\n",
        "    # Create DataFrame for easier plotting\n",
        "    df = pd.DataFrame({\n",
        "        'token': tokens,\n",
        "        'attribution': attributions,\n",
        "        'abs_attribution': [abs(attr) for attr in attributions]\n",
        "    })\n",
        "    \n",
        "    # Sort by absolute attribution\n",
        "    df = df.sort_values('abs_attribution', ascending=True)\n",
        "    \n",
        "    # Create color mapping\n",
        "    colors = ['red' if attr < 0 else 'blue' for attr in df['attribution']]\n",
        "    \n",
        "    # Create horizontal bar chart\n",
        "    fig, ax = plt.subplots(figsize=(10, max(6, len(tokens) * 0.4)))\n",
        "    \n",
        "    bars = ax.barh(range(len(df)), df['attribution'], color=colors, alpha=0.7)\n",
        "    \n",
        "    # Customize plot\n",
        "    ax.set_yticks(range(len(df)))\n",
        "    ax.set_yticklabels(df['token'])\n",
        "    ax.set_xlabel('Attribution Score')\n",
        "    ax.set_title(title, fontsize=14, weight='bold')\n",
        "    ax.grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    # Add vertical line at x=0\n",
        "    ax.axvline(x=0, color='black', linestyle='-', alpha=0.5)\n",
        "    \n",
        "    # Add legend\n",
        "    from matplotlib.patches import Patch\n",
        "    legend_elements = [\n",
        "        Patch(facecolor='blue', alpha=0.7, label='Positive Attribution'),\n",
        "        Patch(facecolor='red', alpha=0.7, label='Negative Attribution')\n",
        "    ]\n",
        "    ax.legend(handles=legend_elements)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def create_interactive_attribution_plot(tokens, attributions, text, prediction_info):\n",
        "    \"\"\"\n",
        "    Create an interactive plot using Plotly.\n",
        "    \"\"\"\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'token': tokens,\n",
        "        'attribution': attributions,\n",
        "        'abs_attribution': [abs(attr) for attr in attributions],\n",
        "        'position': range(len(tokens))\n",
        "    })\n",
        "    \n",
        "    # Create color mapping\n",
        "    colors = ['red' if attr < 0 else 'blue' for attr in attributions]\n",
        "    \n",
        "    # Create interactive bar plot\n",
        "    fig = go.Figure()\n",
        "    \n",
        "    fig.add_trace(go.Bar(\n",
        "        x=df['position'],\n",
        "        y=df['attribution'],\n",
        "        text=df['token'],\n",
        "        textposition='auto',\n",
        "        marker_color=colors,\n",
        "        hovertemplate='<b>Token:</b> %{text}<br><b>Attribution:</b> %{y:.4f}<extra></extra>'\n",
        "    ))\n",
        "    \n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        title=f\"Interactive Attribution Analysis<br><sub>Text: {text[:100]}{'...' if len(text) > 100 else ''}</sub>\",\n",
        "        xaxis_title=\"Token Position\",\n",
        "        yaxis_title=\"Attribution Score\",\n",
        "        template=\"plotly_white\",\n",
        "        height=500\n",
        "    )\n",
        "    \n",
        "    # Add horizontal line at y=0\n",
        "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"black\", opacity=0.5)\n",
        "    \n",
        "    fig.show()\n",
        "\n",
        "print(\"\u2705 Visualization functions created!\")\n",
        "print(\"\ud83d\udcca Available visualizations:\")\n",
        "print(\"   \u2022 visualize_attribution_heatmap() - Color-coded token heatmap\")\n",
        "print(\"   \u2022 create_attribution_bar_chart() - Static bar chart with seaborn\")\n",
        "print(\"   \u2022 create_interactive_attribution_plot() - Interactive Plotly visualization\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive analysis example with multiple interpretation methods\n",
        "print(\"\ud83d\udd2c Comprehensive Interpretation Analysis: Australian Tourism Review\\n\")\n",
        "\n",
        "# Select an interesting example for detailed analysis\n",
        "analysis_text = \"The Sydney Opera House is breathtaking but the tour was overpriced and disappointing!\"\n",
        "print(f\"\ud83d\udcdd Analyzing Complex Sentiment: '{analysis_text}'\\n\")\n",
        "print(\"\ud83d\udcad This example contains both positive and negative sentiment words...\\n\")\n",
        "\n",
        "# Get interpretations from multiple methods\n",
        "print(\"\ud83d\udd0d Running Multiple Interpretation Methods...\\n\")\n",
        "\n",
        "# 1. Integrated Gradients\n",
        "ig_attr, ig_pred, ig_probs, ig_tokens = interpret_with_integrated_gradients(analysis_text)\n",
        "\n",
        "# 2. Input X Gradient\n",
        "ixg_attr, ixg_pred, ixg_probs, ixg_tokens = interpret_with_input_x_gradient(analysis_text)\n",
        "\n",
        "# 3. Layer Conductance\n",
        "lc_attr, lc_pred, lc_probs, lc_tokens = analyze_layer_conductance(analysis_text)\n",
        "\n",
        "# Display results\n",
        "print(f\"\ud83d\udcca Model Prediction Results:\")\n",
        "print(f\"   Predicted Class: {'Positive' if ig_pred == 1 else 'Negative'}\")\n",
        "print(f\"   Confidence: {ig_probs[ig_pred]:.3f}\")\n",
        "print(f\"   Probabilities: [Negative: {ig_probs[0]:.3f}, Positive: {ig_probs[1]:.3f}]\")\n",
        "\n",
        "print(f\"\\n\ud83c\udfaf Top Influential Words by Method:\")\n",
        "\n",
        "# Helper function to get top words\n",
        "def get_top_words(tokens, attributions, n=5):\n",
        "    word_attr_pairs = list(zip(tokens, attributions))\n",
        "    # Sort by absolute attribution value\n",
        "    word_attr_pairs.sort(key=lambda x: abs(x[1]), reverse=True)\n",
        "    return word_attr_pairs[:n]\n",
        "\n",
        "# Display top words for each method\n",
        "methods = [\n",
        "    (\"Integrated Gradients\", ig_tokens, ig_attr),\n",
        "    (\"Input X Gradient\", ixg_tokens, ixg_attr),\n",
        "    (\"Layer Conductance\", lc_tokens, lc_attr)\n",
        "]\n",
        "\n",
        "for method_name, tokens, attributions in methods:\n",
        "    print(f\"\\n   {method_name}:\")\n",
        "    top_words = get_top_words(tokens, attributions, n=5)\n",
        "    for word, attr in top_words:\n",
        "        sentiment_direction = \"\u2192 Positive\" if attr > 0 else \"\u2192 Negative\"\n",
        "        print(f\"     '{word}': {attr:.4f} {sentiment_direction}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udcc8 Creating Visualizations...\")\n",
        "\n",
        "# Create visualizations\n",
        "print(f\"\\n1\ufe0f\u20e3 Heatmap Visualization (Integrated Gradients):\")\n",
        "visualize_attribution_heatmap(ig_tokens, ig_attr, \n",
        "                            \"Integrated Gradients: Australian Tourism Sentiment\")\n",
        "\n",
        "print(f\"\\n2\ufe0f\u20e3 Bar Chart Visualization (Input X Gradient):\")\n",
        "create_attribution_bar_chart(ixg_tokens, ixg_attr, \n",
        "                            \"Input X Gradient: Token Importance Analysis\")\n",
        "\n",
        "print(f\"\\n3\ufe0f\u20e3 Interactive Visualization (Layer Conductance):\")\n",
        "create_interactive_attribution_plot(lc_tokens, lc_attr, analysis_text, \n",
        "                                  {'prediction': ig_pred, 'confidence': ig_probs[ig_pred]})\n",
        "\n",
        "print(f\"\\n\ud83e\udde0 Interpretation Insights:\")\n",
        "print(f\"   \u2022 'breathtaking' and 'beautiful' words likely push toward positive\")\n",
        "print(f\"   \u2022 'overpriced' and 'disappointing' likely push toward negative\")\n",
        "print(f\"   \u2022 Model resolves mixed sentiment based on word importance\")\n",
        "print(f\"   \u2022 Different methods may highlight different aspects\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multilingual interpretation: English vs Vietnamese\n",
        "print(\"\ud83c\udf0f Multilingual Text Interpretation: English vs Vietnamese\\n\")\n",
        "\n",
        "# Create English-Vietnamese paired examples\n",
        "multilingual_pairs = [\n",
        "    {\n",
        "        'english': \"Sydney Opera House is absolutely beautiful and amazing!\",\n",
        "        'vietnamese': \"Nh\u00e0 h\u00e1t Opera Sydney tuy\u1ec7t v\u1eddi v\u00e0 \u0111\u1eb9p tuy\u1ec7t!\",\n",
        "        'expected_sentiment': 'Positive'\n",
        "    },\n",
        "    {\n",
        "        'english': \"Melbourne weather is terrible and disappointing!\",\n",
        "        'vietnamese': \"Th\u1eddi ti\u1ebft Melbourne kh\u1ee7ng khi\u1ebfp v\u00e0 th\u1ea5t v\u1ecdng!\",\n",
        "        'expected_sentiment': 'Negative'\n",
        "    }\n",
        "]\n",
        "\n",
        "def compare_multilingual_interpretation(english_text, vietnamese_text, expected_sentiment):\n",
        "    \"\"\"\n",
        "    Compare interpretation results between English and Vietnamese text.\n",
        "    \"\"\"\n",
        "    print(f\"\ud83d\udd2c Multilingual Comparison Analysis\\n\")\n",
        "    print(f\"Expected Sentiment: {expected_sentiment}\")\n",
        "    print(f\"English: '{english_text}'\")\n",
        "    print(f\"Vietnamese: '{vietnamese_text}'\\n\")\n",
        "    \n",
        "    # Analyze English text\n",
        "    en_attr, en_pred, en_probs, en_tokens = interpret_with_integrated_gradients(english_text)\n",
        "    \n",
        "    # Analyze Vietnamese text  \n",
        "    vi_attr, vi_pred, vi_probs, vi_tokens = interpret_with_integrated_gradients(vietnamese_text)\n",
        "    \n",
        "    # Compare predictions\n",
        "    en_sentiment = 'Positive' if en_pred == 1 else 'Negative'\n",
        "    vi_sentiment = 'Positive' if vi_pred == 1 else 'Negative'\n",
        "    \n",
        "    print(f\"\ud83d\udcca Prediction Comparison:\")\n",
        "    print(f\"   English:    {en_sentiment} (confidence: {en_probs[en_pred]:.3f})\")\n",
        "    print(f\"   Vietnamese: {vi_sentiment} (confidence: {vi_probs[vi_pred]:.3f})\")\n",
        "    print(f\"   Agreement:  {'\u2705 Yes' if en_pred == vi_pred else '\u274c No'}\")\n",
        "    \n",
        "    # Show top attributed words\n",
        "    print(f\"\\n\ud83d\udd0d Top Influential Words:\")\n",
        "    \n",
        "    en_top = get_top_words(en_tokens, en_attr, n=3)\n",
        "    vi_top = get_top_words(vi_tokens, vi_attr, n=3)\n",
        "    \n",
        "    print(f\"   English:\")\n",
        "    for word, attr in en_top:\n",
        "        print(f\"     '{word}': {attr:.4f}\")\n",
        "    \n",
        "    print(f\"   Vietnamese:\")\n",
        "    for word, attr in vi_top:\n",
        "        print(f\"     '{word}': {attr:.4f}\")\n",
        "    \n",
        "    # Create side-by-side visualization\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8))\n",
        "    \n",
        "    # English visualization\n",
        "    ax1.bar(range(len(en_tokens)), en_attr, \n",
        "           color=['blue' if attr > 0 else 'red' for attr in en_attr], alpha=0.7)\n",
        "    ax1.set_xticks(range(len(en_tokens)))\n",
        "    ax1.set_xticklabels(en_tokens, rotation=45, ha='right')\n",
        "    ax1.set_title(f'English Attribution: {en_sentiment} ({en_probs[en_pred]:.3f})', fontweight='bold')\n",
        "    ax1.set_ylabel('Attribution Score')\n",
        "    ax1.grid(axis='y', alpha=0.3)\n",
        "    ax1.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
        "    \n",
        "    # Vietnamese visualization\n",
        "    ax2.bar(range(len(vi_tokens)), vi_attr, \n",
        "           color=['blue' if attr > 0 else 'red' for attr in vi_attr], alpha=0.7)\n",
        "    ax2.set_xticks(range(len(vi_tokens)))\n",
        "    ax2.set_xticklabels(vi_tokens, rotation=45, ha='right')\n",
        "    ax2.set_title(f'Vietnamese Attribution: {vi_sentiment} ({vi_probs[vi_pred]:.3f})', fontweight='bold')\n",
        "    ax2.set_ylabel('Attribution Score')\n",
        "    ax2.grid(axis='y', alpha=0.3)\n",
        "    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return {\n",
        "        'english': {'prediction': en_pred, 'confidence': en_probs[en_pred], 'top_words': en_top},\n",
        "        'vietnamese': {'prediction': vi_pred, 'confidence': vi_probs[vi_pred], 'top_words': vi_top},\n",
        "        'agreement': en_pred == vi_pred\n",
        "    }\n",
        "\n",
        "# Analyze multilingual pairs\n",
        "multilingual_results = []\n",
        "\n",
        "for i, pair in enumerate(multilingual_pairs):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Analysis {i+1}: {pair['expected_sentiment']} Sentiment\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    result = compare_multilingual_interpretation(\n",
        "        pair['english'], \n",
        "        pair['vietnamese'], \n",
        "        pair['expected_sentiment']\n",
        "    )\n",
        "    multilingual_results.append(result)\n",
        "\n",
        "# Summary analysis\n",
        "print(f\"\\n\ud83c\udf0d Multilingual Interpretation Summary:\")\n",
        "agreement_rate = sum(1 for r in multilingual_results if r['agreement']) / len(multilingual_results)\n",
        "print(f\"   \ud83d\udcca English-Vietnamese Agreement Rate: {agreement_rate:.1%}\")\n",
        "print(f\"   \ud83d\udcad Model Performance on Vietnamese: Limited by training on English\")\n",
        "print(f\"   \ud83d\udd0d Attribution patterns may differ due to vocabulary differences\")\n",
        "print(f\"   \ud83c\udfaf Future work: Multilingual models for better cross-language interpretation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TensorBoard integration for tracking interpretation experiments\n",
        "print(\"\ud83d\udcca Setting up TensorBoard for Interpretation Tracking\\n\")\n",
        "\n",
        "import os\n",
        "import time\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Platform-specific TensorBoard log directory setup\n",
        "def get_run_logdir(experiment_name):\n",
        "    \"\"\"Generate unique log directory for this interpretation experiment.\"\"\"\n",
        "    \n",
        "    if IS_COLAB:\n",
        "        root_logdir = \"/content/tensorboard_logs\"\n",
        "    elif IS_KAGGLE:\n",
        "        root_logdir = \"./tensorboard_logs\"\n",
        "    else:\n",
        "        root_logdir = \"./tensorboard_logs\"\n",
        "    \n",
        "    # Create timestamp for unique run\n",
        "    timestamp = time.strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
        "    run_logdir = os.path.join(root_logdir, f\"{experiment_name}_{timestamp}\")\n",
        "    \n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(run_logdir, exist_ok=True)\n",
        "    \n",
        "    return run_logdir\n",
        "\n",
        "# Create TensorBoard writer for interpretation experiments\n",
        "interpretation_logdir = get_run_logdir(\"text_model_interpretation\")\n",
        "writer = SummaryWriter(log_dir=interpretation_logdir)\n",
        "\n",
        "print(f\"\ud83d\udcc1 TensorBoard logs will be saved to: {interpretation_logdir}\")\n",
        "\n",
        "# Function to log interpretation results to TensorBoard\n",
        "def log_interpretation_results(text, method_name, tokens, attributions, prediction_info, step):\n",
        "    \"\"\"\n",
        "    Log interpretation results to TensorBoard for tracking and comparison.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Log prediction metrics\n",
        "    writer.add_scalar(f'Predictions/{method_name}/Confidence', \n",
        "                     prediction_info['confidence'], step)\n",
        "    writer.add_scalar(f'Predictions/{method_name}/Predicted_Class', \n",
        "                     prediction_info['prediction'], step)\n",
        "    \n",
        "    # Log attribution statistics\n",
        "    attr_mean = np.mean(attributions)\n",
        "    attr_std = np.std(attributions)\n",
        "    attr_max = np.max(attributions)\n",
        "    attr_min = np.min(attributions)\n",
        "    \n",
        "    writer.add_scalar(f'Attributions/{method_name}/Mean', attr_mean, step)\n",
        "    writer.add_scalar(f'Attributions/{method_name}/Std', attr_std, step)\n",
        "    writer.add_scalar(f'Attributions/{method_name}/Max', attr_max, step)\n",
        "    writer.add_scalar(f'Attributions/{method_name}/Min', attr_min, step)\n",
        "    \n",
        "    # Log attribution histogram\n",
        "    writer.add_histogram(f'Attribution_Distribution/{method_name}', \n",
        "                        np.array(attributions), step)\n",
        "    \n",
        "    # Create text summary with top attributed words\n",
        "    top_words = get_top_words(tokens, attributions, n=5)\n",
        "    text_summary = f\"Text: {text}\\n\\nTop Words:\\n\"\n",
        "    for word, attr in top_words:\n",
        "        text_summary += f\"'{word}': {attr:.4f}\\n\"\n",
        "    \n",
        "    writer.add_text(f'Analysis/{method_name}', text_summary, step)\n",
        "\n",
        "# Log some of our previous analyses to TensorBoard\n",
        "print(\"\ud83d\udcc8 Logging interpretation experiments to TensorBoard...\\n\")\n",
        "\n",
        "# Example analyses to log\n",
        "example_analyses = [\n",
        "    \"The Sydney Opera House is absolutely beautiful and amazing!\",\n",
        "    \"Sydney Opera House tours are overpriced and disappointing!\",\n",
        "    \"Melbourne coffee culture is amazing and wonderful!\",\n",
        "    \"Perth is boring and terrible, worst vacation ever!\"\n",
        "]\n",
        "\n",
        "for step, text in enumerate(example_analyses):\n",
        "    print(f\"Logging analysis {step+1}: {text[:50]}...\")\n",
        "    \n",
        "    # Get interpretations\n",
        "    ig_attr, ig_pred, ig_probs, ig_tokens = interpret_with_integrated_gradients(text)\n",
        "    ixg_attr, ixg_pred, ixg_probs, ixg_tokens = interpret_with_input_x_gradient(text)\n",
        "    \n",
        "    # Log to TensorBoard\n",
        "    pred_info = {'prediction': ig_pred, 'confidence': ig_probs[ig_pred]}\n",
        "    \n",
        "    log_interpretation_results(text, 'Integrated_Gradients', ig_tokens, ig_attr, pred_info, step)\n",
        "    log_interpretation_results(text, 'Input_X_Gradient', ixg_tokens, ixg_attr, pred_info, step)\n",
        "\n",
        "print(f\"\\n\u2705 Interpretation experiments logged to TensorBoard!\")\n",
        "print(f\"\\n\ud83d\udcca To view TensorBoard:\")\n",
        "\n",
        "if IS_COLAB:\n",
        "    print(\"   In Google Colab:\")\n",
        "    print(\"   1. Run: %load_ext tensorboard\")\n",
        "    print(f\"   2. Run: %tensorboard --logdir {interpretation_logdir}\")\n",
        "elif IS_KAGGLE:\n",
        "    print(\"   In Kaggle:\")\n",
        "    print(f\"   1. Download logs from: {interpretation_logdir}\")\n",
        "    print(\"   2. Run locally: tensorboard --logdir ./tensorboard_logs\")\n",
        "else:\n",
        "    print(\"   Locally:\")\n",
        "    print(f\"   1. Run: tensorboard --logdir {interpretation_logdir}\")\n",
        "    print(\"   2. Open http://localhost:6006 in browser\")\n",
        "\n",
        "print(f\"\\n\ud83d\udcc8 Available TensorBoard visualizations:\")\n",
        "print(f\"   \u2022 Scalars: Prediction confidence and attribution statistics\")\n",
        "print(f\"   \u2022 Histograms: Attribution value distributions\")\n",
        "print(f\"   \u2022 Text: Detailed analysis summaries with top words\")\n",
        "print(f\"   \u2022 Compare: Side-by-side method comparisons\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an interactive interpretation dashboard\n",
        "print(\"\ud83c\udfae Creating Interactive Interpretation Dashboard\\n\")\n",
        "\n",
        "def create_interpretation_dashboard():\n",
        "    \"\"\"\n",
        "    Create an interactive dashboard for text interpretation.\n",
        "    \n",
        "    This function provides a simple interface for users to input text\n",
        "    and see interpretation results from multiple methods.\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"\ud83c\udfaf Interactive Text Interpretation Dashboard\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Enter text to analyze (or use provided examples):\")\n",
        "    print()\n",
        "    \n",
        "    # Predefined examples for quick testing\n",
        "    example_texts = [\n",
        "        \"The Sydney Opera House is absolutely breathtaking!\",\n",
        "        \"Melbourne coffee shops are overpriced and disappointing.\",\n",
        "        \"Bondi Beach has perfect waves for surfing!\",\n",
        "        \"Perth weather is terrible and ruins vacations.\",\n",
        "        \"Brisbane wildlife parks are educational and fun!\",\n",
        "        \"Adelaide restaurants have poor service and bad food.\"\n",
        "    ]\n",
        "    \n",
        "    print(\"\ud83d\udcdd Example texts (copy and modify as needed):\")\n",
        "    for i, example in enumerate(example_texts, 1):\n",
        "        print(f\"   {i}. {example}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    \n",
        "    return example_texts\n",
        "\n",
        "def analyze_custom_text(text, show_visualizations=True):\n",
        "    \"\"\"\n",
        "    Comprehensive analysis function for custom text input.\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"\ud83d\udd0d Analyzing: '{text}'\\n\")\n",
        "    \n",
        "    # Run all interpretation methods\n",
        "    print(\"\u26a1 Running interpretation methods...\")\n",
        "    \n",
        "    try:\n",
        "        ig_attr, ig_pred, ig_probs, ig_tokens = interpret_with_integrated_gradients(text)\n",
        "        ixg_attr, ixg_pred, ixg_probs, ixg_tokens = interpret_with_input_x_gradient(text)\n",
        "        lc_attr, lc_pred, lc_probs, lc_tokens = analyze_layer_conductance(text)\n",
        "        \n",
        "        # Display results\n",
        "        print(f\"\\n\ud83d\udcca Analysis Results:\")\n",
        "        print(f\"   Predicted Sentiment: {'Positive' if ig_pred == 1 else 'Negative'}\")\n",
        "        print(f\"   Confidence: {ig_probs[ig_pred]:.3f}\")\n",
        "        print(f\"   Probability Distribution: [Neg: {ig_probs[0]:.3f}, Pos: {ig_probs[1]:.3f}]\")\n",
        "        \n",
        "        # Show top influential words across methods\n",
        "        print(f\"\\n\ud83c\udfaf Most Influential Words:\")\n",
        "        \n",
        "        methods = [\n",
        "            (\"Integrated Gradients\", ig_tokens, ig_attr),\n",
        "            (\"Input X Gradient\", ixg_tokens, ixg_attr),\n",
        "            (\"Layer Conductance\", lc_tokens, lc_attr)\n",
        "        ]\n",
        "        \n",
        "        for method_name, tokens, attributions in methods:\n",
        "            top_words = get_top_words(tokens, attributions, n=3)\n",
        "            print(f\"   {method_name}:\")\n",
        "            for word, attr in top_words:\n",
        "                sentiment_dir = \"Positive\" if attr > 0 else \"Negative\"\n",
        "                print(f\"     '{word}': {attr:.4f} \u2192 {sentiment_dir}\")\n",
        "        \n",
        "        # Create visualizations if requested\n",
        "        if show_visualizations:\n",
        "            print(f\"\\n\ud83c\udfa8 Creating visualizations...\")\n",
        "            \n",
        "            # Heatmap for Integrated Gradients\n",
        "            visualize_attribution_heatmap(ig_tokens, ig_attr, \n",
        "                                        f\"Integrated Gradients Analysis\")\n",
        "            \n",
        "            # Interactive plot for comparison\n",
        "            create_interactive_attribution_plot(ig_tokens, ig_attr, text, \n",
        "                                              {'prediction': ig_pred, 'confidence': ig_probs[ig_pred]})\n",
        "        \n",
        "        # Log to TensorBoard\n",
        "        step = len(example_analyses)  # Use as next step\n",
        "        pred_info = {'prediction': ig_pred, 'confidence': ig_probs[ig_pred]}\n",
        "        log_interpretation_results(text, 'Custom_Analysis', ig_tokens, ig_attr, pred_info, step)\n",
        "        \n",
        "        print(f\"\\n\u2705 Analysis completed and logged to TensorBoard!\")\n",
        "        \n",
        "        return {\n",
        "            'prediction': ig_pred,\n",
        "            'confidence': ig_probs[ig_pred],\n",
        "            'methods': {\n",
        "                'integrated_gradients': {'tokens': ig_tokens, 'attributions': ig_attr},\n",
        "                'input_x_gradient': {'tokens': ixg_tokens, 'attributions': ixg_attr},\n",
        "                'layer_conductance': {'tokens': lc_tokens, 'attributions': lc_attr}\n",
        "            }\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Error during analysis: {e}\")\n",
        "        print(f\"\ud83d\udca1 Please check your text and try again\")\n",
        "        return None\n",
        "\n",
        "# Initialize dashboard\n",
        "dashboard_examples = create_interpretation_dashboard()\n",
        "\n",
        "# Example usage - analyze one of the provided examples\n",
        "print(f\"\\n\ud83d\ude80 Example Analysis:\")\n",
        "example_result = analyze_custom_text(dashboard_examples[0], show_visualizations=True)\n",
        "\n",
        "print(f\"\\n\ud83d\udca1 Usage Instructions:\")\n",
        "print(f\"   1. Call analyze_custom_text('your text here') to analyze any text\")\n",
        "print(f\"   2. Set show_visualizations=False for text-only results\")\n",
        "print(f\"   3. All analyses are automatically logged to TensorBoard\")\n",
        "print(f\"   4. Use the examples above as starting points for your own analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfaf Best Practices for Text Model Interpretation\n",
        "\n",
        "### Key Interpretation Methods Comparison\n",
        "\n",
        "| Method | Pros | Cons | Best Use Case |\n",
        "|--------|------|------|---------------|\n",
        "| **Integrated Gradients** | Most accurate, theoretically grounded | Computationally expensive | Critical interpretations, research |\n",
        "| **Input X Gradient** | Fast, simple implementation | Can be noisy, less accurate | Quick analysis, prototyping |\n",
        "| **Layer Conductance** | Shows internal representations | Requires layer selection | Understanding model internals |\n",
        "\n",
        "### \ud83d\udccb Interpretation Guidelines\n",
        "\n",
        "#### 1. **Multiple Methods Strategy**\n",
        "- Always use multiple interpretation methods\n",
        "- Look for consistency across methods\n",
        "- Investigate disagreements between methods\n",
        "\n",
        "#### 2. **Context Considerations**\n",
        "- Consider domain shift (IMDB \u2192 Australian tourism)\n",
        "- Account for vocabulary differences\n",
        "- Be aware of model limitations\n",
        "\n",
        "#### 3. **Multilingual Challenges**\n",
        "- Models trained on English may not handle other languages well\n",
        "- Attribution patterns may differ significantly across languages\n",
        "- Consider using multilingual models for non-English text\n",
        "\n",
        "#### 4. **Validation Techniques**\n",
        "- Test with known positive/negative examples\n",
        "- Verify attributions make semantic sense\n",
        "- Compare with human intuition\n",
        "\n",
        "### \ud83d\udea8 Common Pitfalls to Avoid\n",
        "\n",
        "1. **Over-interpreting Results**: Attributions show correlation, not causation\n",
        "2. **Ignoring Model Limitations**: Pre-trained models have domain biases\n",
        "3. **Single Method Reliance**: Always cross-validate with multiple methods\n",
        "4. **Vocabulary Mismatch**: Consider out-of-vocabulary effects\n",
        "\n",
        "### \ud83c\udfaf Australian Tourism Interpretation Insights\n",
        "\n",
        "From our analysis, we observed:\n",
        "\n",
        "- **Positive indicators**: 'beautiful', 'amazing', 'wonderful', 'perfect'\n",
        "- **Negative indicators**: 'terrible', 'disappointing', 'overpriced', 'boring'\n",
        "- **Location-specific**: Sydney Opera House, Melbourne coffee, Perth beaches\n",
        "- **Domain transfer**: IMDB model works reasonably well on tourism text\n",
        "\n",
        "### \ud83d\udcca TensorBoard for Interpretation Tracking\n",
        "\n",
        "Use TensorBoard to:\n",
        "- Track interpretation experiments over time\n",
        "- Compare different methods side-by-side\n",
        "- Monitor attribution statistics and distributions\n",
        "- Document analysis results with text summaries\n",
        "\n",
        "### \ud83d\udd2c Advanced Interpretation Techniques\n",
        "\n",
        "For deeper analysis, consider:\n",
        "- **Gradient SHAP**: Combines gradients with SHAP values\n",
        "- **DeepLift**: Attribution based on reference activations\n",
        "- **Occlusion**: Remove words to see impact on predictions\n",
        "- **LIME**: Local surrogate model explanations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udf89 Summary and Next Steps\n",
        "\n",
        "### What We Accomplished\n",
        "\n",
        "In this notebook, we successfully:\n",
        "\n",
        "\u2705 **Loaded and used a pre-trained IMDB CNN model** from Captum tutorials  \n",
        "\u2705 **Implemented multiple interpretation methods**: Integrated Gradients, Input X Gradient, Layer Conductance  \n",
        "\u2705 **Analyzed Australian tourism sentiment** with real-world examples  \n",
        "\u2705 **Created comprehensive visualizations** for attribution analysis  \n",
        "\u2705 **Explored multilingual interpretation** with English-Vietnamese examples  \n",
        "\u2705 **Integrated TensorBoard logging** for experiment tracking  \n",
        "\u2705 **Built an interactive dashboard** for custom text analysis  \n",
        "\n",
        "### Key Learning Outcomes\n",
        "\n",
        "\ud83e\udde0 **Understanding Model Behavior**: We learned how CNN models process sentiment in text  \n",
        "\ud83d\udd0d **Attribution Analysis**: We can now identify which words drive model predictions  \n",
        "\ud83c\udfa8 **Visualization Skills**: We created multiple types of interpretation visualizations  \n",
        "\ud83c\udf0f **Multilingual Awareness**: We understand challenges in cross-language interpretation  \n",
        "\ud83d\udcca **Experiment Tracking**: We learned to use TensorBoard for interpretation experiments  \n",
        "\n",
        "### Real-World Applications\n",
        "\n",
        "This knowledge enables you to:\n",
        "\n",
        "- **Debug model predictions** when they seem incorrect\n",
        "- **Build trust in AI systems** by explaining their decisions\n",
        "- **Identify model biases** and fairness issues\n",
        "- **Improve model performance** by understanding failure modes\n",
        "- **Comply with AI regulations** requiring explainable decisions\n",
        "\n",
        "### \ud83d\ude80 Next Steps for Advanced Interpretation\n",
        "\n",
        "#### 1. **Experiment with Other Captum Methods**\n",
        "```python\n",
        "from captum.attr import (\n",
        "    GradientShap,     # Gradient-based SHAP\n",
        "    DeepLift,         # Reference-based attribution\n",
        "    Occlusion,        # Perturbation-based\n",
        "    Saliency          # Simple gradient attribution\n",
        ")\n",
        "```\n",
        "\n",
        "#### 2. **Try Modern Transformer Models**\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from captum.attr import LayerIntegratedGradients\n",
        "\n",
        "# Load BERT model for more sophisticated analysis\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "```\n",
        "\n",
        "#### 3. **Explore Captum Insights (Interactive Dashboard)**\n",
        "```python\n",
        "from captum.insights import AttributionVisualizer, Batch\n",
        "\n",
        "# Create interactive web-based interpretation dashboard\n",
        "visualizer = AttributionVisualizer(\n",
        "    models=[model],\n",
        "    score_func=lambda o: torch.nn.functional.softmax(o, 1),\n",
        "    classes=['Negative', 'Positive']\n",
        ")\n",
        "```\n",
        "\n",
        "#### 4. **Build Domain-Specific Models**\n",
        "- Fine-tune models on Australian tourism data\n",
        "- Train multilingual models for better Vietnamese support\n",
        "- Create specialized models for specific use cases\n",
        "\n",
        "#### 5. **Integration with Production Systems**\n",
        "- Add interpretation to model serving APIs\n",
        "- Create automated interpretation reports\n",
        "- Build interpretation-aware model monitoring\n",
        "\n",
        "### \ud83d\udcda Additional Resources\n",
        "\n",
        "- **Captum Documentation**: https://captum.ai\n",
        "- **Captum Tutorials**: https://captum.ai/tutorials/\n",
        "- **Interpretable AI Research**: https://arxiv.org/abs/1909.01319\n",
        "- **PyTorch Model Interpretability**: https://pytorch.org/tutorials/beginner/Captum_Recipe.html\n",
        "\n",
        "### \ud83e\udd1d Contributing to the PyTorch Mastery Repository\n",
        "\n",
        "This notebook is part of the PyTorch Mastery learning repository focused on:\n",
        "- \ud83c\udde6\ud83c\uddfa Australian context examples\n",
        "- \ud83c\udf0f English-Vietnamese multilingual support\n",
        "- \ud83d\udd04 TensorFlow \u2192 PyTorch transition guidance\n",
        "- \ud83d\udcca Comprehensive TensorBoard integration\n",
        "\n",
        "Help improve this resource by:\n",
        "- Adding more Australian-specific examples\n",
        "- Expanding Vietnamese language support\n",
        "- Contributing additional interpretation methods\n",
        "- Sharing your own interpretation experiments\n",
        "\n",
        "---\n",
        "\n",
        "**\ud83c\udf93 Congratulations! You've mastered text model interpretation with Captum!** \ud83c\udde6\ud83c\uddfa\n",
        "\n",
        "*Ready to build more interpretable AI systems? The journey continues...*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}