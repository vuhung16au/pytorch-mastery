#!/usr/bin/env python3
"""
Complete the Deep Learning NLP notebook with neural network models and training
"""

import json

def add_neural_network_cells():
    """Add neural network model and training cells to the notebook."""
    
    # Load existing notebook
    with open("deep_learning_nlp.ipynb", "r") as f:
        notebook = json.load(f)
    
    # Cell 9: Australian Tourism Sentiment Classifier Model
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "class AustralianTourismSentimentClassifier(nn.Module):\n",
            "    \"\"\"\n",
            "    Neural network for Australian tourism sentiment analysis with multilingual support.\n",
            "    \n",
            "    Architecture:\n",
            "    - Embedding layer: Maps vocabulary words to dense vectors\n",
            "    - LSTM layer: Processes sequence of embeddings to capture context\n",
            "    - Dropout: Prevents overfitting with regularization\n",
            "    - Linear layers: Classify sentiment (negative, neutral, positive)\n",
            "    \n",
            "    TensorFlow equivalent:\n",
            "        model = tf.keras.Sequential([\n",
            "            tf.keras.layers.Embedding(vocab_size, embed_dim),\n",
            "            tf.keras.layers.LSTM(128, return_sequences=False),\n",
            "            tf.keras.layers.Dropout(0.3),\n",
            "            tf.keras.layers.Dense(64, activation='relu'),\n",
            "            tf.keras.layers.Dense(3, activation='softmax')\n",
            "        ])\n",
            "    \n",
            "    Args:\n",
            "        vocab_size (int): Size of vocabulary\n",
            "        embed_dim (int): Dimension of embedding vectors\n",
            "        hidden_dim (int): LSTM hidden dimension\n",
            "        num_classes (int): Number of sentiment classes (3)\n",
            "        dropout_rate (float): Dropout probability\n",
            "    \"\"\"\n",
            "    \n",
            "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_classes=3, dropout_rate=0.3):\n",
            "        super(AustralianTourismSentimentClassifier, self).__init__()\n",
            "        \n",
            "        # Store parameters\n",
            "        self.vocab_size = vocab_size\n",
            "        self.embed_dim = embed_dim\n",
            "        self.hidden_dim = hidden_dim\n",
            "        self.num_classes = num_classes\n",
            "        \n",
            "        # Embedding layer - converts token IDs to dense vectors\n",
            "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
            "        \n",
            "        # LSTM for sequence processing\n",
            "        self.lstm = nn.LSTM(\n",
            "            embed_dim, hidden_dim, \n",
            "            batch_first=True, \n",
            "            bidirectional=False,\n",
            "            dropout=dropout_rate if hidden_dim > 1 else 0\n",
            "        )\n",
            "        \n",
            "        # Dropout for regularization\n",
            "        self.dropout = nn.Dropout(dropout_rate)\n",
            "        \n",
            "        # Classification head\n",
            "        self.classifier = nn.Sequential(\n",
            "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
            "            nn.ReLU(),\n",
            "            nn.Dropout(dropout_rate),\n",
            "            nn.Linear(hidden_dim // 2, num_classes)\n",
            "        )\n",
            "        \n",
            "        # Initialize weights\n",
            "        self._init_weights()\n",
            "        \n",
            "        # Sentiment labels for interpretation\n",
            "        self.sentiment_labels = ['negative', 'neutral', 'positive']\n",
            "        \n",
            "        # Australian cities for context\n",
            "        self.australian_cities = [\"Sydney\", \"Melbourne\", \"Brisbane\", \"Perth\", \n",
            "                                \"Adelaide\", \"Darwin\", \"Hobart\", \"Canberra\"]\n",
            "    \n",
            "    def _init_weights(self):\n",
            "        \"\"\"Initialize model weights - manual in PyTorch, automatic in TensorFlow\"\"\"\n",
            "        for name, param in self.named_parameters():\n",
            "            if 'weight' in name:\n",
            "                if len(param.shape) > 1:\n",
            "                    nn.init.xavier_uniform_(param)\n",
            "                else:\n",
            "                    nn.init.zeros_(param)\n",
            "            elif 'bias' in name:\n",
            "                nn.init.zeros_(param)\n",
            "    \n",
            "    def forward(self, x):\n",
            "        \"\"\"\n",
            "        Forward pass through the network.\n",
            "        \n",
            "        Args:\n",
            "            x (torch.Tensor): Input token IDs [batch_size, seq_len]\n",
            "        \n",
            "        Returns:\n",
            "            torch.Tensor: Logits for sentiment classification [batch_size, num_classes]\n",
            "        \"\"\"\n",
            "        batch_size, seq_len = x.shape\n",
            "        \n",
            "        # Embedding lookup: [batch_size, seq_len] -> [batch_size, seq_len, embed_dim]\n",
            "        embedded = self.embedding(x)\n",
            "        \n",
            "        # LSTM processing: [batch_size, seq_len, embed_dim] -> [batch_size, seq_len, hidden_dim]\n",
            "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
            "        \n",
            "        # Use last hidden state for classification: [batch_size, hidden_dim]\n",
            "        last_hidden = hidden[-1]  # Last layer's hidden state\n",
            "        \n",
            "        # Apply dropout\n",
            "        dropped = self.dropout(last_hidden)\n",
            "        \n",
            "        # Classification: [batch_size, hidden_dim] -> [batch_size, num_classes]\n",
            "        logits = self.classifier(dropped)\n",
            "        \n",
            "        return logits\n",
            "    \n",
            "    def predict_sentiment(self, text_tensor, preprocessor):\n",
            "        \"\"\"\n",
            "        Predict sentiment for Australian tourism text.\n",
            "        \n",
            "        Args:\n",
            "            text_tensor (torch.Tensor): Encoded text tensor\n",
            "            preprocessor: Text preprocessor for decoding\n",
            "        \n",
            "        Returns:\n",
            "            dict: Prediction results with probabilities\n",
            "        \"\"\"\n",
            "        self.eval()\n",
            "        with torch.no_grad():\n",
            "            if text_tensor.dim() == 1:\n",
            "                text_tensor = text_tensor.unsqueeze(0)  # Add batch dimension\n",
            "            \n",
            "            logits = self.forward(text_tensor.to(next(self.parameters()).device))\n",
            "            probabilities = F.softmax(logits, dim=1)\n",
            "            predicted_class = torch.argmax(probabilities, dim=1).item()\n",
            "            confidence = probabilities[0][predicted_class].item()\n",
            "            \n",
            "            return {\n",
            "                'sentiment': self.sentiment_labels[predicted_class],\n",
            "                'confidence': confidence,\n",
            "                'probabilities': {\n",
            "                    label: prob.item() \n",
            "                    for label, prob in zip(self.sentiment_labels, probabilities[0])\n",
            "                }\n",
            "            }\n",
            "    \n",
            "    def get_model_info(self) -> dict:\n",
            "        \"\"\"Return model architecture information.\"\"\"\n",
            "        total_params = sum(p.numel() for p in self.parameters())\n",
            "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
            "        \n",
            "        return {\n",
            "            'model_name': 'Australian Tourism Sentiment Classifier',\n",
            "            'vocab_size': self.vocab_size,\n",
            "            'embed_dim': self.embed_dim,\n",
            "            'hidden_dim': self.hidden_dim,\n",
            "            'num_classes': self.num_classes,\n",
            "            'total_params': total_params,\n",
            "            'trainable_params': trainable_params,\n",
            "            'target_cities': ', '.join(self.australian_cities[:4]) + '...'\n",
            "        }\n",
            "\n",
            "# Initialize the model with device-aware setup\n",
            "model = AustralianTourismSentimentClassifier(\n",
            "    vocab_size=preprocessor.vocab_size,\n",
            "    embed_dim=128,\n",
            "    hidden_dim=256,\n",
            "    num_classes=3,\n",
            "    dropout_rate=0.3\n",
            ").to(DEVICE)\n",
            "\n",
            "# Display model information\n",
            "model_info = model.get_model_info()\n",
            "print(\"ğŸ—ï¸ Australian Tourism Sentiment Classifier\")\n",
            "print(\"=\" * 50)\n",
            "for key, value in model_info.items():\n",
            "    print(f\"   {key.replace('_', ' ').title()}: {value}\")\n",
            "\n",
            "print(f\"\\nğŸ“± Model device: {next(model.parameters()).device}\")\n",
            "print(f\"ğŸ¯ Target: Classify sentiment of Australian tourism reviews\")\n",
            "print(f\"ğŸŒ Languages: English and Vietnamese\")\n",
            "\n",
            "# Model summary (similar to TensorFlow model.summary())\n",
            "print(f\"\\nğŸ”§ Model Architecture:\")\n",
            "print(model)"
        ]
    })
    
    # Cell 10: Training Setup with TensorBoard
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "import time\n",
            "from datetime import datetime\n",
            "\n",
            "# Platform-specific TensorBoard log directory setup\n",
            "def get_run_logdir(name=\"australian_sentiment\"):\n",
            "    \"\"\"Create unique log directory for TensorBoard.\"\"\"\n",
            "    if IS_COLAB:\n",
            "        root_logdir = \"/content/tensorboard_logs\"\n",
            "    elif IS_KAGGLE:\n",
            "        root_logdir = \"./tensorboard_logs\"\n",
            "    else:\n",
            "        root_logdir = \"./tensorboard_logs\"\n",
            "    \n",
            "    os.makedirs(root_logdir, exist_ok=True)\n",
            "    timestamp = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
            "    return os.path.join(root_logdir, f\"{name}_{timestamp}\")\n",
            "\n",
            "# Setup training configuration\n",
            "class TrainingConfig:\n",
            "    \"\"\"Training configuration for Australian sentiment classifier.\"\"\"\n",
            "    \n",
            "    def __init__(self, device):\n",
            "        self.device = device\n",
            "        \n",
            "        # Adjust hyperparameters based on device\n",
            "        if device.type == 'cuda':\n",
            "            self.epochs = 20\n",
            "            self.learning_rate = 0.001\n",
            "            self.batch_size = 32\n",
            "        elif device.type == 'mps':\n",
            "            self.epochs = 15\n",
            "            self.learning_rate = 0.001\n",
            "            self.batch_size = 16\n",
            "        else:  # CPU\n",
            "            self.epochs = 10\n",
            "            self.learning_rate = 0.002\n",
            "            self.batch_size = 8\n",
            "        \n",
            "        self.weight_decay = 1e-4\n",
            "        self.patience = 5  # Early stopping patience\n",
            "        self.log_interval = 10  # Log every N batches\n",
            "\n",
            "config = TrainingConfig(DEVICE)\n",
            "\n",
            "# Setup loss function and optimizer\n",
            "criterion = nn.CrossEntropyLoss()  # TensorFlow: loss='sparse_categorical_crossentropy'\n",
            "optimizer = optim.Adam(\n",
            "    model.parameters(), \n",
            "    lr=config.learning_rate, \n",
            "    weight_decay=config.weight_decay\n",
            ")  # TensorFlow: optimizer='adam'\n",
            "\n",
            "# Learning rate scheduler\n",
            "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
            "    optimizer, mode='max', factor=0.5, patience=3, verbose=True\n",
            ")\n",
            "\n",
            "# TensorBoard setup\n",
            "run_logdir = get_run_logdir(\"australian_tourism_sentiment\")\n",
            "writer = SummaryWriter(log_dir=run_logdir)\n",
            "\n",
            "print(\"âš™ï¸ Training Configuration\")\n",
            "print(\"=\" * 40)\n",
            "print(f\"   Device: {config.device}\")\n",
            "print(f\"   Epochs: {config.epochs}\")\n",
            "print(f\"   Learning Rate: {config.learning_rate}\")\n",
            "print(f\"   Batch Size: {config.batch_size}\")\n",
            "print(f\"   Weight Decay: {config.weight_decay}\")\n",
            "print(f\"   Early Stopping Patience: {config.patience}\")\n",
            "\n",
            "print(f\"\\nğŸ“Š TensorBoard Logging:\")\n",
            "print(f\"   Log Directory: {run_logdir}\")\n",
            "print(f\"   Logging Interval: Every {config.log_interval} batches\")\n",
            "\n",
            "# Device-specific optimizations\n",
            "if DEVICE.type == 'cuda':\n",
            "    torch.backends.cudnn.benchmark = True\n",
            "    print(f\"\\nğŸ”§ CUDA optimizations enabled\")\n",
            "elif DEVICE.type == 'mps':\n",
            "    print(f\"\\nğŸ”§ MPS device detected - optimizing for Apple Silicon\")\n",
            "else:\n",
            "    torch.set_num_threads(torch.get_num_threads())\n",
            "    print(f\"\\nğŸ”§ CPU optimization: Using {torch.get_num_threads()} threads\")\n",
            "\n",
            "print(f\"\\nğŸš€ Ready to train Australian tourism sentiment classifier!\")"
        ]
    })
    
    # Cell 11: Training Loop with TensorBoard Integration
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "def train_australian_sentiment_model(model, train_loader, val_loader, config, criterion, optimizer, scheduler, writer):\n",
            "    \"\"\"\n",
            "    Train the Australian tourism sentiment classifier with comprehensive logging.\n",
            "    \n",
            "    TensorFlow equivalent:\n",
            "        history = model.fit(\n",
            "            train_data, epochs=epochs, \n",
            "            validation_data=val_data,\n",
            "            callbacks=[tensorboard_callback]\n",
            "        )\n",
            "    \n",
            "    Args:\n",
            "        model: PyTorch model to train\n",
            "        train_loader: Training data loader\n",
            "        val_loader: Validation data loader\n",
            "        config: Training configuration\n",
            "        criterion: Loss function\n",
            "        optimizer: Optimizer\n",
            "        scheduler: Learning rate scheduler\n",
            "        writer: TensorBoard writer\n",
            "    \n",
            "    Returns:\n",
            "        dict: Training history with metrics\n",
            "    \"\"\"\n",
            "    \n",
            "    # Training history\n",
            "    history = {\n",
            "        'train_loss': [],\n",
            "        'train_acc': [],\n",
            "        'val_loss': [],\n",
            "        'val_acc': [],\n",
            "        'learning_rates': []\n",
            "    }\n",
            "    \n",
            "    best_val_acc = 0.0\n",
            "    patience_counter = 0\n",
            "    \n",
            "    print(\"ğŸ‹ï¸ Training Australian Tourism Sentiment Classifier\")\n",
            "    print(\"=\" * 60)\n",
            "    print(f\"ğŸ“ Sample predictions target:\")\n",
            "    print(f\"   ğŸ‡¦ğŸ‡º 'Sydney Opera House is amazing!' -> Positive\")\n",
            "    print(f\"   ğŸ‡»ğŸ‡³ 'CÃ  phÃª Melbourne Ä‘áº¯t quÃ¡' -> Negative\")\n",
            "    print(f\"   ğŸ‡¦ğŸ‡º 'Perth beaches are okay' -> Neutral\")\n",
            "    print(\"\\n\" + \"=\" * 60)\n",
            "    \n",
            "    for epoch in range(config.epochs):\n",
            "        start_time = time.time()\n",
            "        \n",
            "        # Training phase\n",
            "        model.train()\n",
            "        train_loss = 0.0\n",
            "        train_correct = 0\n",
            "        train_total = 0\n",
            "        \n",
            "        for batch_idx, (data, target) in enumerate(train_loader):\n",
            "            # Move data to device\n",
            "            data, target = data.to(config.device), target.to(config.device)\n",
            "            \n",
            "            # Zero gradients (required in PyTorch, automatic in TensorFlow)\n",
            "            optimizer.zero_grad()\n",
            "            \n",
            "            # Forward pass\n",
            "            outputs = model(data)\n",
            "            loss = criterion(outputs, target)\n",
            "            \n",
            "            # Backward pass (explicit in PyTorch)\n",
            "            loss.backward()\n",
            "            \n",
            "            # Gradient clipping to prevent exploding gradients\n",
            "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
            "            \n",
            "            # Update parameters\n",
            "            optimizer.step()\n",
            "            \n",
            "            # Calculate metrics\n",
            "            train_loss += loss.item()\n",
            "            _, predicted = torch.max(outputs.data, 1)\n",
            "            train_total += target.size(0)\n",
            "            train_correct += (predicted == target).sum().item()\n",
            "            \n",
            "            # Log to TensorBoard every N batches\n",
            "            if batch_idx % config.log_interval == 0:\n",
            "                step = epoch * len(train_loader) + batch_idx\n",
            "                writer.add_scalar('Loss/Train_Batch', loss.item(), step)\n",
            "                writer.add_scalar('Accuracy/Train_Batch', \n",
            "                                (predicted == target).float().mean().item(), step)\n",
            "                \n",
            "                # Log device-specific metrics\n",
            "                if config.device.type == 'cuda':\n",
            "                    gpu_memory_used = torch.cuda.memory_allocated(config.device) / 1024**3\n",
            "                    writer.add_scalar('Memory/GPU_Used_GB', gpu_memory_used, step)\n",
            "        \n",
            "        # Calculate epoch training metrics\n",
            "        train_loss = train_loss / len(train_loader)\n",
            "        train_acc = train_correct / train_total\n",
            "        \n",
            "        # Validation phase\n",
            "        model.eval()\n",
            "        val_loss = 0.0\n",
            "        val_correct = 0\n",
            "        val_total = 0\n",
            "        all_predictions = []\n",
            "        all_targets = []\n",
            "        \n",
            "        with torch.no_grad():\n",
            "            for data, target in val_loader:\n",
            "                data, target = data.to(config.device), target.to(config.device)\n",
            "                outputs = model(data)\n",
            "                loss = criterion(outputs, target)\n",
            "                \n",
            "                val_loss += loss.item()\n",
            "                _, predicted = torch.max(outputs.data, 1)\n",
            "                val_total += target.size(0)\n",
            "                val_correct += (predicted == target).sum().item()\n",
            "                \n",
            "                # Store for detailed metrics\n",
            "                all_predictions.extend(predicted.cpu().numpy())\n",
            "                all_targets.extend(target.cpu().numpy())\n",
            "        \n",
            "        val_loss = val_loss / len(val_loader)\n",
            "        val_acc = val_correct / val_total\n",
            "        current_lr = optimizer.param_groups[0]['lr']\n",
            "        \n",
            "        # Update learning rate scheduler\n",
            "        scheduler.step(val_acc)\n",
            "        \n",
            "        # Store history\n",
            "        history['train_loss'].append(train_loss)\n",
            "        history['train_acc'].append(train_acc)\n",
            "        history['val_loss'].append(val_loss)\n",
            "        history['val_acc'].append(val_acc)\n",
            "        history['learning_rates'].append(current_lr)\n",
            "        \n",
            "        # Log epoch metrics to TensorBoard\n",
            "        writer.add_scalar('Loss/Train_Epoch', train_loss, epoch)\n",
            "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
            "        writer.add_scalar('Accuracy/Train_Epoch', train_acc, epoch)\n",
            "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
            "        writer.add_scalar('Learning_Rate', current_lr, epoch)\n",
            "        \n",
            "        # Log model parameters histogram\n",
            "        for name, param in model.named_parameters():\n",
            "            if param.grad is not None:\n",
            "                writer.add_histogram(f'Parameters/{name}', param, epoch)\n",
            "                writer.add_histogram(f'Gradients/{name}', param.grad, epoch)\n",
            "        \n",
            "        # Calculate epoch time\n",
            "        epoch_time = time.time() - start_time\n",
            "        \n",
            "        # Print progress\n",
            "        print(f'Epoch {epoch+1:2d}/{config.epochs}: '\n",
            "              f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | '\n",
            "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} | '\n",
            "              f'LR: {current_lr:.6f} | Time: {epoch_time:.1f}s')\n",
            "        \n",
            "        # Early stopping check\n",
            "        if val_acc > best_val_acc:\n",
            "            best_val_acc = val_acc\n",
            "            patience_counter = 0\n",
            "            # Save best model\n",
            "            torch.save(model.state_dict(), 'best_australian_sentiment_model.pth')\n",
            "            print(f'   ğŸ¯ New best validation accuracy: {best_val_acc:.4f}')\n",
            "        else:\n",
            "            patience_counter += 1\n",
            "            if patience_counter >= config.patience:\n",
            "                print(f'\\nâ° Early stopping triggered after {epoch+1} epochs')\n",
            "                print(f'   Best validation accuracy: {best_val_acc:.4f}')\n",
            "                break\n",
            "    \n",
            "    writer.close()\n",
            "    \n",
            "    print(f\"\\nğŸ‰ Training completed!\")\n",
            "    print(f\"   Final validation accuracy: {val_acc:.4f}\")\n",
            "    print(f\"   Best validation accuracy: {best_val_acc:.4f}\")\n",
            "    print(f\"   Model saved as: best_australian_sentiment_model.pth\")\n",
            "    \n",
            "    return history\n",
            "\n",
            "# Start training\n",
            "print(\"ğŸš€ Starting training of Australian Tourism Sentiment Classifier...\")\n",
            "training_history = train_australian_sentiment_model(\n",
            "    model, train_loader, val_loader, config, criterion, optimizer, scheduler, writer\n",
            ")"
        ]
    })
    
    # Save the completed notebook
    with open("deep_learning_nlp.ipynb", "w") as f:
        json.dump(notebook, f, indent=2)
    return notebook

if __name__ == "__main__":
    add_neural_network_cells()
    print("âœ… Neural network cells added to deep learning NLP notebook!")