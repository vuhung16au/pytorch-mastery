{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLP From Scratch: Classifying Names with a Character-Level RNN \ud83c\udde6\ud83c\uddfa\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/pytorch-mastery/blob/main/examples/pytorch-nlp/classify-names-character-level-RNN.ipynb)\n",
        "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/pytorch-mastery/blob/main/examples/pytorch-nlp/classify-names-character-level-RNN.ipynb)\n",
        "\n",
        "A comprehensive introduction to character-level Recurrent Neural Networks (RNNs) for name classification using PyTorch, featuring Australian names and locations with Vietnamese multilingual support.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "- \ud83d\udd24 **Master character-level text processing** with PyTorch\n",
        "- \ud83e\udde0 **Build RNN from scratch** for sequence classification\n",
        "- \ud83c\udde6\ud83c\uddfa **Classify Australian names and locations** by origin/type\n",
        "- \ud83c\udf0f **Handle multilingual text** with English-Vietnamese examples\n",
        "- \ud83d\udd04 **Compare with TensorFlow** approaches for RNN implementation\n",
        "- \ud83d\udcca **Implement comprehensive logging** with TensorBoard\n",
        "\n",
        "## What You'll Build\n",
        "\n",
        "1. **Australian Name Origin Classifier** - Classify names by ethnic origin (English, Irish, Greek, Vietnamese, etc.)\n",
        "2. **Location Type Classifier** - Distinguish between cities, suburbs, landmarks, and natural features\n",
        "3. **Character-level RNN Architecture** - Build vanilla RNN, LSTM, and GRU variants\n",
        "4. **Multilingual Support** - Handle both English and Vietnamese character sets\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment Detection and Setup\n",
        "import sys\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Detect the runtime environment\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules or \"kaggle\" in os.environ.get('KAGGLE_URL_BASE', '')\n",
        "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
        "\n",
        "print(f\"Environment detected:\")\n",
        "print(f\"  - Local: {IS_LOCAL}\")\n",
        "print(f\"  - Google Colab: {IS_COLAB}\")\n",
        "print(f\"  - Kaggle: {IS_KAGGLE}\")\n",
        "\n",
        "# Platform-specific system setup\n",
        "if IS_COLAB:\n",
        "    print(\"\\nSetting up Google Colab environment...\")\n",
        "    !apt update -qq\n",
        "    !apt install -y -qq software-properties-common\n",
        "elif IS_KAGGLE:\n",
        "    print(\"\\nSetting up Kaggle environment...\")\n",
        "    # Kaggle usually has most packages pre-installed\n",
        "else:\n",
        "    print(\"\\nSetting up local environment...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages for this notebook\n",
        "required_packages = [\n",
        "    \"torch\",\n",
        "    \"pandas\",\n",
        "    \"seaborn\",\n",
        "    \"matplotlib\",\n",
        "    \"tensorboard\",\n",
        "    \"scikit-learn\",\n",
        "    \"numpy\"\n",
        "]\n",
        "\n",
        "print(\"Installing required packages...\")\n",
        "for package in required_packages:\n",
        "    if IS_COLAB or IS_KAGGLE:\n",
        "        !pip install -q {package}\n",
        "    else:\n",
        "        try:\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package],\n",
        "                          capture_output=True, check=True)\n",
        "        except subprocess.CalledProcessError:\n",
        "            print(f\"Note: {package} installation skipped (likely already installed)\")\n",
        "    print(f\"\u2713 {package}\")\n",
        "\n",
        "print(\"\\n\ud83d\udce6 Package installation completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import essential libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Data handling and visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Text processing and utilities\n",
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "import time\n",
        "from datetime import datetime\n",
        "import platform\n",
        "\n",
        "# Set style for better notebook aesthetics\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(f\"\u2705 PyTorch {torch.__version__} ready!\")\n",
        "print(f\"\ud83d\udcca Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_device():\n",
        "    \"\"\"\n",
        "    Detect the best available PyTorch device with comprehensive hardware support.\n",
        "    \n",
        "    Priority order:\n",
        "    1. CUDA (NVIDIA GPUs) - Best performance for deep learning\n",
        "    2. MPS (Apple Silicon) - Optimized for M1/M2/M3 Macs  \n",
        "    3. CPU (Universal) - Always available fallback\n",
        "    \n",
        "    Returns:\n",
        "        torch.device: The optimal device for PyTorch operations\n",
        "        str: Human-readable device description for logging\n",
        "    \"\"\"\n",
        "    # Check for CUDA (NVIDIA GPU)\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        device_info = f\"CUDA GPU: {gpu_name}\"\n",
        "        \n",
        "        # Additional CUDA info for optimization\n",
        "        cuda_version = torch.version.cuda\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "        \n",
        "        print(f\"\ud83d\ude80 Using CUDA acceleration\")\n",
        "        print(f\"   GPU: {gpu_name}\")\n",
        "        print(f\"   CUDA Version: {cuda_version}\")\n",
        "        print(f\"   GPU Memory: {gpu_memory:.1f} GB\")\n",
        "        \n",
        "        return device, device_info\n",
        "    \n",
        "    # Check for MPS (Apple Silicon)\n",
        "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "        device_info = \"Apple Silicon MPS\"\n",
        "        \n",
        "        # Get system info for Apple Silicon\n",
        "        system_info = platform.uname()\n",
        "        \n",
        "        print(f\"\ud83c\udf4e Using Apple Silicon MPS acceleration\")\n",
        "        print(f\"   System: {system_info.system} {system_info.release}\")\n",
        "        print(f\"   Machine: {system_info.machine}\")\n",
        "        print(f\"   Processor: {system_info.processor}\")\n",
        "        \n",
        "        return device, device_info\n",
        "    \n",
        "    # Fallback to CPU\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        device_info = \"CPU (No GPU acceleration available)\"\n",
        "        \n",
        "        # Get CPU info for optimization guidance\n",
        "        cpu_count = torch.get_num_threads()\n",
        "        system_info = platform.uname()\n",
        "        \n",
        "        print(f\"\ud83d\udcbb Using CPU (no GPU acceleration detected)\")\n",
        "        print(f\"   Processor: {system_info.processor}\")\n",
        "        print(f\"   PyTorch Threads: {cpu_count}\")\n",
        "        print(f\"   System: {system_info.system} {system_info.release}\")\n",
        "        \n",
        "        # Provide optimization suggestions for CPU-only setups\n",
        "        print(f\"\\n\ud83d\udca1 CPU Optimization Tips:\")\n",
        "        print(f\"   \u2022 Reduce batch size to prevent memory issues\")\n",
        "        print(f\"   \u2022 Consider using smaller models for faster training\")\n",
        "        print(f\"   \u2022 Enable PyTorch optimizations: torch.set_num_threads({cpu_count})\")\n",
        "        \n",
        "        return device, device_info\n",
        "\n",
        "# Usage in all PyTorch notebooks\n",
        "device, device_info = detect_device()\n",
        "print(f\"\\n\u2705 PyTorch device selected: {device}\")\n",
        "print(f\"\ud83d\udcca Device info: {device_info}\")\n",
        "\n",
        "# Set global device for the notebook\n",
        "DEVICE = device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udde6\ud83c\uddfa Australian Names and Locations Dataset\n",
        "\n",
        "We'll create a comprehensive dataset featuring Australian names classified by ethnic origin, and Australian locations classified by type. This follows the repository's Australian context policy while providing practical multilingual examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_australian_names_dataset():\n",
        "    \"\"\"\n",
        "    Create Australian names dataset with ethnic origin classification.\n",
        "    Includes both English and Vietnamese names commonly found in Australia.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Australian names by ethnic origin\n",
        "    names_by_origin = {\n",
        "        'English': [\n",
        "            'Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Wilson', 'Davis', 'Taylor',\n",
        "            'Anderson', 'Thomas', 'Jackson', 'White', 'Harris', 'Martin', 'Thompson',\n",
        "            'Garcia', 'Martinez', 'Robinson', 'Clark', 'Lewis', 'Walker', 'Hall',\n",
        "            'Allen', 'Young', 'King', 'Wright', 'Lopez', 'Hill', 'Scott', 'Green',\n",
        "            'Adams', 'Baker', 'Gonzalez', 'Nelson', 'Carter', 'Mitchell', 'Perez'\n",
        "        ],\n",
        "        'Irish': [\n",
        "            'Murphy', 'Kelly', 'Sullivan', 'Walsh', 'Smith', 'OBrien', 'Byrne', 'Ryan',\n",
        "            'Connor', 'ONeill', 'Reilly', 'Doyle', 'McCarthy', 'Gallagher', 'Doherty',\n",
        "            'Kennedy', 'Lynch', 'Murray', 'Quinn', 'Moore', 'McLaughlin', 'Carroll',\n",
        "            'Connolly', 'Daly', 'Connell', 'Wilson', 'Dunne', 'Brennan', 'Burke',\n",
        "            'Collins', 'Campbell', 'Clarke', 'Johnston', 'Hughes', 'Farrell'\n",
        "        ],\n",
        "        'Italian': [\n",
        "            'Rossi', 'Russo', 'Ferrari', 'Esposito', 'Bianchi', 'Romano', 'Colombo',\n",
        "            'Ricci', 'Marino', 'Greco', 'Bruno', 'Gallo', 'Conti', 'DeLuca',\n",
        "            'Mancini', 'Costa', 'Giordano', 'Rizzo', 'Lombardi', 'Moretti',\n",
        "            'Barbieri', 'Fontana', 'Santoro', 'Mariani', 'Rinaldi', 'Caruso',\n",
        "            'Ferrara', 'Galli', 'Martini', 'Leone', 'Longo', 'Gentile', 'Martinelli'\n",
        "        ],\n",
        "        'Greek': [\n",
        "            'Papadopoulos', 'Georgiou', 'Dimitriou', 'Andreou', 'Nikolaou', 'Christou',\n",
        "            'Ioannou', 'Constantinou', 'Antoniou', 'Savva', 'Charalambous', 'Stylianou',\n",
        "            'Petrou', 'Michaelidou', 'Hadjisavvas', 'Kokkinos', 'Stavrou', 'Loizou',\n",
        "            'Panayiotou', 'Economou', 'Demetriou', 'Philippou', 'Vassiliou', 'Kyprianou',\n",
        "            'Theodorou', 'Christodoulou', 'Anastasiadou', 'Hadjiconstantinou'\n",
        "        ],\n",
        "        'Vietnamese': [\n",
        "            'Nguyen', 'Tran', 'Le', 'Pham', 'Hoang', 'Huynh', 'Vo', 'Vu', 'Dang', 'Bui',\n",
        "            'Do', 'Ho', 'Ngo', 'Duong', 'Ly', 'Trinh', 'Dinh', 'Thai', 'Cao', 'Lam',\n",
        "            'Phan', 'Truong', 'Tang', 'Doan', 'Mai', 'Ton', 'Ha', 'Chau', 'Bach', 'Kim',\n",
        "            'Luu', 'Ong', 'Tong', 'Quan', 'Dam', 'Khang', 'Thang', 'Phung', 'Duc', 'Vinh'\n",
        "        ],\n",
        "        'Chinese': [\n",
        "            'Wang', 'Li', 'Zhang', 'Liu', 'Chen', 'Yang', 'Huang', 'Zhao', 'Wu', 'Zhou',\n",
        "            'Xu', 'Sun', 'Ma', 'Zhu', 'Hu', 'Guo', 'He', 'Lin', 'Gao', 'Luo',\n",
        "            'Zheng', 'Liang', 'Xie', 'Tang', 'Song', 'Xu', 'Han', 'Feng', 'Deng', 'Cao',\n",
        "            'Peng', 'Zeng', 'Xiao', 'Tian', 'Pan', 'Cheng', 'Wei', 'Jiang', 'Yu', 'Shi'\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    # Flatten the dataset\n",
        "    names_data = []\n",
        "    for origin, names in names_by_origin.items():\n",
        "        for name in names:\n",
        "            names_data.append((name, origin))\n",
        "    \n",
        "    return names_data\n",
        "\n",
        "def create_australian_locations_dataset():\n",
        "    \"\"\"\n",
        "    Create Australian locations dataset classified by type.\n",
        "    Includes cities, suburbs, landmarks, and natural features.\n",
        "    \"\"\"\n",
        "    \n",
        "    locations_by_type = {\n",
        "        'City': [\n",
        "            'Sydney', 'Melbourne', 'Brisbane', 'Perth', 'Adelaide', 'Darwin', 'Hobart', 'Canberra',\n",
        "            'Newcastle', 'Wollongong', 'Geelong', 'Townsville', 'Cairns', 'Ballarat', 'Bendigo',\n",
        "            'Albury', 'Wodonga', 'Shepparton', 'Wagga', 'Rockhampton', 'Bundaberg', 'Hervey',\n",
        "            'Toowoomba', 'Mackay', 'Gladstone', 'Warrnambool', 'Mildura', 'Launceston'\n",
        "        ],\n",
        "        'Suburb': [\n",
        "            'Bondi', 'Manly', 'Paddington', 'Surry', 'Newtown', 'Leichhardt', 'Balmain',\n",
        "            'Toorak', 'Brighton', 'Camberwell', 'Hawthorn', 'Richmond', 'Fitzroy', 'Carlton',\n",
        "            'Southbank', 'Docklands', 'Fortitude', 'Paddington', 'Milton', 'Ascot',\n",
        "            'Cottesloe', 'Subiaco', 'Fremantle', 'Scarborough', 'Joondalup', 'Midland'\n",
        "        ],\n",
        "        'Landmark': [\n",
        "            'Opera', 'Harbour', 'Luna', 'Royal', 'Federation', 'Parliament', 'Story', 'Shrine',\n",
        "            'Botanic', 'Observatory', 'Anzac', 'War', 'National', 'Australian', 'Museum',\n",
        "            'Gallery', 'Library', 'University', 'Stadium', 'Arena', 'Centre', 'Tower',\n",
        "            'Bridge', 'Wharf', 'Market', 'Square', 'Gardens', 'Reserve', 'Park'\n",
        "        ],\n",
        "        'Natural': [\n",
        "            'Uluru', 'Kakadu', 'Daintree', 'Grampians', 'Flinders', 'Cradle', 'Freycinet',\n",
        "            'Wilsons', 'Kosciuszko', 'Alpine', 'Snowy', 'Murray', 'Darling', 'Cooper',\n",
        "            'Murrumbidgee', 'Lachlan', 'Macquarie', 'Hawkesbury', 'Yarra', 'Maribyrnong',\n",
        "            'Barwon', 'Goulburn', 'Campaspe', 'Loddon', 'Wimmera', 'Glenelg', 'Torrens'\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    # Flatten the dataset\n",
        "    locations_data = []\n",
        "    for location_type, locations in locations_by_type.items():\n",
        "        for location in locations:\n",
        "            locations_data.append((location, location_type))\n",
        "    \n",
        "    return locations_data\n",
        "\n",
        "# Create the datasets\n",
        "names_data = create_australian_names_dataset()\n",
        "locations_data = create_australian_locations_dataset()\n",
        "\n",
        "print(\"\ud83c\udde6\ud83c\uddfa Australian Names and Locations Dataset Created\")\n",
        "print(\"=\" * 55)\n",
        "print(f\"   Names dataset: {len(names_data)} entries\")\n",
        "print(f\"   Locations dataset: {len(locations_data)} entries\")\n",
        "\n",
        "# Show sample data\n",
        "print(\"\\n\ud83d\udcdd Sample Names by Origin:\")\n",
        "names_df = pd.DataFrame(names_data, columns=['Name', 'Origin'])\n",
        "for origin in ['English', 'Vietnamese', 'Greek']:\n",
        "    samples = names_df[names_df['Origin'] == origin]['Name'].head(3).tolist()\n",
        "    print(f\"   {origin}: {', '.join(samples)}\")\n",
        "\n",
        "print(\"\\n\ud83c\udfdb\ufe0f Sample Locations by Type:\")\n",
        "locations_df = pd.DataFrame(locations_data, columns=['Location', 'Type'])\n",
        "for loc_type in ['City', 'Suburb', 'Landmark', 'Natural']:\n",
        "    samples = locations_df[locations_df['Type'] == loc_type]['Location'].head(3).tolist()\n",
        "    print(f\"   {loc_type}: {', '.join(samples)}\")\n",
        "\n",
        "# Distribution visualization\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Names distribution\n",
        "sns.countplot(data=names_df, x='Origin', ax=ax1)\n",
        "ax1.set_title('Australian Names by Ethnic Origin')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Locations distribution\n",
        "sns.countplot(data=locations_df, x='Type', ax=ax2)\n",
        "ax2.set_title('Australian Locations by Type')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\u2705 Datasets prepared for character-level RNN training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd24 Character-Level Text Processing\n",
        "\n",
        "Character-level processing is fundamental to our RNN. We'll create utilities to:\n",
        "\n",
        "1. **Build character vocabulary** from both English and Vietnamese text\n",
        "2. **Convert text to tensors** and vice versa\n",
        "3. **Handle Unicode characters** for multilingual support\n",
        "4. **Normalize text** for consistent processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CharacterProcessor:\n",
        "    \"\"\"\n",
        "    Character-level text processing for Australian multilingual names and locations.\n",
        "    \n",
        "    Handles both ASCII and Unicode characters for English-Vietnamese support.\n",
        "    Comparable to TensorFlow's text preprocessing but with explicit character control.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.all_letters = string.ascii_letters + \" .,;'-\"\n",
        "        self.n_letters = len(self.all_letters)\n",
        "        self.letter_to_index = {}\n",
        "        self.index_to_letter = {}\n",
        "        \n",
        "    def unicode_to_ascii(self, text):\n",
        "        \"\"\"\n",
        "        Convert Unicode characters to ASCII for Vietnamese names.\n",
        "        \n",
        "        Examples:\n",
        "        - 'Nguy\u1ec5n' -> 'Nguyen'\n",
        "        - 'Tr\u1ea7n' -> 'Tran'\n",
        "        \"\"\"\n",
        "        return ''.join(\n",
        "            c for c in unicodedata.normalize('NFD', text)\n",
        "            if unicodedata.category(c) != 'Mn'\n",
        "            and c in self.all_letters\n",
        "        )\n",
        "    \n",
        "    def build_vocabulary(self, text_data):\n",
        "        \"\"\"\n",
        "        Build character vocabulary from text data.\n",
        "        \n",
        "        Args:\n",
        "            text_data: List of (text, label) tuples\n",
        "        \"\"\"\n",
        "        all_characters = set()\n",
        "        \n",
        "        # Collect all unique characters\n",
        "        for text, _ in text_data:\n",
        "            normalized = self.unicode_to_ascii(text)\n",
        "            all_characters.update(normalized)\n",
        "        \n",
        "        # Sort for consistency\n",
        "        self.all_letters = ''.join(sorted(all_characters))\n",
        "        self.n_letters = len(self.all_letters)\n",
        "        \n",
        "        # Build mappings\n",
        "        self.letter_to_index = {letter: i for i, letter in enumerate(self.all_letters)}\n",
        "        self.index_to_letter = {i: letter for i, letter in enumerate(self.all_letters)}\n",
        "        \n",
        "        print(f\"\ud83d\udcdd Character vocabulary built:\")\n",
        "        print(f\"   Unique characters: {self.n_letters}\")\n",
        "        print(f\"   Character set: {self.all_letters[:50]}{'...' if len(self.all_letters) > 50 else ''}\")\n",
        "    \n",
        "    def text_to_tensor(self, text):\n",
        "        \"\"\"\n",
        "        Convert text to PyTorch tensor.\n",
        "        \n",
        "        TensorFlow equivalent:\n",
        "            tf.strings.unicode_decode(text, 'UTF-8')\n",
        "        \n",
        "        Args:\n",
        "            text: Input text string\n",
        "            \n",
        "        Returns:\n",
        "            torch.Tensor: Character indices tensor\n",
        "        \"\"\"\n",
        "        normalized = self.unicode_to_ascii(text)\n",
        "        indices = [self.letter_to_index.get(char, 0) for char in normalized]\n",
        "        return torch.tensor(indices, dtype=torch.long)\n",
        "    \n",
        "    def tensor_to_text(self, tensor):\n",
        "        \"\"\"\n",
        "        Convert tensor back to text string.\n",
        "        \n",
        "        Args:\n",
        "            tensor: PyTorch tensor of character indices\n",
        "            \n",
        "        Returns:\n",
        "            str: Reconstructed text\n",
        "        \"\"\"\n",
        "        indices = tensor.cpu().numpy() if tensor.is_cuda else tensor.numpy()\n",
        "        return ''.join([self.index_to_letter.get(int(idx), '') for idx in indices])\n",
        "    \n",
        "    def char_to_onehot(self, char_index, device=None):\n",
        "        \"\"\"\n",
        "        Convert character index to one-hot vector.\n",
        "        \n",
        "        Args:\n",
        "            char_index: Index of character\n",
        "            device: PyTorch device for tensor\n",
        "            \n",
        "        Returns:\n",
        "            torch.Tensor: One-hot encoded vector\n",
        "        \"\"\"\n",
        "        if device is None:\n",
        "            device = torch.device('cpu')\n",
        "            \n",
        "        onehot = torch.zeros(self.n_letters, device=device)\n",
        "        if 0 <= char_index < self.n_letters:\n",
        "            onehot[char_index] = 1\n",
        "        return onehot\n",
        "    \n",
        "    def text_to_onehot_sequence(self, text, device=None):\n",
        "        \"\"\"\n",
        "        Convert text to sequence of one-hot vectors.\n",
        "        \n",
        "        Args:\n",
        "            text: Input text\n",
        "            device: PyTorch device\n",
        "            \n",
        "        Returns:\n",
        "            torch.Tensor: Sequence tensor [seq_len, vocab_size]\n",
        "        \"\"\"\n",
        "        if device is None:\n",
        "            device = torch.device('cpu')\n",
        "            \n",
        "        normalized = self.unicode_to_ascii(text)\n",
        "        sequence_length = len(normalized)\n",
        "        \n",
        "        # Create tensor to hold the sequence\n",
        "        onehot_sequence = torch.zeros(sequence_length, self.n_letters, device=device)\n",
        "        \n",
        "        for i, char in enumerate(normalized):\n",
        "            char_idx = self.letter_to_index.get(char, 0)\n",
        "            onehot_sequence[i][char_idx] = 1\n",
        "            \n",
        "        return onehot_sequence\n",
        "\n",
        "# Create character processor and build vocabulary\n",
        "char_processor = CharacterProcessor()\n",
        "\n",
        "# Combine both datasets for vocabulary building\n",
        "all_data = names_data + locations_data\n",
        "char_processor.build_vocabulary(all_data)\n",
        "\n",
        "# Test character processing\n",
        "print(\"\\n\ud83e\uddea Testing Character Processing:\")\n",
        "test_names = ['Nguyen', 'Papadopoulos', 'Sydney', 'Uluru']\n",
        "for name in test_names:\n",
        "    tensor = char_processor.text_to_tensor(name)\n",
        "    reconstructed = char_processor.tensor_to_text(tensor)\n",
        "    print(f\"   '{name}' -> {tensor.tolist()} -> '{reconstructed}'\")\n",
        "\n",
        "# Show character vocabulary details\n",
        "print(f\"\\n\ud83d\udcda Character Vocabulary Details:\")\n",
        "print(f\"   Total characters: {char_processor.n_letters}\")\n",
        "print(f\"   Character mapping sample: {dict(list(char_processor.letter_to_index.items())[:10])}\")\n",
        "\n",
        "print(\"\\n\u2705 Character processing system ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\uddc3\ufe0f PyTorch Dataset Implementation\n",
        "\n",
        "We'll create a custom PyTorch Dataset class that handles our character-level data efficiently. This follows PyTorch best practices and enables easy integration with DataLoader for batching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AustralianNamesDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for Australian names and locations with character-level processing.\n",
        "    \n",
        "    TensorFlow equivalent:\n",
        "        tf.data.Dataset.from_tensor_slices((texts, labels))\n",
        "    \n",
        "    This dataset handles:\n",
        "    - Character-level tokenization\n",
        "    - Variable sequence lengths\n",
        "    - Label encoding for classification\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, text_data, char_processor, transform=None):\n",
        "        \"\"\"\n",
        "        Initialize the dataset.\n",
        "        \n",
        "        Args:\n",
        "            text_data: List of (text, label) tuples\n",
        "            char_processor: CharacterProcessor instance\n",
        "            transform: Optional data transformations\n",
        "        \"\"\"\n",
        "        self.data = text_data\n",
        "        self.char_processor = char_processor\n",
        "        self.transform = transform\n",
        "        \n",
        "        # Extract unique labels and create label encoder\n",
        "        unique_labels = list(set([label for _, label in text_data]))\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.label_encoder.fit(unique_labels)\n",
        "        \n",
        "        self.num_classes = len(unique_labels)\n",
        "        self.label_names = self.label_encoder.classes_\n",
        "        \n",
        "        print(f\"\ud83d\udcca Dataset initialized:\")\n",
        "        print(f\"   Samples: {len(self.data)}\")\n",
        "        print(f\"   Classes: {self.num_classes}\")\n",
        "        print(f\"   Labels: {list(self.label_names)}\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text, label = self.data[idx]\n",
        "        \n",
        "        # Convert text to character tensor\n",
        "        char_tensor = self.char_processor.text_to_tensor(text)\n",
        "        \n",
        "        # Encode label\n",
        "        label_encoded = torch.tensor(self.label_encoder.transform([label])[0], dtype=torch.long)\n",
        "        \n",
        "        if self.transform:\n",
        "            char_tensor = self.transform(char_tensor)\n",
        "            \n",
        "        return char_tensor, label_encoded\n",
        "    \n",
        "    def get_label_name(self, encoded_label):\n",
        "        \"\"\"Convert encoded label back to name.\"\"\"\n",
        "        return self.label_encoder.inverse_transform([encoded_label])[0]\n",
        "\n",
        "def collate_sequences(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function for variable-length sequences.\n",
        "    \n",
        "    PyTorch DataLoader requires fixed-size tensors, so we pad sequences\n",
        "    to the maximum length in each batch.\n",
        "    \n",
        "    TensorFlow equivalent:\n",
        "        tf.keras.preprocessing.sequence.pad_sequences()\n",
        "    \"\"\"\n",
        "    sequences, labels = zip(*batch)\n",
        "    \n",
        "    # Find maximum sequence length in batch\n",
        "    max_length = max(len(seq) for seq in sequences)\n",
        "    \n",
        "    # Pad sequences to max length\n",
        "    padded_sequences = []\n",
        "    sequence_lengths = []\n",
        "    \n",
        "    for seq in sequences:\n",
        "        seq_len = len(seq)\n",
        "        sequence_lengths.append(seq_len)\n",
        "        \n",
        "        # Pad with zeros (assuming 0 is a valid padding index)\n",
        "        if seq_len < max_length:\n",
        "            padding = torch.zeros(max_length - seq_len, dtype=torch.long)\n",
        "            padded_seq = torch.cat([seq, padding])\n",
        "        else:\n",
        "            padded_seq = seq\n",
        "            \n",
        "        padded_sequences.append(padded_seq)\n",
        "    \n",
        "    # Stack into batch tensors\n",
        "    sequences_tensor = torch.stack(padded_sequences)\n",
        "    labels_tensor = torch.stack(labels)\n",
        "    lengths_tensor = torch.tensor(sequence_lengths, dtype=torch.long)\n",
        "    \n",
        "    return sequences_tensor, labels_tensor, lengths_tensor\n",
        "\n",
        "# Create datasets for names and locations separately\n",
        "names_dataset = AustralianNamesDataset(names_data, char_processor)\n",
        "locations_dataset = AustralianNamesDataset(locations_data, char_processor)\n",
        "\n",
        "# Split names dataset for training/validation\n",
        "train_names, val_names = train_test_split(names_data, test_size=0.2, random_state=42, \n",
        "                                        stratify=[label for _, label in names_data])\n",
        "\n",
        "train_names_dataset = AustralianNamesDataset(train_names, char_processor)\n",
        "val_names_dataset = AustralianNamesDataset(val_names, char_processor)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 32 if DEVICE.type == 'cpu' else 64\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_names_dataset, \n",
        "    batch_size=batch_size, \n",
        "    shuffle=True,\n",
        "    collate_fn=collate_sequences,\n",
        "    pin_memory=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_names_dataset, \n",
        "    batch_size=batch_size, \n",
        "    shuffle=False,\n",
        "    collate_fn=collate_sequences,\n",
        "    pin_memory=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "print(f\"\\n\ud83d\udce6 Data Loaders Created:\")\n",
        "print(f\"   Training batches: {len(train_loader)}\")\n",
        "print(f\"   Validation batches: {len(val_loader)}\")\n",
        "print(f\"   Batch size: {batch_size}\")\n",
        "\n",
        "# Test data loading\n",
        "sample_batch = next(iter(train_loader))\n",
        "sequences, labels, lengths = sample_batch\n",
        "print(f\"\\n\ud83e\uddea Sample batch shapes:\")\n",
        "print(f\"   Sequences: {sequences.shape}\")\n",
        "print(f\"   Labels: {labels.shape}\")\n",
        "print(f\"   Lengths: {lengths.shape}\")\n",
        "print(f\"   Sample sequence length range: {lengths.min().item()}-{lengths.max().item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83e\udde0 Character-Level RNN Architecture\n",
        "\n",
        "We'll implement a character-level RNN that processes names one character at a time. The model architecture includes:\n",
        "\n",
        "1. **Character Embedding** - Convert one-hot characters to dense vectors\n",
        "2. **RNN Layer** - Process character sequences (LSTM/GRU variants)\n",
        "3. **Classification Head** - Map final hidden state to class predictions\n",
        "4. **Attention Mechanism** - Optional attention over character sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CharacterLevelRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Character-level RNN for Australian name and location classification.\n",
        "    \n",
        "    TensorFlow equivalent:\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Embedding(vocab_size, embed_dim),\n",
        "            tf.keras.layers.LSTM(hidden_dim, return_sequences=False),\n",
        "            tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "        ])\n",
        "    \n",
        "    Key differences from TensorFlow:\n",
        "    - Explicit forward pass definition\n",
        "    - Manual hidden state initialization\n",
        "    - Device management with .to(device)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, \n",
        "                 rnn_type='LSTM', num_layers=1, dropout=0.2, bidirectional=False):\n",
        "        \"\"\"\n",
        "        Initialize the character-level RNN model.\n",
        "        \n",
        "        Args:\n",
        "            vocab_size: Size of character vocabulary\n",
        "            embed_dim: Embedding dimension\n",
        "            hidden_dim: Hidden state dimension\n",
        "            num_classes: Number of classification classes\n",
        "            rnn_type: Type of RNN ('RNN', 'LSTM', 'GRU')\n",
        "            num_layers: Number of RNN layers\n",
        "            dropout: Dropout rate\n",
        "            bidirectional: Whether to use bidirectional RNN\n",
        "        \"\"\"\n",
        "        super(CharacterLevelRNN, self).__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.rnn_type = rnn_type\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        \n",
        "        # Embedding layer - maps character indices to dense vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        \n",
        "        # RNN layer - choose between RNN, LSTM, and GRU\n",
        "        if rnn_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(\n",
        "                embed_dim, hidden_dim, num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
        "                bidirectional=bidirectional\n",
        "            )\n",
        "        elif rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(\n",
        "                embed_dim, hidden_dim, num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
        "                bidirectional=bidirectional\n",
        "            )\n",
        "        else:  # Vanilla RNN\n",
        "            self.rnn = nn.RNN(\n",
        "                embed_dim, hidden_dim, num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
        "                bidirectional=bidirectional, nonlinearity='relu'\n",
        "            )\n",
        "        \n",
        "        # Calculate final hidden dimension\n",
        "        final_hidden_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
        "        \n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(final_hidden_dim, final_hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(final_hidden_dim // 2, num_classes)\n",
        "        )\n",
        "        \n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "    \n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize model weights using Xavier/Glorot initialization.\"\"\"\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                if param.dim() > 1:\n",
        "                    nn.init.xavier_uniform_(param)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.constant_(param, 0)\n",
        "    \n",
        "    def forward(self, sequences, lengths=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the character-level RNN.\n",
        "        \n",
        "        Args:\n",
        "            sequences: Input character sequences [batch_size, seq_len]\n",
        "            lengths: Actual sequence lengths [batch_size]\n",
        "            \n",
        "        Returns:\n",
        "            torch.Tensor: Class logits [batch_size, num_classes]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = sequences.size()\n",
        "        \n",
        "        # Embedding lookup\n",
        "        embedded = self.embedding(sequences)  # [batch_size, seq_len, embed_dim]\n",
        "        \n",
        "        # Apply dropout to embeddings\n",
        "        embedded = self.dropout(embedded)\n",
        "        \n",
        "        # Pack sequences for efficient RNN processing (handles variable lengths)\n",
        "        if lengths is not None:\n",
        "            # Sort by length (required for packing)\n",
        "            sorted_lengths, sorted_idx = torch.sort(lengths, descending=True)\n",
        "            sorted_embedded = embedded[sorted_idx]\n",
        "            \n",
        "            # Pack the sequences\n",
        "            packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
        "                sorted_embedded, sorted_lengths.cpu(), batch_first=True\n",
        "            )\n",
        "            \n",
        "            # RNN forward pass\n",
        "            packed_output, hidden = self.rnn(packed_embedded)\n",
        "            \n",
        "            # Unpack the output (not needed for classification, but shown for completeness)\n",
        "            # output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "            \n",
        "            # Restore original order\n",
        "            _, unsorted_idx = torch.sort(sorted_idx)\n",
        "            \n",
        "        else:\n",
        "            # Simple forward pass without packing\n",
        "            output, hidden = self.rnn(embedded)\n",
        "            unsorted_idx = None\n",
        "        \n",
        "        # Extract final hidden state\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            # For LSTM, hidden is (h_n, c_n), we want h_n\n",
        "            final_hidden = hidden[0]  # [num_layers * num_directions, batch, hidden_dim]\n",
        "        else:\n",
        "            # For RNN and GRU\n",
        "            final_hidden = hidden  # [num_layers * num_directions, batch, hidden_dim]\n",
        "        \n",
        "        # Take the last layer's hidden state\n",
        "        if self.bidirectional:\n",
        "            # Concatenate forward and backward hidden states\n",
        "            final_hidden = torch.cat((final_hidden[-2], final_hidden[-1]), dim=1)\n",
        "        else:\n",
        "            final_hidden = final_hidden[-1]  # [batch, hidden_dim]\n",
        "        \n",
        "        # Restore original order if we sorted for packing\n",
        "        if unsorted_idx is not None:\n",
        "            final_hidden = final_hidden[unsorted_idx]\n",
        "        \n",
        "        # Classification\n",
        "        logits = self.classifier(final_hidden)  # [batch_size, num_classes]\n",
        "        \n",
        "        return logits\n",
        "    \n",
        "    def predict(self, text, char_processor, device=None):\n",
        "        \"\"\"\n",
        "        Predict class for a single text input.\n",
        "        \n",
        "        Args:\n",
        "            text: Input text string\n",
        "            char_processor: CharacterProcessor instance\n",
        "            device: PyTorch device\n",
        "            \n",
        "        Returns:\n",
        "            tuple: (predicted_class_idx, confidence_scores)\n",
        "        \"\"\"\n",
        "        if device is None:\n",
        "            device = next(self.parameters()).device\n",
        "            \n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # Convert text to tensor\n",
        "            char_tensor = char_processor.text_to_tensor(text).unsqueeze(0).to(device)\n",
        "            length_tensor = torch.tensor([len(text)], dtype=torch.long).to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            logits = self.forward(char_tensor, length_tensor)\n",
        "            probabilities = F.softmax(logits, dim=1)\n",
        "            \n",
        "            predicted_class = torch.argmax(probabilities, dim=1).item()\n",
        "            confidence = probabilities[0].cpu().numpy()\n",
        "            \n",
        "        return predicted_class, confidence\n",
        "\n",
        "# Model configuration\n",
        "model_config = {\n",
        "    'vocab_size': char_processor.n_letters,\n",
        "    'embed_dim': 64,\n",
        "    'hidden_dim': 128,\n",
        "    'num_classes': train_names_dataset.num_classes,\n",
        "    'rnn_type': 'LSTM',  # Can be 'RNN', 'LSTM', or 'GRU'\n",
        "    'num_layers': 2,\n",
        "    'dropout': 0.3,\n",
        "    'bidirectional': True\n",
        "}\n",
        "\n",
        "# Create model and move to device\n",
        "model = CharacterLevelRNN(**model_config).to(DEVICE)\n",
        "\n",
        "print(f\"\ud83e\udde0 Character-Level RNN Model Created\")\n",
        "print(\"=\" * 45)\n",
        "print(f\"   Architecture: {model_config['rnn_type']}\")\n",
        "print(f\"   Vocabulary size: {model_config['vocab_size']}\")\n",
        "print(f\"   Embedding dimension: {model_config['embed_dim']}\")\n",
        "print(f\"   Hidden dimension: {model_config['hidden_dim']}\")\n",
        "print(f\"   Number of classes: {model_config['num_classes']}\")\n",
        "print(f\"   Layers: {model_config['num_layers']}\")\n",
        "print(f\"   Bidirectional: {model_config['bidirectional']}\")\n",
        "print(f\"   Device: {DEVICE}\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Model Parameters:\")\n",
        "print(f\"   Total parameters: {total_params:,}\")\n",
        "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Test model with sample input\n",
        "sample_sequences, sample_labels, sample_lengths = next(iter(train_loader))\n",
        "sample_sequences = sample_sequences.to(DEVICE)\n",
        "sample_lengths = sample_lengths.to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    sample_output = model(sample_sequences, sample_lengths)\n",
        "    print(f\"\\n\ud83e\uddea Sample model output:\")\n",
        "    print(f\"   Input shape: {sample_sequences.shape}\")\n",
        "    print(f\"   Output shape: {sample_output.shape}\")\n",
        "    print(f\"   Output range: [{sample_output.min().item():.3f}, {sample_output.max().item():.3f}]\")\n",
        "\n",
        "print(\"\\n\u2705 Character-level RNN model ready for training!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}