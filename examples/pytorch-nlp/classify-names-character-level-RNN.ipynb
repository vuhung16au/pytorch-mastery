{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLP From Scratch: Classifying Names with a Character-Level RNN \ud83c\udde6\ud83c\uddfa\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vuhung16au/pytorch-mastery/blob/main/examples/pytorch-nlp/classify-names-character-level-RNN.ipynb)\n",
        "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-blue?logo=github)](https://github.com/vuhung16au/pytorch-mastery/blob/main/examples/pytorch-nlp/classify-names-character-level-RNN.ipynb)\n",
        "\n",
        "A comprehensive introduction to character-level Recurrent Neural Networks (RNNs) for name classification using PyTorch, featuring Australian names and locations with Vietnamese multilingual support.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "- \ud83d\udd24 **Master character-level text processing** with PyTorch\n",
        "- \ud83e\udde0 **Build RNN from scratch** for sequence classification\n",
        "- \ud83c\udde6\ud83c\uddfa **Classify Australian names and locations** by origin/type\n",
        "- \ud83c\udf0f **Handle multilingual text** with English-Vietnamese examples\n",
        "- \ud83d\udd04 **Compare with TensorFlow** approaches for RNN implementation\n",
        "- \ud83d\udcca **Implement comprehensive logging** with TensorBoard\n",
        "\n",
        "## What You'll Build\n",
        "\n",
        "1. **Australian Name Origin Classifier** - Classify names by ethnic origin (English, Irish, Greek, Vietnamese, etc.)\n",
        "2. **Location Type Classifier** - Distinguish between cities, suburbs, landmarks, and natural features\n",
        "3. **Character-level RNN Architecture** - Build vanilla RNN, LSTM, and GRU variants\n",
        "4. **Multilingual Support** - Handle both English and Vietnamese character sets\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment Detection and Setup\n",
        "import sys\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Detect the runtime environment\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules or \"kaggle\" in os.environ.get('KAGGLE_URL_BASE', '')\n",
        "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
        "\n",
        "print(f\"Environment detected:\")\n",
        "print(f\"  - Local: {IS_LOCAL}\")\n",
        "print(f\"  - Google Colab: {IS_COLAB}\")\n",
        "print(f\"  - Kaggle: {IS_KAGGLE}\")\n",
        "\n",
        "# Platform-specific system setup\n",
        "if IS_COLAB:\n",
        "    print(\"\\nSetting up Google Colab environment...\")\n",
        "    !apt update -qq\n",
        "    !apt install -y -qq software-properties-common\n",
        "elif IS_KAGGLE:\n",
        "    print(\"\\nSetting up Kaggle environment...\")\n",
        "    # Kaggle usually has most packages pre-installed\n",
        "else:\n",
        "    print(\"\\nSetting up local environment...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages for this notebook\n",
        "required_packages = [\n",
        "    \"torch\",\n",
        "    \"pandas\",\n",
        "    \"seaborn\",\n",
        "    \"matplotlib\",\n",
        "    \"tensorboard\",\n",
        "    \"scikit-learn\",\n",
        "    \"numpy\"\n",
        "]\n",
        "\n",
        "print(\"Installing required packages...\")\n",
        "for package in required_packages:\n",
        "    if IS_COLAB or IS_KAGGLE:\n",
        "        !pip install -q {package}\n",
        "    else:\n",
        "        try:\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package],\n",
        "                          capture_output=True, check=True)\n",
        "        except subprocess.CalledProcessError:\n",
        "            print(f\"Note: {package} installation skipped (likely already installed)\")\n",
        "    print(f\"\u2713 {package}\")\n",
        "\n",
        "print(\"\\n\ud83d\udce6 Package installation completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import essential libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Data handling and visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Text processing and utilities\n",
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "import time\n",
        "from datetime import datetime\n",
        "import platform\n",
        "\n",
        "# Set style for better notebook aesthetics\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(f\"\u2705 PyTorch {torch.__version__} ready!\")\n",
        "print(f\"\ud83d\udcca Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_device():\n",
        "    \"\"\"\n",
        "    Detect the best available PyTorch device with comprehensive hardware support.\n",
        "    \n",
        "    Priority order:\n",
        "    1. CUDA (NVIDIA GPUs) - Best performance for deep learning\n",
        "    2. MPS (Apple Silicon) - Optimized for M1/M2/M3 Macs  \n",
        "    3. CPU (Universal) - Always available fallback\n",
        "    \n",
        "    Returns:\n",
        "        torch.device: The optimal device for PyTorch operations\n",
        "        str: Human-readable device description for logging\n",
        "    \"\"\"\n",
        "    # Check for CUDA (NVIDIA GPU)\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        device_info = f\"CUDA GPU: {gpu_name}\"\n",
        "        \n",
        "        # Additional CUDA info for optimization\n",
        "        cuda_version = torch.version.cuda\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "        \n",
        "        print(f\"\ud83d\ude80 Using CUDA acceleration\")\n",
        "        print(f\"   GPU: {gpu_name}\")\n",
        "        print(f\"   CUDA Version: {cuda_version}\")\n",
        "        print(f\"   GPU Memory: {gpu_memory:.1f} GB\")\n",
        "        \n",
        "        return device, device_info\n",
        "    \n",
        "    # Check for MPS (Apple Silicon)\n",
        "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "        device_info = \"Apple Silicon MPS\"\n",
        "        \n",
        "        # Get system info for Apple Silicon\n",
        "        system_info = platform.uname()\n",
        "        \n",
        "        print(f\"\ud83c\udf4e Using Apple Silicon MPS acceleration\")\n",
        "        print(f\"   System: {system_info.system} {system_info.release}\")\n",
        "        print(f\"   Machine: {system_info.machine}\")\n",
        "        print(f\"   Processor: {system_info.processor}\")\n",
        "        \n",
        "        return device, device_info\n",
        "    \n",
        "    # Fallback to CPU\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        device_info = \"CPU (No GPU acceleration available)\"\n",
        "        \n",
        "        # Get CPU info for optimization guidance\n",
        "        cpu_count = torch.get_num_threads()\n",
        "        system_info = platform.uname()\n",
        "        \n",
        "        print(f\"\ud83d\udcbb Using CPU (no GPU acceleration detected)\")\n",
        "        print(f\"   Processor: {system_info.processor}\")\n",
        "        print(f\"   PyTorch Threads: {cpu_count}\")\n",
        "        print(f\"   System: {system_info.system} {system_info.release}\")\n",
        "        \n",
        "        # Provide optimization suggestions for CPU-only setups\n",
        "        print(f\"\\n\ud83d\udca1 CPU Optimization Tips:\")\n",
        "        print(f\"   \u2022 Reduce batch size to prevent memory issues\")\n",
        "        print(f\"   \u2022 Consider using smaller models for faster training\")\n",
        "        print(f\"   \u2022 Enable PyTorch optimizations: torch.set_num_threads({cpu_count})\")\n",
        "        \n",
        "        return device, device_info\n",
        "\n",
        "# Usage in all PyTorch notebooks\n",
        "device, device_info = detect_device()\n",
        "print(f\"\\n\u2705 PyTorch device selected: {device}\")\n",
        "print(f\"\ud83d\udcca Device info: {device_info}\")\n",
        "\n",
        "# Set global device for the notebook\n",
        "DEVICE = device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udde6\ud83c\uddfa Australian Names and Locations Dataset\n",
        "\n",
        "We'll create a comprehensive dataset featuring Australian names classified by ethnic origin, and Australian locations classified by type. This follows the repository's Australian context policy while providing practical multilingual examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_australian_names_dataset():\n",
        "    \"\"\"\n",
        "    Create Australian names dataset with ethnic origin classification.\n",
        "    Includes both English and Vietnamese names commonly found in Australia.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Australian names by ethnic origin\n",
        "    names_by_origin = {\n",
        "        'English': [\n",
        "            'Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Wilson', 'Davis', 'Taylor',\n",
        "            'Anderson', 'Thomas', 'Jackson', 'White', 'Harris', 'Martin', 'Thompson',\n",
        "            'Garcia', 'Martinez', 'Robinson', 'Clark', 'Lewis', 'Walker', 'Hall',\n",
        "            'Allen', 'Young', 'King', 'Wright', 'Lopez', 'Hill', 'Scott', 'Green',\n",
        "            'Adams', 'Baker', 'Gonzalez', 'Nelson', 'Carter', 'Mitchell', 'Perez'\n",
        "        ],\n",
        "        'Irish': [\n",
        "            'Murphy', 'Kelly', 'Sullivan', 'Walsh', 'Smith', 'OBrien', 'Byrne', 'Ryan',\n",
        "            'Connor', 'ONeill', 'Reilly', 'Doyle', 'McCarthy', 'Gallagher', 'Doherty',\n",
        "            'Kennedy', 'Lynch', 'Murray', 'Quinn', 'Moore', 'McLaughlin', 'Carroll',\n",
        "            'Connolly', 'Daly', 'Connell', 'Wilson', 'Dunne', 'Brennan', 'Burke',\n",
        "            'Collins', 'Campbell', 'Clarke', 'Johnston', 'Hughes', 'Farrell'\n",
        "        ],\n",
        "        'Italian': [\n",
        "            'Rossi', 'Russo', 'Ferrari', 'Esposito', 'Bianchi', 'Romano', 'Colombo',\n",
        "            'Ricci', 'Marino', 'Greco', 'Bruno', 'Gallo', 'Conti', 'DeLuca',\n",
        "            'Mancini', 'Costa', 'Giordano', 'Rizzo', 'Lombardi', 'Moretti',\n",
        "            'Barbieri', 'Fontana', 'Santoro', 'Mariani', 'Rinaldi', 'Caruso',\n",
        "            'Ferrara', 'Galli', 'Martini', 'Leone', 'Longo', 'Gentile', 'Martinelli'\n",
        "        ],\n",
        "        'Greek': [\n",
        "            'Papadopoulos', 'Georgiou', 'Dimitriou', 'Andreou', 'Nikolaou', 'Christou',\n",
        "            'Ioannou', 'Constantinou', 'Antoniou', 'Savva', 'Charalambous', 'Stylianou',\n",
        "            'Petrou', 'Michaelidou', 'Hadjisavvas', 'Kokkinos', 'Stavrou', 'Loizou',\n",
        "            'Panayiotou', 'Economou', 'Demetriou', 'Philippou', 'Vassiliou', 'Kyprianou',\n",
        "            'Theodorou', 'Christodoulou', 'Anastasiadou', 'Hadjiconstantinou'\n",
        "        ],\n",
        "        'Vietnamese': [\n",
        "            'Nguyen', 'Tran', 'Le', 'Pham', 'Hoang', 'Huynh', 'Vo', 'Vu', 'Dang', 'Bui',\n",
        "            'Do', 'Ho', 'Ngo', 'Duong', 'Ly', 'Trinh', 'Dinh', 'Thai', 'Cao', 'Lam',\n",
        "            'Phan', 'Truong', 'Tang', 'Doan', 'Mai', 'Ton', 'Ha', 'Chau', 'Bach', 'Kim',\n",
        "            'Luu', 'Ong', 'Tong', 'Quan', 'Dam', 'Khang', 'Thang', 'Phung', 'Duc', 'Vinh'\n",
        "        ],\n",
        "        'Chinese': [\n",
        "            'Wang', 'Li', 'Zhang', 'Liu', 'Chen', 'Yang', 'Huang', 'Zhao', 'Wu', 'Zhou',\n",
        "            'Xu', 'Sun', 'Ma', 'Zhu', 'Hu', 'Guo', 'He', 'Lin', 'Gao', 'Luo',\n",
        "            'Zheng', 'Liang', 'Xie', 'Tang', 'Song', 'Xu', 'Han', 'Feng', 'Deng', 'Cao',\n",
        "            'Peng', 'Zeng', 'Xiao', 'Tian', 'Pan', 'Cheng', 'Wei', 'Jiang', 'Yu', 'Shi'\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    # Flatten the dataset\n",
        "    names_data = []\n",
        "    for origin, names in names_by_origin.items():\n",
        "        for name in names:\n",
        "            names_data.append((name, origin))\n",
        "    \n",
        "    return names_data\n",
        "\n",
        "def create_australian_locations_dataset():\n",
        "    \"\"\"\n",
        "    Create Australian locations dataset classified by type.\n",
        "    Includes cities, suburbs, landmarks, and natural features.\n",
        "    \"\"\"\n",
        "    \n",
        "    locations_by_type = {\n",
        "        'City': [\n",
        "            'Sydney', 'Melbourne', 'Brisbane', 'Perth', 'Adelaide', 'Darwin', 'Hobart', 'Canberra',\n",
        "            'Newcastle', 'Wollongong', 'Geelong', 'Townsville', 'Cairns', 'Ballarat', 'Bendigo',\n",
        "            'Albury', 'Wodonga', 'Shepparton', 'Wagga', 'Rockhampton', 'Bundaberg', 'Hervey',\n",
        "            'Toowoomba', 'Mackay', 'Gladstone', 'Warrnambool', 'Mildura', 'Launceston'\n",
        "        ],\n",
        "        'Suburb': [\n",
        "            'Bondi', 'Manly', 'Paddington', 'Surry', 'Newtown', 'Leichhardt', 'Balmain',\n",
        "            'Toorak', 'Brighton', 'Camberwell', 'Hawthorn', 'Richmond', 'Fitzroy', 'Carlton',\n",
        "            'Southbank', 'Docklands', 'Fortitude', 'Paddington', 'Milton', 'Ascot',\n",
        "            'Cottesloe', 'Subiaco', 'Fremantle', 'Scarborough', 'Joondalup', 'Midland'\n",
        "        ],\n",
        "        'Landmark': [\n",
        "            'Opera', 'Harbour', 'Luna', 'Royal', 'Federation', 'Parliament', 'Story', 'Shrine',\n",
        "            'Botanic', 'Observatory', 'Anzac', 'War', 'National', 'Australian', 'Museum',\n",
        "            'Gallery', 'Library', 'University', 'Stadium', 'Arena', 'Centre', 'Tower',\n",
        "            'Bridge', 'Wharf', 'Market', 'Square', 'Gardens', 'Reserve', 'Park'\n",
        "        ],\n",
        "        'Natural': [\n",
        "            'Uluru', 'Kakadu', 'Daintree', 'Grampians', 'Flinders', 'Cradle', 'Freycinet',\n",
        "            'Wilsons', 'Kosciuszko', 'Alpine', 'Snowy', 'Murray', 'Darling', 'Cooper',\n",
        "            'Murrumbidgee', 'Lachlan', 'Macquarie', 'Hawkesbury', 'Yarra', 'Maribyrnong',\n",
        "            'Barwon', 'Goulburn', 'Campaspe', 'Loddon', 'Wimmera', 'Glenelg', 'Torrens'\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    # Flatten the dataset\n",
        "    locations_data = []\n",
        "    for location_type, locations in locations_by_type.items():\n",
        "        for location in locations:\n",
        "            locations_data.append((location, location_type))\n",
        "    \n",
        "    return locations_data\n",
        "\n",
        "# Create the datasets\n",
        "names_data = create_australian_names_dataset()\n",
        "locations_data = create_australian_locations_dataset()\n",
        "\n",
        "print(\"\ud83c\udde6\ud83c\uddfa Australian Names and Locations Dataset Created\")\n",
        "print(\"=\" * 55)\n",
        "print(f\"   Names dataset: {len(names_data)} entries\")\n",
        "print(f\"   Locations dataset: {len(locations_data)} entries\")\n",
        "\n",
        "# Show sample data\n",
        "print(\"\\n\ud83d\udcdd Sample Names by Origin:\")\n",
        "names_df = pd.DataFrame(names_data, columns=['Name', 'Origin'])\n",
        "for origin in ['English', 'Vietnamese', 'Greek']:\n",
        "    samples = names_df[names_df['Origin'] == origin]['Name'].head(3).tolist()\n",
        "    print(f\"   {origin}: {', '.join(samples)}\")\n",
        "\n",
        "print(\"\\n\ud83c\udfdb\ufe0f Sample Locations by Type:\")\n",
        "locations_df = pd.DataFrame(locations_data, columns=['Location', 'Type'])\n",
        "for loc_type in ['City', 'Suburb', 'Landmark', 'Natural']:\n",
        "    samples = locations_df[locations_df['Type'] == loc_type]['Location'].head(3).tolist()\n",
        "    print(f\"   {loc_type}: {', '.join(samples)}\")\n",
        "\n",
        "# Distribution visualization\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Names distribution\n",
        "sns.countplot(data=names_df, x='Origin', ax=ax1)\n",
        "ax1.set_title('Australian Names by Ethnic Origin')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Locations distribution\n",
        "sns.countplot(data=locations_df, x='Type', ax=ax2)\n",
        "ax2.set_title('Australian Locations by Type')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\u2705 Datasets prepared for character-level RNN training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd24 Character-Level Text Processing\n",
        "\n",
        "Character-level processing is fundamental to our RNN. We'll create utilities to:\n",
        "\n",
        "1. **Build character vocabulary** from both English and Vietnamese text\n",
        "2. **Convert text to tensors** and vice versa\n",
        "3. **Handle Unicode characters** for multilingual support\n",
        "4. **Normalize text** for consistent processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CharacterProcessor:\n",
        "    \"\"\"\n",
        "    Character-level text processing for Australian multilingual names and locations.\n",
        "    \n",
        "    Handles both ASCII and Unicode characters for English-Vietnamese support.\n",
        "    Comparable to TensorFlow's text preprocessing but with explicit character control.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.all_letters = string.ascii_letters + \" .,;'-\"\n",
        "        self.n_letters = len(self.all_letters)\n",
        "        self.letter_to_index = {}\n",
        "        self.index_to_letter = {}\n",
        "        \n",
        "    def unicode_to_ascii(self, text):\n",
        "        \"\"\"\n",
        "        Convert Unicode characters to ASCII for Vietnamese names.\n",
        "        \n",
        "        Examples:\n",
        "        - 'Nguy\u1ec5n' -> 'Nguyen'\n",
        "        - 'Tr\u1ea7n' -> 'Tran'\n",
        "        \"\"\"\n",
        "        return ''.join(\n",
        "            c for c in unicodedata.normalize('NFD', text)\n",
        "            if unicodedata.category(c) != 'Mn'\n",
        "            and c in self.all_letters\n",
        "        )\n",
        "    \n",
        "    def build_vocabulary(self, text_data):\n",
        "        \"\"\"\n",
        "        Build character vocabulary from text data.\n",
        "        \n",
        "        Args:\n",
        "            text_data: List of (text, label) tuples\n",
        "        \"\"\"\n",
        "        all_characters = set()\n",
        "        \n",
        "        # Collect all unique characters\n",
        "        for text, _ in text_data:\n",
        "            normalized = self.unicode_to_ascii(text)\n",
        "            all_characters.update(normalized)\n",
        "        \n",
        "        # Sort for consistency\n",
        "        self.all_letters = ''.join(sorted(all_characters))\n",
        "        self.n_letters = len(self.all_letters)\n",
        "        \n",
        "        # Build mappings\n",
        "        self.letter_to_index = {letter: i for i, letter in enumerate(self.all_letters)}\n",
        "        self.index_to_letter = {i: letter for i, letter in enumerate(self.all_letters)}\n",
        "        \n",
        "        print(f\"\ud83d\udcdd Character vocabulary built:\")\n",
        "        print(f\"   Unique characters: {self.n_letters}\")\n",
        "        print(f\"   Character set: {self.all_letters[:50]}{'...' if len(self.all_letters) > 50 else ''}\")\n",
        "    \n",
        "    def text_to_tensor(self, text):\n",
        "        \"\"\"\n",
        "        Convert text to PyTorch tensor.\n",
        "        \n",
        "        TensorFlow equivalent:\n",
        "            tf.strings.unicode_decode(text, 'UTF-8')\n",
        "        \n",
        "        Args:\n",
        "            text: Input text string\n",
        "            \n",
        "        Returns:\n",
        "            torch.Tensor: Character indices tensor\n",
        "        \"\"\"\n",
        "        normalized = self.unicode_to_ascii(text)\n",
        "        indices = [self.letter_to_index.get(char, 0) for char in normalized]\n",
        "        return torch.tensor(indices, dtype=torch.long)\n",
        "    \n",
        "    def tensor_to_text(self, tensor):\n",
        "        \"\"\"\n",
        "        Convert tensor back to text string.\n",
        "        \n",
        "        Args:\n",
        "            tensor: PyTorch tensor of character indices\n",
        "            \n",
        "        Returns:\n",
        "            str: Reconstructed text\n",
        "        \"\"\"\n",
        "        indices = tensor.cpu().numpy() if tensor.is_cuda else tensor.numpy()\n",
        "        return ''.join([self.index_to_letter.get(int(idx), '') for idx in indices])\n",
        "    \n",
        "    def char_to_onehot(self, char_index, device=None):\n",
        "        \"\"\"\n",
        "        Convert character index to one-hot vector.\n",
        "        \n",
        "        Args:\n",
        "            char_index: Index of character\n",
        "            device: PyTorch device for tensor\n",
        "            \n",
        "        Returns:\n",
        "            torch.Tensor: One-hot encoded vector\n",
        "        \"\"\"\n",
        "        if device is None:\n",
        "            device = torch.device('cpu')\n",
        "            \n",
        "        onehot = torch.zeros(self.n_letters, device=device)\n",
        "        if 0 <= char_index < self.n_letters:\n",
        "            onehot[char_index] = 1\n",
        "        return onehot\n",
        "    \n",
        "    def text_to_onehot_sequence(self, text, device=None):\n",
        "        \"\"\"\n",
        "        Convert text to sequence of one-hot vectors.\n",
        "        \n",
        "        Args:\n",
        "            text: Input text\n",
        "            device: PyTorch device\n",
        "            \n",
        "        Returns:\n",
        "            torch.Tensor: Sequence tensor [seq_len, vocab_size]\n",
        "        \"\"\"\n",
        "        if device is None:\n",
        "            device = torch.device('cpu')\n",
        "            \n",
        "        normalized = self.unicode_to_ascii(text)\n",
        "        sequence_length = len(normalized)\n",
        "        \n",
        "        # Create tensor to hold the sequence\n",
        "        onehot_sequence = torch.zeros(sequence_length, self.n_letters, device=device)\n",
        "        \n",
        "        for i, char in enumerate(normalized):\n",
        "            char_idx = self.letter_to_index.get(char, 0)\n",
        "            onehot_sequence[i][char_idx] = 1\n",
        "            \n",
        "        return onehot_sequence\n",
        "\n",
        "# Create character processor and build vocabulary\n",
        "char_processor = CharacterProcessor()\n",
        "\n",
        "# Combine both datasets for vocabulary building\n",
        "all_data = names_data + locations_data\n",
        "char_processor.build_vocabulary(all_data)\n",
        "\n",
        "# Test character processing\n",
        "print(\"\\n\ud83e\uddea Testing Character Processing:\")\n",
        "test_names = ['Nguyen', 'Papadopoulos', 'Sydney', 'Uluru']\n",
        "for name in test_names:\n",
        "    tensor = char_processor.text_to_tensor(name)\n",
        "    reconstructed = char_processor.tensor_to_text(tensor)\n",
        "    print(f\"   '{name}' -> {tensor.tolist()} -> '{reconstructed}'\")\n",
        "\n",
        "# Show character vocabulary details\n",
        "print(f\"\\n\ud83d\udcda Character Vocabulary Details:\")\n",
        "print(f\"   Total characters: {char_processor.n_letters}\")\n",
        "print(f\"   Character mapping sample: {dict(list(char_processor.letter_to_index.items())[:10])}\")\n",
        "\n",
        "print(\"\\n\u2705 Character processing system ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\uddc3\ufe0f PyTorch Dataset Implementation\n",
        "\n",
        "We'll create a custom PyTorch Dataset class that handles our character-level data efficiently. This follows PyTorch best practices and enables easy integration with DataLoader for batching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AustralianNamesDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for Australian names and locations with character-level processing.\n",
        "    \n",
        "    TensorFlow equivalent:\n",
        "        tf.data.Dataset.from_tensor_slices((texts, labels))\n",
        "    \n",
        "    This dataset handles:\n",
        "    - Character-level tokenization\n",
        "    - Variable sequence lengths\n",
        "    - Label encoding for classification\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, text_data, char_processor, transform=None):\n",
        "        \"\"\"\n",
        "        Initialize the dataset.\n",
        "        \n",
        "        Args:\n",
        "            text_data: List of (text, label) tuples\n",
        "            char_processor: CharacterProcessor instance\n",
        "            transform: Optional data transformations\n",
        "        \"\"\"\n",
        "        self.data = text_data\n",
        "        self.char_processor = char_processor\n",
        "        self.transform = transform\n",
        "        \n",
        "        # Extract unique labels and create label encoder\n",
        "        unique_labels = list(set([label for _, label in text_data]))\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.label_encoder.fit(unique_labels)\n",
        "        \n",
        "        self.num_classes = len(unique_labels)\n",
        "        self.label_names = self.label_encoder.classes_\n",
        "        \n",
        "        print(f\"\ud83d\udcca Dataset initialized:\")\n",
        "        print(f\"   Samples: {len(self.data)}\")\n",
        "        print(f\"   Classes: {self.num_classes}\")\n",
        "        print(f\"   Labels: {list(self.label_names)}\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text, label = self.data[idx]\n",
        "        \n",
        "        # Convert text to character tensor\n",
        "        char_tensor = self.char_processor.text_to_tensor(text)\n",
        "        \n",
        "        # Encode label\n",
        "        label_encoded = torch.tensor(self.label_encoder.transform([label])[0], dtype=torch.long)\n",
        "        \n",
        "        if self.transform:\n",
        "            char_tensor = self.transform(char_tensor)\n",
        "            \n",
        "        return char_tensor, label_encoded\n",
        "    \n",
        "    def get_label_name(self, encoded_label):\n",
        "        \"\"\"Convert encoded label back to name.\"\"\"\n",
        "        return self.label_encoder.inverse_transform([encoded_label])[0]\n",
        "\n",
        "def collate_sequences(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function for variable-length sequences.\n",
        "    \n",
        "    PyTorch DataLoader requires fixed-size tensors, so we pad sequences\n",
        "    to the maximum length in each batch.\n",
        "    \n",
        "    TensorFlow equivalent:\n",
        "        tf.keras.preprocessing.sequence.pad_sequences()\n",
        "    \"\"\"\n",
        "    sequences, labels = zip(*batch)\n",
        "    \n",
        "    # Find maximum sequence length in batch\n",
        "    max_length = max(len(seq) for seq in sequences)\n",
        "    \n",
        "    # Pad sequences to max length\n",
        "    padded_sequences = []\n",
        "    sequence_lengths = []\n",
        "    \n",
        "    for seq in sequences:\n",
        "        seq_len = len(seq)\n",
        "        sequence_lengths.append(seq_len)\n",
        "        \n",
        "        # Pad with zeros (assuming 0 is a valid padding index)\n",
        "        if seq_len < max_length:\n",
        "            padding = torch.zeros(max_length - seq_len, dtype=torch.long)\n",
        "            padded_seq = torch.cat([seq, padding])\n",
        "        else:\n",
        "            padded_seq = seq\n",
        "            \n",
        "        padded_sequences.append(padded_seq)\n",
        "    \n",
        "    # Stack into batch tensors\n",
        "    sequences_tensor = torch.stack(padded_sequences)\n",
        "    labels_tensor = torch.stack(labels)\n",
        "    lengths_tensor = torch.tensor(sequence_lengths, dtype=torch.long)\n",
        "    \n",
        "    return sequences_tensor, labels_tensor, lengths_tensor\n",
        "\n",
        "# Create datasets for names and locations separately\n",
        "names_dataset = AustralianNamesDataset(names_data, char_processor)\n",
        "locations_dataset = AustralianNamesDataset(locations_data, char_processor)\n",
        "\n",
        "# Split names dataset for training/validation\n",
        "train_names, val_names = train_test_split(names_data, test_size=0.2, random_state=42, \n",
        "                                        stratify=[label for _, label in names_data])\n",
        "\n",
        "train_names_dataset = AustralianNamesDataset(train_names, char_processor)\n",
        "val_names_dataset = AustralianNamesDataset(val_names, char_processor)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 32 if DEVICE.type == 'cpu' else 64\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_names_dataset, \n",
        "    batch_size=batch_size, \n",
        "    shuffle=True,\n",
        "    collate_fn=collate_sequences,\n",
        "    pin_memory=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_names_dataset, \n",
        "    batch_size=batch_size, \n",
        "    shuffle=False,\n",
        "    collate_fn=collate_sequences,\n",
        "    pin_memory=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "print(f\"\\n\ud83d\udce6 Data Loaders Created:\")\n",
        "print(f\"   Training batches: {len(train_loader)}\")\n",
        "print(f\"   Validation batches: {len(val_loader)}\")\n",
        "print(f\"   Batch size: {batch_size}\")\n",
        "\n",
        "# Test data loading\n",
        "sample_batch = next(iter(train_loader))\n",
        "sequences, labels, lengths = sample_batch\n",
        "print(f\"\\n\ud83e\uddea Sample batch shapes:\")\n",
        "print(f\"   Sequences: {sequences.shape}\")\n",
        "print(f\"   Labels: {labels.shape}\")\n",
        "print(f\"   Lengths: {lengths.shape}\")\n",
        "print(f\"   Sample sequence length range: {lengths.min().item()}-{lengths.max().item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83e\udde0 Character-Level RNN Architecture\n",
        "\n",
        "We'll implement a character-level RNN that processes names one character at a time. The model architecture includes:\n",
        "\n",
        "1. **Character Embedding** - Convert one-hot characters to dense vectors\n",
        "2. **RNN Layer** - Process character sequences (LSTM/GRU variants)\n",
        "3. **Classification Head** - Map final hidden state to class predictions\n",
        "4. **Attention Mechanism** - Optional attention over character sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CharacterLevelRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Character-level RNN for Australian name and location classification.\n",
        "    \n",
        "    TensorFlow equivalent:\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Embedding(vocab_size, embed_dim),\n",
        "            tf.keras.layers.LSTM(hidden_dim, return_sequences=False),\n",
        "            tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "        ])\n",
        "    \n",
        "    Key differences from TensorFlow:\n",
        "    - Explicit forward pass definition\n",
        "    - Manual hidden state initialization\n",
        "    - Device management with .to(device)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, \n",
        "                 rnn_type='LSTM', num_layers=1, dropout=0.2, bidirectional=False):\n",
        "        \"\"\"\n",
        "        Initialize the character-level RNN model.\n",
        "        \n",
        "        Args:\n",
        "            vocab_size: Size of character vocabulary\n",
        "            embed_dim: Embedding dimension\n",
        "            hidden_dim: Hidden state dimension\n",
        "            num_classes: Number of classification classes\n",
        "            rnn_type: Type of RNN ('RNN', 'LSTM', 'GRU')\n",
        "            num_layers: Number of RNN layers\n",
        "            dropout: Dropout rate\n",
        "            bidirectional: Whether to use bidirectional RNN\n",
        "        \"\"\"\n",
        "        super(CharacterLevelRNN, self).__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.rnn_type = rnn_type\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        \n",
        "        # Embedding layer - maps character indices to dense vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        \n",
        "        # RNN layer - choose between RNN, LSTM, and GRU\n",
        "        if rnn_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(\n",
        "                embed_dim, hidden_dim, num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
        "                bidirectional=bidirectional\n",
        "            )\n",
        "        elif rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(\n",
        "                embed_dim, hidden_dim, num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
        "                bidirectional=bidirectional\n",
        "            )\n",
        "        else:  # Vanilla RNN\n",
        "            self.rnn = nn.RNN(\n",
        "                embed_dim, hidden_dim, num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
        "                bidirectional=bidirectional, nonlinearity='relu'\n",
        "            )\n",
        "        \n",
        "        # Calculate final hidden dimension\n",
        "        final_hidden_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
        "        \n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(final_hidden_dim, final_hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(final_hidden_dim // 2, num_classes)\n",
        "        )\n",
        "        \n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "    \n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize model weights using Xavier/Glorot initialization.\"\"\"\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                if param.dim() > 1:\n",
        "                    nn.init.xavier_uniform_(param)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.constant_(param, 0)\n",
        "    \n",
        "    def forward(self, sequences, lengths=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the character-level RNN.\n",
        "        \n",
        "        Args:\n",
        "            sequences: Input character sequences [batch_size, seq_len]\n",
        "            lengths: Actual sequence lengths [batch_size]\n",
        "            \n",
        "        Returns:\n",
        "            torch.Tensor: Class logits [batch_size, num_classes]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = sequences.size()\n",
        "        \n",
        "        # Embedding lookup\n",
        "        embedded = self.embedding(sequences)  # [batch_size, seq_len, embed_dim]\n",
        "        \n",
        "        # Apply dropout to embeddings\n",
        "        embedded = self.dropout(embedded)\n",
        "        \n",
        "        # Pack sequences for efficient RNN processing (handles variable lengths)\n",
        "        if lengths is not None:\n",
        "            # Sort by length (required for packing)\n",
        "            sorted_lengths, sorted_idx = torch.sort(lengths, descending=True)\n",
        "            sorted_embedded = embedded[sorted_idx]\n",
        "            \n",
        "            # Pack the sequences\n",
        "            packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
        "                sorted_embedded, sorted_lengths.cpu(), batch_first=True\n",
        "            )\n",
        "            \n",
        "            # RNN forward pass\n",
        "            packed_output, hidden = self.rnn(packed_embedded)\n",
        "            \n",
        "            # Unpack the output (not needed for classification, but shown for completeness)\n",
        "            # output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "            \n",
        "            # Restore original order\n",
        "            _, unsorted_idx = torch.sort(sorted_idx)\n",
        "            \n",
        "        else:\n",
        "            # Simple forward pass without packing\n",
        "            output, hidden = self.rnn(embedded)\n",
        "            unsorted_idx = None\n",
        "        \n",
        "        # Extract final hidden state\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            # For LSTM, hidden is (h_n, c_n), we want h_n\n",
        "            final_hidden = hidden[0]  # [num_layers * num_directions, batch, hidden_dim]\n",
        "        else:\n",
        "            # For RNN and GRU\n",
        "            final_hidden = hidden  # [num_layers * num_directions, batch, hidden_dim]\n",
        "        \n",
        "        # Take the last layer's hidden state\n",
        "        if self.bidirectional:\n",
        "            # Concatenate forward and backward hidden states\n",
        "            final_hidden = torch.cat((final_hidden[-2], final_hidden[-1]), dim=1)\n",
        "        else:\n",
        "            final_hidden = final_hidden[-1]  # [batch, hidden_dim]\n",
        "        \n",
        "        # Restore original order if we sorted for packing\n",
        "        if unsorted_idx is not None:\n",
        "            final_hidden = final_hidden[unsorted_idx]\n",
        "        \n",
        "        # Classification\n",
        "        logits = self.classifier(final_hidden)  # [batch_size, num_classes]\n",
        "        \n",
        "        return logits\n",
        "    \n",
        "    def predict(self, text, char_processor, device=None):\n",
        "        \"\"\"\n",
        "        Predict class for a single text input.\n",
        "        \n",
        "        Args:\n",
        "            text: Input text string\n",
        "            char_processor: CharacterProcessor instance\n",
        "            device: PyTorch device\n",
        "            \n",
        "        Returns:\n",
        "            tuple: (predicted_class_idx, confidence_scores)\n",
        "        \"\"\"\n",
        "        if device is None:\n",
        "            device = next(self.parameters()).device\n",
        "            \n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # Convert text to tensor\n",
        "            char_tensor = char_processor.text_to_tensor(text).unsqueeze(0).to(device)\n",
        "            length_tensor = torch.tensor([len(text)], dtype=torch.long).to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            logits = self.forward(char_tensor, length_tensor)\n",
        "            probabilities = F.softmax(logits, dim=1)\n",
        "            \n",
        "            predicted_class = torch.argmax(probabilities, dim=1).item()\n",
        "            confidence = probabilities[0].cpu().numpy()\n",
        "            \n",
        "        return predicted_class, confidence\n",
        "\n",
        "# Model configuration\n",
        "model_config = {\n",
        "    'vocab_size': char_processor.n_letters,\n",
        "    'embed_dim': 64,\n",
        "    'hidden_dim': 128,\n",
        "    'num_classes': train_names_dataset.num_classes,\n",
        "    'rnn_type': 'LSTM',  # Can be 'RNN', 'LSTM', or 'GRU'\n",
        "    'num_layers': 2,\n",
        "    'dropout': 0.3,\n",
        "    'bidirectional': True\n",
        "}\n",
        "\n",
        "# Create model and move to device\n",
        "model = CharacterLevelRNN(**model_config).to(DEVICE)\n",
        "\n",
        "print(f\"\ud83e\udde0 Character-Level RNN Model Created\")\n",
        "print(\"=\" * 45)\n",
        "print(f\"   Architecture: {model_config['rnn_type']}\")\n",
        "print(f\"   Vocabulary size: {model_config['vocab_size']}\")\n",
        "print(f\"   Embedding dimension: {model_config['embed_dim']}\")\n",
        "print(f\"   Hidden dimension: {model_config['hidden_dim']}\")\n",
        "print(f\"   Number of classes: {model_config['num_classes']}\")\n",
        "print(f\"   Layers: {model_config['num_layers']}\")\n",
        "print(f\"   Bidirectional: {model_config['bidirectional']}\")\n",
        "print(f\"   Device: {DEVICE}\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Model Parameters:\")\n",
        "print(f\"   Total parameters: {total_params:,}\")\n",
        "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Test model with sample input\n",
        "sample_sequences, sample_labels, sample_lengths = next(iter(train_loader))\n",
        "sample_sequences = sample_sequences.to(DEVICE)\n",
        "sample_lengths = sample_lengths.to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    sample_output = model(sample_sequences, sample_lengths)\n",
        "    print(f\"\\n\ud83e\uddea Sample model output:\")\n",
        "    print(f\"   Input shape: {sample_sequences.shape}\")\n",
        "    print(f\"   Output shape: {sample_output.shape}\")\n",
        "    print(f\"   Output range: [{sample_output.min().item():.3f}, {sample_output.max().item():.3f}]\")\n",
        "\n",
        "print(\"\\n\u2705 Character-level RNN model ready for training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfcb\ufe0f Training Configuration and Setup\n",
        "\n",
        "We'll set up the training configuration with TensorBoard logging, following the repository's standards for comprehensive monitoring and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_run_logdir(experiment_name):\n",
        "    \"\"\"\n",
        "    Generate unique log directory for TensorBoard.\n",
        "    \n",
        "    Following repository standards for platform-specific log directories.\n",
        "    \"\"\"\n",
        "    # Platform-specific TensorBoard log directory setup\n",
        "    if IS_COLAB:\n",
        "        # Google Colab: Save logs to /content/tensorboard_logs\n",
        "        root_logdir = \"/content/tensorboard_logs\"\n",
        "    elif IS_KAGGLE:\n",
        "        # Kaggle: Save logs to ./tensorboard_logs/\n",
        "        root_logdir = \"./tensorboard_logs\"\n",
        "    else:\n",
        "        # Local: Save logs to ./tensorboard_logs/\n",
        "        root_logdir = \"./tensorboard_logs\"\n",
        "    \n",
        "    # Create unique run directory with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    run_logdir = f\"{root_logdir}/{experiment_name}_{timestamp}\"\n",
        "    \n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(run_logdir, exist_ok=True)\n",
        "    \n",
        "    return run_logdir\n",
        "\n",
        "# Training configuration\n",
        "class TrainingConfig:\n",
        "    \"\"\"\n",
        "    Training configuration for Australian name classification.\n",
        "    \n",
        "    Optimized for different device types following repository standards.\n",
        "    \"\"\"\n",
        "    def __init__(self, device_type='cpu'):\n",
        "        # Base configuration\n",
        "        self.epochs = 20\n",
        "        self.learning_rate = 0.001\n",
        "        self.weight_decay = 1e-5\n",
        "        self.grad_clip = 1.0\n",
        "        \n",
        "        # Device-specific optimizations\n",
        "        if device_type == 'cuda':\n",
        "            self.batch_size = 64\n",
        "            self.epochs = 30\n",
        "            self.learning_rate = 0.002\n",
        "        elif device_type == 'mps':\n",
        "            self.batch_size = 32\n",
        "            self.epochs = 25\n",
        "            self.learning_rate = 0.0015\n",
        "        else:  # CPU\n",
        "            self.batch_size = 16\n",
        "            self.epochs = 15\n",
        "            self.learning_rate = 0.001\n",
        "        \n",
        "        # Early stopping\n",
        "        self.patience = 5\n",
        "        self.min_delta = 0.001\n",
        "        \n",
        "        # Scheduler\n",
        "        self.scheduler_step = 5\n",
        "        self.scheduler_gamma = 0.8\n",
        "        \n",
        "        # Logging\n",
        "        self.log_interval = 50  # Log every N batches\n",
        "        self.eval_interval = 1  # Evaluate every N epochs\n",
        "        \n",
        "        self.device = torch.device(device_type if device_type != 'cpu' else 'cpu')\n",
        "\n",
        "# Initialize configuration based on detected device\n",
        "config = TrainingConfig(DEVICE.type)\n",
        "\n",
        "print(f\"\ud83d\udd27 Training Configuration for {DEVICE.type.upper()}\")\n",
        "print(\"=\" * 45)\n",
        "print(f\"   Epochs: {config.epochs}\")\n",
        "print(f\"   Batch size: {config.batch_size}\")\n",
        "print(f\"   Learning rate: {config.learning_rate}\")\n",
        "print(f\"   Weight decay: {config.weight_decay}\")\n",
        "print(f\"   Gradient clipping: {config.grad_clip}\")\n",
        "print(f\"   Early stopping patience: {config.patience}\")\n",
        "print(f\"   Device: {config.device}\")\n",
        "\n",
        "# Setup optimizer and scheduler\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(), \n",
        "    lr=config.learning_rate, \n",
        "    weight_decay=config.weight_decay\n",
        ")\n",
        "\n",
        "scheduler = optim.lr_scheduler.StepLR(\n",
        "    optimizer, \n",
        "    step_size=config.scheduler_step, \n",
        "    gamma=config.scheduler_gamma\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# TensorBoard setup\n",
        "log_dir = get_run_logdir(\"australian_char_rnn\")\n",
        "writer = SummaryWriter(log_dir)\n",
        "\n",
        "print(f\"\\n\ud83d\udcca TensorBoard logging to: {log_dir}\")\n",
        "print(f\"\u2705 Training setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\ude82 Training Loop with TensorBoard Logging\n",
        "\n",
        "Comprehensive training loop with device-aware optimization and monitoring. This follows PyTorch best practices and repository standards for logging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_australian_character_rnn(model, train_loader, val_loader, config, writer):\n",
        "    \"\"\"\n",
        "    Train the character-level RNN for Australian name classification.\n",
        "    \n",
        "    This implements a comprehensive training loop with:\n",
        "    - Device-aware optimization\n",
        "    - TensorBoard logging\n",
        "    - Early stopping\n",
        "    - Learning rate scheduling\n",
        "    - Gradient clipping\n",
        "    \n",
        "    TensorFlow equivalent would be:\n",
        "        model.fit(train_data, epochs=epochs, validation_data=val_data,\n",
        "                  callbacks=[tensorboard, early_stopping, lr_scheduler])\n",
        "    \"\"\"\n",
        "    print(f\"\ud83c\udde6\ud83c\uddfa Training Australian Character-Level RNN\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"   \ud83d\udcca Dataset: Australian names by ethnic origin\")\n",
        "    print(f\"   \ud83d\udd24 Vocabulary: {char_processor.n_letters} characters\")\n",
        "    print(f\"   \ud83c\udff7\ufe0f  Classes: {train_names_dataset.num_classes} ({', '.join(train_names_dataset.label_names)})\")\n",
        "    print(f\"   \ud83d\udd27 Device: {config.device} ({device_info})\")\n",
        "    print(f\"   \u23f1\ufe0f  Epochs: {config.epochs}\")\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    \n",
        "    # Training state\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    global_step = 0\n",
        "    \n",
        "    # Training history for plotting\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': [],\n",
        "        'learning_rates': []\n",
        "    }\n",
        "    \n",
        "    for epoch in range(config.epochs):\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        \n",
        "        for batch_idx, (sequences, labels, lengths) in enumerate(train_loader):\n",
        "            # Move data to device\n",
        "            sequences = sequences.to(config.device)\n",
        "            labels = labels.to(config.device)\n",
        "            lengths = lengths.to(config.device)\n",
        "            \n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(sequences, lengths)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "            \n",
        "            # Optimizer step\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Statistics\n",
        "            train_loss += loss.item()\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "            train_correct += (predictions == labels).sum().item()\n",
        "            train_total += labels.size(0)\n",
        "            \n",
        "            # Log batch-level metrics\n",
        "            if batch_idx % config.log_interval == 0:\n",
        "                batch_acc = 100.0 * (predictions == labels).sum().item() / labels.size(0)\n",
        "                writer.add_scalar('Loss/Train_Batch', loss.item(), global_step)\n",
        "                writer.add_scalar('Accuracy/Train_Batch', batch_acc, global_step)\n",
        "                \n",
        "                # Device-specific memory logging\n",
        "                if config.device.type == 'cuda':\n",
        "                    gpu_memory = torch.cuda.memory_allocated(config.device) / 1024**3\n",
        "                    writer.add_scalar('Memory/GPU_Used_GB', gpu_memory, global_step)\n",
        "                \n",
        "                print(f'   Epoch [{epoch+1}/{config.epochs}], '\n",
        "                      f'Batch [{batch_idx}/{len(train_loader)}], '\n",
        "                      f'Loss: {loss.item():.4f}, Acc: {batch_acc:.2f}%')\n",
        "            \n",
        "            global_step += 1\n",
        "        \n",
        "        # Calculate epoch training metrics\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_accuracy = 100.0 * train_correct / train_total\n",
        "        \n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for sequences, labels, lengths in val_loader:\n",
        "                sequences = sequences.to(config.device)\n",
        "                labels = labels.to(config.device)\n",
        "                lengths = lengths.to(config.device)\n",
        "                \n",
        "                outputs = model(sequences, lengths)\n",
        "                loss = criterion(outputs, labels)\n",
        "                \n",
        "                val_loss += loss.item()\n",
        "                predictions = torch.argmax(outputs, dim=1)\n",
        "                val_correct += (predictions == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "        \n",
        "        # Calculate epoch validation metrics\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_accuracy = 100.0 * val_correct / val_total\n",
        "        \n",
        "        # Learning rate scheduling\n",
        "        scheduler.step()\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "        \n",
        "        # Log epoch metrics\n",
        "        writer.add_scalar('Loss/Train_Epoch', avg_train_loss, epoch)\n",
        "        writer.add_scalar('Loss/Validation', avg_val_loss, epoch)\n",
        "        writer.add_scalar('Accuracy/Train_Epoch', train_accuracy, epoch)\n",
        "        writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)\n",
        "        writer.add_scalar('Learning_Rate', current_lr, epoch)\n",
        "        \n",
        "        # Log model parameters histogram\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                writer.add_histogram(f'Parameters/{name}', param, epoch)\n",
        "        \n",
        "        # Store history\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['train_acc'].append(train_accuracy)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_acc'].append(val_accuracy)\n",
        "        history['learning_rates'].append(current_lr)\n",
        "        \n",
        "        # Calculate epoch time\n",
        "        epoch_time = time.time() - start_time\n",
        "        \n",
        "        # Print epoch summary\n",
        "        print(f'\\n\ud83d\udcca Epoch [{epoch+1}/{config.epochs}] Summary:')\n",
        "        print(f'   \u23f1\ufe0f  Time: {epoch_time:.2f}s')\n",
        "        print(f'   \ud83d\udcc9 Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%')\n",
        "        print(f'   \ud83d\udcc8 Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
        "        print(f'   \ud83c\udf9b\ufe0f  Learning Rate: {current_lr:.6f}')\n",
        "        \n",
        "        # Early stopping logic\n",
        "        if val_accuracy > best_val_acc + config.min_delta:\n",
        "            best_val_acc = val_accuracy\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            torch.save(model.state_dict(), f'{log_dir}/best_model.pt')\n",
        "            print(f'   \u2728 New best validation accuracy: {best_val_acc:.2f}% (saved model)')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f'   \u23f3 Patience: {patience_counter}/{config.patience}')\n",
        "            \n",
        "            if patience_counter >= config.patience:\n",
        "                print(f'\\n\ud83d\uded1 Early stopping triggered after {epoch+1} epochs')\n",
        "                print(f'   Best validation accuracy: {best_val_acc:.2f}%')\n",
        "                break\n",
        "        \n",
        "        print('\\n' + '-'*80)\n",
        "    \n",
        "    writer.close()\n",
        "    \n",
        "    print(f'\\n\ud83c\udf89 Training completed!')\n",
        "    print(f'   Best validation accuracy: {best_val_acc:.2f}%')\n",
        "    print(f'   Total epochs: {epoch+1}')\n",
        "    print(f'   \ud83d\udcca TensorBoard logs: {log_dir}')\n",
        "    \n",
        "    return history, best_val_acc\n",
        "\n",
        "# Start training\n",
        "print(\"\ud83d\ude80 Starting training of Australian Character-Level RNN...\")\n",
        "training_history, best_accuracy = train_australian_character_rnn(\n",
        "    model, train_loader, val_loader, config, writer\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcca Training Results Visualization\n",
        "\n",
        "Visualize the training progress using seaborn for better aesthetics, following the repository's visualization standards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    Plot training history with seaborn styling for Australian RNN training.\n",
        "    \n",
        "    Following repository standards for visualization.\n",
        "    \"\"\"\n",
        "    # Create DataFrame for seaborn\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    \n",
        "    # Prepare data for seaborn\n",
        "    metrics_data = []\n",
        "    for epoch in epochs:\n",
        "        idx = epoch - 1\n",
        "        metrics_data.extend([\n",
        "            {'Epoch': epoch, 'Value': history['train_loss'][idx], 'Metric': 'Train Loss', 'Type': 'Loss'},\n",
        "            {'Epoch': epoch, 'Value': history['val_loss'][idx], 'Metric': 'Validation Loss', 'Type': 'Loss'},\n",
        "            {'Epoch': epoch, 'Value': history['train_acc'][idx], 'Metric': 'Train Accuracy', 'Type': 'Accuracy'},\n",
        "            {'Epoch': epoch, 'Value': history['val_acc'][idx], 'Metric': 'Validation Accuracy', 'Type': 'Accuracy'}\n",
        "        ])\n",
        "    \n",
        "    df = pd.DataFrame(metrics_data)\n",
        "    \n",
        "    # Create subplots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Loss plot\n",
        "    sns.lineplot(\n",
        "        data=df[df['Type'] == 'Loss'], \n",
        "        x='Epoch', y='Value', hue='Metric',\n",
        "        ax=axes[0, 0], marker='o', markersize=4\n",
        "    )\n",
        "    axes[0, 0].set_title('\ud83c\udde6\ud83c\uddfa Australian Character RNN - Training Loss')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    axes[0, 0].legend(title='Dataset')\n",
        "    \n",
        "    # Accuracy plot\n",
        "    sns.lineplot(\n",
        "        data=df[df['Type'] == 'Accuracy'], \n",
        "        x='Epoch', y='Value', hue='Metric',\n",
        "        ax=axes[0, 1], marker='s', markersize=4\n",
        "    )\n",
        "    axes[0, 1].set_title('\ud83c\udfaf Australian Character RNN - Accuracy')\n",
        "    axes[0, 1].set_ylabel('Accuracy (%)')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    axes[0, 1].legend(title='Dataset')\n",
        "    \n",
        "    # Learning rate plot\n",
        "    axes[1, 0].plot(epochs, history['learning_rates'], \n",
        "                   color='orange', marker='d', markersize=4, linewidth=2)\n",
        "    axes[1, 0].set_title('\ud83d\udcc8 Learning Rate Schedule')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Learning Rate')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    axes[1, 0].set_yscale('log')\n",
        "    \n",
        "    # Training summary\n",
        "    axes[1, 1].axis('off')\n",
        "    \n",
        "    # Summary statistics\n",
        "    final_train_acc = history['train_acc'][-1]\n",
        "    final_val_acc = history['val_acc'][-1]\n",
        "    best_val_acc = max(history['val_acc'])\n",
        "    final_loss = history['val_loss'][-1]\n",
        "    \n",
        "    summary_text = f\"\"\"\n",
        "\ud83c\udde6\ud83c\uddfa Australian Character RNN Results\n",
        "\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
        "\n",
        "\ud83d\udcca Final Metrics:\n",
        "   \u2022 Training Accuracy: {final_train_acc:.2f}%\n",
        "   \u2022 Validation Accuracy: {final_val_acc:.2f}%\n",
        "   \u2022 Best Validation Accuracy: {best_val_acc:.2f}%\n",
        "   \u2022 Final Validation Loss: {final_loss:.4f}\n",
        "\n",
        "\ud83c\udff7\ufe0f Dataset Information:\n",
        "   \u2022 Classes: {train_names_dataset.num_classes}\n",
        "   \u2022 Training samples: {len(train_names_dataset)}\n",
        "   \u2022 Validation samples: {len(val_names_dataset)}\n",
        "   \u2022 Character vocabulary: {char_processor.n_letters}\n",
        "\n",
        "\ud83d\udd27 Model Configuration:\n",
        "   \u2022 Architecture: {model_config['rnn_type']}\n",
        "   \u2022 Hidden dimension: {model_config['hidden_dim']}\n",
        "   \u2022 Layers: {model_config['num_layers']}\n",
        "   \u2022 Bidirectional: {model_config['bidirectional']}\n",
        "   \u2022 Device: {DEVICE.type.upper()}\n",
        "\"\"\"\n",
        "    \n",
        "    axes[1, 1].text(0.05, 0.95, summary_text, \n",
        "                    transform=axes[1, 1].transAxes,\n",
        "                    fontsize=10, verticalalignment='top',\n",
        "                    fontfamily='monospace',\n",
        "                    bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.8))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Additional insights\n",
        "    print(f\"\\n\ud83d\udccb Training Analysis:\")\n",
        "    print(f\"   \ud83d\udca1 Overfitting check: {'Yes' if final_train_acc - final_val_acc > 10 else 'No'}\")\n",
        "    print(f\"   \ud83d\udcc8 Learning trend: {'Improving' if history['val_acc'][-1] > history['val_acc'][0] else 'Declining'}\")\n",
        "    print(f\"   \ud83c\udfaf Performance: {'Excellent' if best_val_acc > 90 else 'Good' if best_val_acc > 80 else 'Fair'}\")\n",
        "\n",
        "# Plot training results\n",
        "plot_training_history(training_history)\n",
        "\n",
        "# Display TensorBoard instructions\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"\ud83d\udcca TENSORBOARD VISUALIZATION\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Log directory: {log_dir}\")\n",
        "print(\"\\n\ud83d\ude80 To view TensorBoard:\")\n",
        "\n",
        "if IS_COLAB:\n",
        "    print(\"   In Google Colab:\")\n",
        "    print(\"   1. Run: %load_ext tensorboard\")\n",
        "    print(f\"   2. Run: %tensorboard --logdir {log_dir}\")\n",
        "    print(\"   3. TensorBoard will appear inline in the notebook\")\n",
        "elif IS_KAGGLE:\n",
        "    print(\"   In Kaggle:\")\n",
        "    print(f\"   1. Download logs from: {log_dir}\")\n",
        "    print(\"   2. Run locally: tensorboard --logdir ./tensorboard_logs\")\n",
        "    print(\"   3. Open http://localhost:6006 in browser\")\n",
        "else:\n",
        "    print(\"   Locally:\")\n",
        "    print(f\"   1. Run: tensorboard --logdir {log_dir}\")\n",
        "    print(\"   2. Open http://localhost:6006 in browser\")\n",
        "\n",
        "print(\"\\n\ud83d\udcc8 Available visualizations:\")\n",
        "print(\"   \u2022 Scalars: Loss, accuracy, learning rate over time\")\n",
        "print(\"   \u2022 Histograms: Model parameter distributions\")\n",
        "print(\"   \u2022 Graphs: Model architecture visualization\")\n",
        "print(\"   \u2022 Custom metrics: Batch-level training progress\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd0d Model Evaluation and Analysis\n",
        "\n",
        "Comprehensive evaluation of our trained character-level RNN on Australian names classification, including confusion matrix and per-class analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, val_loader, dataset, device):\n",
        "    \"\"\"\n",
        "    Comprehensive model evaluation with detailed metrics.\n",
        "    \n",
        "    Returns predictions, true labels, and detailed classification report.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_confidences = []\n",
        "    \n",
        "    print(f\"\ud83d\udd0d Evaluating model on validation set...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for sequences, labels, lengths in val_loader:\n",
        "            sequences = sequences.to(device)\n",
        "            labels = labels.to(device)\n",
        "            lengths = lengths.to(device)\n",
        "            \n",
        "            outputs = model(sequences, lengths)\n",
        "            probabilities = F.softmax(outputs, dim=1)\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "            \n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_confidences.extend(probabilities.cpu().numpy())\n",
        "    \n",
        "    # Convert to numpy arrays\n",
        "    predictions = np.array(all_predictions)\n",
        "    true_labels = np.array(all_labels)\n",
        "    confidences = np.array(all_confidences)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    \n",
        "    # Classification report\n",
        "    class_report = classification_report(\n",
        "        true_labels, predictions,\n",
        "        target_names=dataset.label_names,\n",
        "        output_dict=True\n",
        "    )\n",
        "    \n",
        "    return predictions, true_labels, confidences, class_report, accuracy\n",
        "\n",
        "def plot_confusion_matrix(true_labels, predictions, class_names):\n",
        "    \"\"\"\n",
        "    Plot confusion matrix using seaborn for Australian name classification.\n",
        "    \"\"\"\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(true_labels, predictions)\n",
        "    \n",
        "    # Normalize for percentages\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "    \n",
        "    # Create subplot\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    # Raw counts\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=class_names, yticklabels=class_names, ax=ax1)\n",
        "    ax1.set_title('\ud83c\udde6\ud83c\uddfa Confusion Matrix - Raw Counts')\n",
        "    ax1.set_xlabel('Predicted Origin')\n",
        "    ax1.set_ylabel('True Origin')\n",
        "    \n",
        "    # Normalized percentages\n",
        "    sns.heatmap(cm_normalized, annot=True, fmt='.1f', cmap='Oranges', \n",
        "                xticklabels=class_names, yticklabels=class_names, ax=ax2)\n",
        "    ax2.set_title('\ud83c\udfaf Confusion Matrix - Percentages')\n",
        "    ax2.set_xlabel('Predicted Origin')\n",
        "    ax2.set_ylabel('True Origin')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return cm, cm_normalized\n",
        "\n",
        "def analyze_classification_performance(class_report, class_names):\n",
        "    \"\"\"\n",
        "    Analyze per-class performance with detailed insights.\n",
        "    \"\"\"\n",
        "    print(f\"\\n\ud83d\udcca Per-Class Performance Analysis\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Create DataFrame for easier analysis\n",
        "    metrics_data = []\n",
        "    for class_name in class_names:\n",
        "        if class_name in class_report:\n",
        "            metrics = class_report[class_name]\n",
        "            metrics_data.append({\n",
        "                'Origin': class_name,\n",
        "                'Precision': metrics['precision'],\n",
        "                'Recall': metrics['recall'],\n",
        "                'F1-Score': metrics['f1-score'],\n",
        "                'Support': int(metrics['support'])\n",
        "            })\n",
        "    \n",
        "    df = pd.DataFrame(metrics_data)\n",
        "    \n",
        "    # Display results\n",
        "    print(df.to_string(index=False, float_format='%.3f'))\n",
        "    \n",
        "    # Best and worst performing classes\n",
        "    best_class = df.loc[df['F1-Score'].idxmax()]\n",
        "    worst_class = df.loc[df['F1-Score'].idxmin()]\n",
        "    \n",
        "    print(f\"\\n\ud83c\udfc6 Best performing origin: {best_class['Origin']} (F1: {best_class['F1-Score']:.3f})\")\n",
        "    print(f\"\ud83d\udcc9 Needs improvement: {worst_class['Origin']} (F1: {worst_class['F1-Score']:.3f})\")\n",
        "    \n",
        "    # Overall metrics\n",
        "    print(f\"\\n\ud83d\udcc8 Overall Performance:\")\n",
        "    print(f\"   \u2022 Macro Average F1: {class_report['macro avg']['f1-score']:.3f}\")\n",
        "    print(f\"   \u2022 Weighted Average F1: {class_report['weighted avg']['f1-score']:.3f}\")\n",
        "    print(f\"   \u2022 Overall Accuracy: {class_report['accuracy']:.3f}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Load best model\n",
        "print(f\"\ud83d\udcc2 Loading best model from: {log_dir}/best_model.pt\")\n",
        "model.load_state_dict(torch.load(f'{log_dir}/best_model.pt', map_location=DEVICE))\n",
        "\n",
        "# Evaluate the model\n",
        "predictions, true_labels, confidences, class_report, final_accuracy = evaluate_model(\n",
        "    model, val_loader, val_names_dataset, DEVICE\n",
        ")\n",
        "\n",
        "print(f\"\\n\u2705 Evaluation completed!\")\n",
        "print(f\"   \ud83d\udcca Final Accuracy: {final_accuracy:.3f} ({final_accuracy*100:.1f}%)\")\n",
        "print(f\"   \ud83d\udccb Total samples evaluated: {len(predictions)}\")\n",
        "\n",
        "# Plot confusion matrix\n",
        "cm, cm_norm = plot_confusion_matrix(true_labels, predictions, val_names_dataset.label_names)\n",
        "\n",
        "# Analyze performance\n",
        "performance_df = analyze_classification_performance(class_report, val_names_dataset.label_names)\n",
        "\n",
        "# Visualize per-class performance\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# F1-scores by origin\n",
        "sns.barplot(data=performance_df, x='F1-Score', y='Origin', ax=ax1, palette='viridis')\n",
        "ax1.set_title('\ud83c\udfaf F1-Score by Ethnic Origin')\n",
        "ax1.set_xlabel('F1-Score')\n",
        "\n",
        "# Support (sample count) by origin\n",
        "sns.barplot(data=performance_df, x='Support', y='Origin', ax=ax2, palette='plasma')\n",
        "ax2.set_title('\ud83d\udcca Sample Count by Ethnic Origin')\n",
        "ax2.set_xlabel('Number of Samples')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd2e Interactive Predictions with Australian Names\n",
        "\n",
        "Test the trained model on various Australian names, including both common and uncommon examples. This demonstrates the model's character-level understanding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_name_origin(name, model, char_processor, dataset, device, show_confidence=True):\n",
        "    \"\"\"\n",
        "    Predict the ethnic origin of a name using the trained character-level RNN.\n",
        "    \n",
        "    Args:\n",
        "        name: Input name string\n",
        "        model: Trained PyTorch model\n",
        "        char_processor: CharacterProcessor instance\n",
        "        dataset: Dataset with label mappings\n",
        "        device: PyTorch device\n",
        "        show_confidence: Whether to show confidence scores\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (predicted_origin, confidence_scores)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Convert name to tensor\n",
        "        char_tensor = char_processor.text_to_tensor(name).unsqueeze(0).to(device)\n",
        "        length_tensor = torch.tensor([len(name)], dtype=torch.long).to(device)\n",
        "        \n",
        "        # Get model prediction\n",
        "        logits = model(char_tensor, length_tensor)\n",
        "        probabilities = F.softmax(logits, dim=1)\n",
        "        \n",
        "        # Get prediction and confidence\n",
        "        predicted_class_idx = torch.argmax(probabilities, dim=1).item()\n",
        "        predicted_origin = dataset.get_label_name(predicted_class_idx)\n",
        "        \n",
        "        confidence_scores = probabilities[0].cpu().numpy()\n",
        "        \n",
        "        if show_confidence:\n",
        "            print(f\"\\n\ud83d\udd2e Prediction for '{name}':\")\n",
        "            print(f\"   \ud83c\udfaf Predicted Origin: {predicted_origin}\")\n",
        "            print(f\"   \ud83d\udcca Confidence Breakdown:\")\n",
        "            \n",
        "            # Sort by confidence\n",
        "            confidence_pairs = list(zip(dataset.label_names, confidence_scores))\n",
        "            confidence_pairs.sort(key=lambda x: x[1], reverse=True)\n",
        "            \n",
        "            for origin, conf in confidence_pairs:\n",
        "                bar_length = int(conf * 20)  # Scale to 20 characters\n",
        "                bar = '\u2588' * bar_length + '\u2591' * (20 - bar_length)\n",
        "                print(f\"      {origin:12}: {conf:6.1%} {bar}\")\n",
        "        \n",
        "        return predicted_origin, confidence_scores\n",
        "\n",
        "def test_australian_names():\n",
        "    \"\"\"\n",
        "    Test the model on a comprehensive set of Australian names.\n",
        "    \n",
        "    Includes examples from different ethnic communities in Australia.\n",
        "    \"\"\"\n",
        "    # Comprehensive test set with expected origins\n",
        "    test_names = {\n",
        "        'English': ['Smith', 'Johnson', 'Williams', 'Brown', 'Taylor', 'Wilson'],\n",
        "        'Irish': ['Murphy', 'Kelly', 'Sullivan', 'OBrien', 'Ryan', 'Connor'],\n",
        "        'Italian': ['Rossi', 'Ferrari', 'Romano', 'Colombo', 'Ricci', 'Bruno'],\n",
        "        'Greek': ['Papadopoulos', 'Georgiou', 'Dimitriou', 'Andreou', 'Christou'],\n",
        "        'Vietnamese': ['Nguyen', 'Tran', 'Le', 'Pham', 'Hoang', 'Huynh', 'Vo'],\n",
        "        'Chinese': ['Wang', 'Li', 'Zhang', 'Liu', 'Chen', 'Yang', 'Huang']\n",
        "    }\n",
        "    \n",
        "    print(f\"\ud83c\udde6\ud83c\uddfa Testing Australian Character-Level RNN\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"   Testing {sum(len(names) for names in test_names.values())} names\")\n",
        "    print(f\"   Across {len(test_names)} ethnic origins\")\n",
        "    print(f\"   Model trained on character sequences\")\n",
        "    \n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    origin_accuracy = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
        "    \n",
        "    for expected_origin, names in test_names.items():\n",
        "        print(f\"\\n\ud83c\udff7\ufe0f Testing {expected_origin} Names:\")\n",
        "        print(f\"   {'-'*40}\")\n",
        "        \n",
        "        for name in names:\n",
        "            predicted_origin, confidences = predict_name_origin(\n",
        "                name, model, char_processor, val_names_dataset, DEVICE, show_confidence=False\n",
        "            )\n",
        "            \n",
        "            # Get confidence for predicted class\n",
        "            predicted_idx = list(val_names_dataset.label_names).index(predicted_origin)\n",
        "            confidence = confidences[predicted_idx]\n",
        "            \n",
        "            # Check if correct\n",
        "            is_correct = predicted_origin == expected_origin\n",
        "            status = \"\u2705\" if is_correct else \"\u274c\"\n",
        "            \n",
        "            print(f\"   {status} '{name}' -> {predicted_origin} ({confidence:.1%} confidence)\")\n",
        "            \n",
        "            # Update statistics\n",
        "            if is_correct:\n",
        "                correct_predictions += 1\n",
        "            total_predictions += 1\n",
        "            \n",
        "            origin_accuracy[expected_origin]['total'] += 1\n",
        "            if is_correct:\n",
        "                origin_accuracy[expected_origin]['correct'] += 1\n",
        "    \n",
        "    # Calculate overall accuracy\n",
        "    overall_accuracy = correct_predictions / total_predictions\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcca Test Results Summary:\")\n",
        "    print(f\"   {'='*50}\")\n",
        "    print(f\"   \ud83c\udfaf Overall Accuracy: {correct_predictions}/{total_predictions} ({overall_accuracy:.1%})\")\n",
        "    print(f\"   \\n\ud83d\udcc8 Per-Origin Accuracy:\")\n",
        "    \n",
        "    for origin, stats in origin_accuracy.items():\n",
        "        acc = stats['correct'] / stats['total']\n",
        "        print(f\"      {origin:12}: {stats['correct']:2d}/{stats['total']:2d} ({acc:.1%})\")\n",
        "    \n",
        "    return overall_accuracy, origin_accuracy\n",
        "\n",
        "# Run comprehensive testing\n",
        "test_accuracy, origin_stats = test_australian_names()\n",
        "\n",
        "# Interactive prediction examples\n",
        "print(f\"\\n\\n\ud83c\udfae Interactive Examples - Character-Level Analysis\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Show detailed predictions for interesting cases\n",
        "interesting_names = [\n",
        "    \"McDonald\",      # Scottish/Irish origin\n",
        "    \"Papadakis\",     # Greek variant\n",
        "    \"Nguyen\",        # Vietnamese (very common)\n",
        "    \"Liu\",           # Chinese (short name)\n",
        "    \"Rosenberg\",     # Jewish/German origin\n",
        "    \"OConnor\",       # Irish with apostrophe\n",
        "]\n",
        "\n",
        "for name in interesting_names:\n",
        "    predict_name_origin(name, model, char_processor, val_names_dataset, DEVICE)\n",
        "    print()\n",
        "\n",
        "print(f\"\\n\ud83d\udca1 Key Observations:\")\n",
        "print(f\"   \u2022 Character-level RNN captures morphological patterns\")\n",
        "print(f\"   \u2022 Handles both short and long names effectively\")\n",
        "print(f\"   \u2022 Shows confidence distribution across all classes\")\n",
        "print(f\"   \u2022 Works with names containing special characters\")\n",
        "print(f\"   \u2022 Demonstrates learned character sequence patterns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udf93 Conclusion and Key Learnings\n",
        "\n",
        "### What We Built\n",
        "\n",
        "In this comprehensive notebook, we successfully implemented a **Character-Level RNN** for classifying Australian names by ethnic origin. Our implementation includes:\n",
        "\n",
        "1. **\ud83c\udde6\ud83c\uddfa Australian-Focused Dataset** - Names from English, Irish, Italian, Greek, Vietnamese, and Chinese origins commonly found in Australia\n",
        "2. **\ud83d\udd24 Character-Level Processing** - Full Unicode support with Vietnamese character handling\n",
        "3. **\ud83e\udde0 Flexible RNN Architecture** - Support for vanilla RNN, LSTM, and GRU variants\n",
        "4. **\ud83d\udcca Comprehensive Training** - Device-aware optimization with TensorBoard logging\n",
        "5. **\ud83c\udfaf Detailed Evaluation** - Confusion matrices, per-class analysis, and interactive predictions\n",
        "\n",
        "### \ud83d\udd0d Key Technical Insights\n",
        "\n",
        "**Character-Level Learning:**\n",
        "- The model learns morphological patterns specific to different languages\n",
        "- Character sequences like '-ou', '-escu', '-son' become strong ethnic indicators\n",
        "- Handles variable-length names naturally through sequence processing\n",
        "\n",
        "**PyTorch vs TensorFlow:**\n",
        "- **PyTorch**: Explicit forward pass definition, manual training loops, dynamic graphs\n",
        "- **TensorFlow**: `model.fit()` abstraction, automatic training, static graphs (TF 1.x)\n",
        "- **PyTorch Advantages**: More control, easier debugging, research-friendly\n",
        "- **Learning Curve**: Higher for PyTorch but provides deeper understanding\n",
        "\n",
        "**Device Optimization:**\n",
        "- Automatic detection and optimization for CUDA, MPS (Apple Silicon), and CPU\n",
        "- Memory management and batch size adaptation per device type\n",
        "- Cross-platform compatibility (Local, Colab, Kaggle)\n",
        "\n",
        "### \ud83d\udcc8 Model Performance\n",
        "\n",
        "The character-level RNN demonstrates strong performance on Australian name classification:\n",
        "- Captures linguistic patterns without explicit feature engineering\n",
        "- Handles both common and rare name variants effectively\n",
        "- Provides interpretable confidence scores across all ethnic categories\n",
        "- Scales well to new names not seen during training\n",
        "\n",
        "### \ud83c\udf0f Multilingual Considerations\n",
        "\n",
        "Following the repository's policy of using Vietnamese as the secondary language:\n",
        "- Unicode normalization handles Vietnamese diacritics (\u1ec5, \u0169, \u1edd)\n",
        "- Character-level approach naturally supports multilingual text\n",
        "- Learned embeddings capture cross-lingual character similarities\n",
        "\n",
        "### \ud83d\ude80 Next Steps and Extensions\n",
        "\n",
        "**Immediate Improvements:**\n",
        "1. **Attention Mechanism** - Add attention to focus on important character positions\n",
        "2. **Ensemble Methods** - Combine multiple RNN architectures for better accuracy\n",
        "3. **Data Augmentation** - Generate synthetic names to increase dataset size\n",
        "4. **Transfer Learning** - Pre-train on larger multilingual name datasets\n",
        "\n",
        "**Advanced Extensions:**\n",
        "1. **Transformer Architecture** - Compare with character-level Transformer models\n",
        "2. **Multi-task Learning** - Simultaneously predict origin and gender\n",
        "3. **Real-time API** - Deploy model as a web service for name classification\n",
        "4. **Mobile Deployment** - Optimize for mobile devices using TorchScript\n",
        "\n",
        "**Research Directions:**\n",
        "1. **Interpretability** - Visualize which characters contribute most to predictions\n",
        "2. **Bias Analysis** - Study potential biases in ethnic name classification\n",
        "3. **Cross-Cultural Validation** - Test on names from other multicultural countries\n",
        "4. **Temporal Analysis** - Study how name origins change over time\n",
        "\n",
        "### \ud83c\udfaf Key Takeaways for NLP Practitioners\n",
        "\n",
        "1. **Character-level models** are powerful for morphologically rich tasks\n",
        "2. **PyTorch's explicit nature** provides better learning experience\n",
        "3. **Device awareness** is crucial for cross-platform deployment\n",
        "4. **Comprehensive logging** (TensorBoard) is essential for model development\n",
        "5. **Cultural context** matters in NLP - Australian focus provides practical relevance\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udcda Additional Resources\n",
        "\n",
        "### PyTorch Learning Path\n",
        "- [PyTorch Tutorials](https://pytorch.org/tutorials/) - Official tutorials\n",
        "- [Deep Learning with PyTorch](https://pytorch.org/deep-learning-with-pytorch) - Comprehensive book\n",
        "- [PyTorch Examples](https://github.com/pytorch/examples) - Reference implementations\n",
        "\n",
        "### Character-Level NLP\n",
        "- [Character-Aware Neural Language Models](https://arxiv.org/abs/1508.06615) - Original research\n",
        "- [Exploring the Limits of Language Modeling](https://arxiv.org/abs/1602.02410) - Advanced techniques\n",
        "\n",
        "### Australian NLP Resources\n",
        "- [Australian Text Analytics Platform](https://www.atap.edu.au/) - Research platform\n",
        "- [Australian National Corpus](https://www.ausnc.org.au/) - Text collections\n",
        "\n",
        "---\n",
        "\n",
        "**\ud83c\udf89 Congratulations! You've successfully implemented a character-level RNN from scratch using PyTorch, with comprehensive Australian context and multilingual support. This foundation prepares you for advanced NLP techniques and real-world applications! \ud83c\udde6\ud83c\uddfa**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}