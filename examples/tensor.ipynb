{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tensor Fundamentals: From TensorFlow to PyTorch\n",
    "\n",
    "This notebook provides a comprehensive introduction to PyTorch tensors, designed for learners transitioning from TensorFlow. We'll explore tensor creation, operations, and the NumPy bridge using Australian-themed examples.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand PyTorch tensor basics and how they compare to TensorFlow tensors\n",
    "- Master tensor creation, properties, and data types\n",
    "- Learn essential tensor operations for NLP and deep learning\n",
    "- Explore tensor indexing, slicing, and reshaping\n",
    "- Bridge between PyTorch tensors and NumPy arrays\n",
    "- Apply tensor operations to Australian tourism and multilingual text data\n",
    "\n",
    "## Key Differences from TensorFlow\n",
    "| Aspect | TensorFlow | PyTorch |\n",
    "|--------|------------|---------|\n",
    "| **Execution** | Graph-based (TF 1.x) or Eager (TF 2.x) | Always eager (dynamic graphs) |\n",
    "| **Tensor Creation** | `tf.constant([1, 2, 3])` | `torch.tensor([1, 2, 3])` |\n",
    "| **Random Tensors** | `tf.random.normal([2, 3])` | `torch.randn(2, 3)` |\n",
    "| **Reshaping** | `tf.reshape(x, [2, -1])` | `x.view(2, -1)` or `x.reshape(2, -1)` |\n",
    "| **Device Management** | Automatic with strategies | Explicit with `.to(device)` |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Runtime Detection\n",
    "\n",
    "Following PyTorch best practices for cross-platform compatibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Detection and Setup\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Detect the runtime environment\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules or \"kaggle\" in os.environ.get('KAGGLE_URL_BASE', '')\n",
    "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
    "\n",
    "print(f\"üåê Environment detected:\")\n",
    "print(f\"  - Local: {IS_LOCAL}\")\n",
    "print(f\"  - Google Colab: {IS_COLAB}\")\n",
    "print(f\"  - Kaggle: {IS_KAGGLE}\")\n",
    "\n",
    "# Platform-specific system setup\n",
    "if IS_COLAB:\n",
    "    print(\"\\nüîß Setting up Google Colab environment...\")\n",
    "    # Colab usually has PyTorch pre-installed\n",
    "elif IS_KAGGLE:\n",
    "    print(\"\\nüîß Setting up Kaggle environment...\")\n",
    "    # Kaggle usually has most packages pre-installed\n",
    "else:\n",
    "    print(\"\\nüîß Setting up local environment...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages based on platform\n",
    "required_packages = [\n",
    "    \"torch\",\n",
    "    \"numpy\",\n",
    "    \"matplotlib\",\n",
    "    \"pandas\"\n",
    "]\n",
    "\n",
    "print(\"üì¶ Installing required packages...\")\n",
    "for package in required_packages:\n",
    "    if IS_COLAB or IS_KAGGLE:\n",
    "        # Use IPython magic commands for notebook environments\n",
    "        try:\n",
    "            exec(f\"!pip install -q {package}\")\n",
    "            print(f\"‚úÖ {package}\")\n",
    "        except:\n",
    "            print(f\"‚ö†Ô∏è {package} (may already be installed)\")\n",
    "    else:\n",
    "        try:\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
    "                          capture_output=True, check=True)\n",
    "            print(f\"‚úÖ {package}\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"‚ö†Ô∏è {package} (may already be installed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify PyTorch installation and detect optimal device\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import platform\n",
    "\n",
    "def detect_device():\n",
    "    \"\"\"\n",
    "    Detect the best available PyTorch device with comprehensive hardware support.\n",
    "    \n",
    "    Priority order:\n",
    "    1. CUDA (NVIDIA GPUs) - Best performance for deep learning\n",
    "    2. MPS (Apple Silicon) - Optimized for M1/M2/M3 Macs  \n",
    "    3. CPU (Universal) - Always available fallback\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: The optimal device for PyTorch operations\n",
    "        str: Human-readable device description for logging\n",
    "    \"\"\"\n",
    "    # Check for CUDA (NVIDIA GPU)\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        device_info = f\"CUDA GPU: {gpu_name}\"\n",
    "        \n",
    "        print(f\"üöÄ Using CUDA acceleration\")\n",
    "        print(f\"   GPU: {gpu_name}\")\n",
    "        print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "        \n",
    "        return device, device_info\n",
    "    \n",
    "    # Check for MPS (Apple Silicon)\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        device_info = \"Apple Silicon MPS\"\n",
    "        \n",
    "        system_info = platform.uname()\n",
    "        \n",
    "        print(f\"üçé Using Apple Silicon MPS acceleration\")\n",
    "        print(f\"   System: {system_info.system} {system_info.release}\")\n",
    "        print(f\"   Machine: {system_info.machine}\")\n",
    "        \n",
    "        return device, device_info\n",
    "    \n",
    "    # Fallback to CPU\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        device_info = \"CPU (No GPU acceleration available)\"\n",
    "        \n",
    "        cpu_count = torch.get_num_threads()\n",
    "        system_info = platform.uname()\n",
    "        \n",
    "        print(f\"üíª Using CPU (no GPU acceleration detected)\")\n",
    "        print(f\"   Processor: {system_info.processor}\")\n",
    "        print(f\"   PyTorch Threads: {cpu_count}\")\n",
    "        print(f\"   System: {system_info.system} {system_info.release}\")\n",
    "        \n",
    "        return device, device_info\n",
    "\n",
    "# Detect and set up device\n",
    "device, device_info = detect_device()\n",
    "\n",
    "print(f\"\\n‚úÖ PyTorch {torch.__version__} ready!\")\n",
    "print(f\"üì± Device selected: {device}\")\n",
    "print(f\"üìä Device info: {device_info}\")\n",
    "\n",
    "# Set global device for the notebook\n",
    "DEVICE = device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Tensor Creation\n",
    "\n",
    "PyTorch tensors are the fundamental building blocks for deep learning. Let's explore different ways to create tensors using Australian-themed examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors from Python lists - Australian cities example\n",
    "print(\"üá¶üá∫ Creating tensors with Australian city data\\n\")\n",
    "\n",
    "# Australian cities population data (in millions, approximate)\n",
    "cities = [\"Sydney\", \"Melbourne\", \"Brisbane\", \"Perth\", \"Adelaide\", \"Gold Coast\", \"Newcastle\", \"Canberra\"]\n",
    "populations = [5.3, 5.1, 2.6, 2.1, 1.4, 0.7, 0.5, 0.5]  # millions\n",
    "\n",
    "# Create tensors from lists\n",
    "population_tensor = torch.tensor(populations, dtype=torch.float32)\n",
    "print(f\"Population tensor: {population_tensor}\")\n",
    "print(f\"Shape: {population_tensor.shape}\")\n",
    "print(f\"Data type: {population_tensor.dtype}\")\n",
    "print(f\"Device: {population_tensor.device}\")\n",
    "\n",
    "# TensorFlow comparison\n",
    "print(\"\\nüìä TensorFlow vs PyTorch Comparison:\")\n",
    "print(\"   TensorFlow: tf.constant([5.3, 5.1, 2.6, 2.1, 1.4, 0.7, 0.5, 0.5])\")\n",
    "print(f\"   PyTorch:    torch.tensor([5.3, 5.1, 2.6, 2.1, 1.4, 0.7, 0.5, 0.5])\")\n",
    "\n",
    "# Different data types\n",
    "print(\"\\nüî¢ Different tensor data types:\")\n",
    "int_tensor = torch.tensor([1, 2, 3, 4, 5], dtype=torch.int32)\n",
    "float_tensor = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0], dtype=torch.float32)\n",
    "bool_tensor = torch.tensor([True, False, True, False], dtype=torch.bool)\n",
    "\n",
    "print(f\"Integer tensor: {int_tensor} (dtype: {int_tensor.dtype})\")\n",
    "print(f\"Float tensor: {float_tensor} (dtype: {float_tensor.dtype})\")\n",
    "print(f\"Boolean tensor: {bool_tensor} (dtype: {bool_tensor.dtype})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 2D tensors - Australian tourism ratings\n",
    "print(\"üèñÔ∏è 2D Tensors: Australian Tourism Attraction Ratings\\n\")\n",
    "\n",
    "# Tourism ratings matrix: rows=attractions, columns=rating categories\n",
    "# Categories: [Overall, Scenery, Accessibility, Family-Friendly, Cost-Effective]\n",
    "attractions = [\"Sydney Opera House\", \"Great Barrier Reef\", \"Uluru\", \"Bondi Beach\", \"Melbourne Laneways\"]\n",
    "rating_categories = [\"Overall\", \"Scenery\", \"Accessibility\", \"Family-Friendly\", \"Cost-Effective\"]\n",
    "\n",
    "# Ratings out of 10\n",
    "tourism_ratings = [\n",
    "    [9.5, 9.8, 8.5, 8.0, 6.0],  # Sydney Opera House\n",
    "    [9.8, 10.0, 6.0, 8.5, 4.0], # Great Barrier Reef\n",
    "    [9.0, 10.0, 7.0, 7.5, 5.0], # Uluru\n",
    "    [8.5, 9.0, 9.5, 9.0, 9.5],  # Bondi Beach\n",
    "    [8.0, 7.5, 9.0, 7.0, 8.5]   # Melbourne Laneways\n",
    "]\n",
    "\n",
    "tourism_tensor = torch.tensor(tourism_ratings, dtype=torch.float32)\n",
    "print(f\"Tourism ratings tensor shape: {tourism_tensor.shape}\")\n",
    "print(f\"Tensor:\\n{tourism_tensor}\")\n",
    "\n",
    "# Display with labels for clarity\n",
    "print(\"\\nüìã Tourism Ratings Matrix:\")\n",
    "print(f\"{'Attraction':<20} {'Overall':<8} {'Scenery':<8} {'Access.':<8} {'Family':<8} {'Cost':<8}\")\n",
    "print(\"-\" * 70)\n",
    "for i, attraction in enumerate(attractions):\n",
    "    ratings = tourism_tensor[i]\n",
    "    print(f\"{attraction:<20} {ratings[0]:<8.1f} {ratings[1]:<8.1f} {ratings[2]:<8.1f} {ratings[3]:<8.1f} {ratings[4]:<8.1f}\")\n",
    "\n",
    "# Tensor properties\n",
    "print(f\"\\nüîç Tensor Properties:\")\n",
    "print(f\"   Dimensions: {tourism_tensor.ndim}\")\n",
    "print(f\"   Shape: {tourism_tensor.shape}\")\n",
    "print(f\"   Size: {tourism_tensor.numel()} elements\")\n",
    "print(f\"   Data type: {tourism_tensor.dtype}\")\n",
    "print(f\"   Device: {tourism_tensor.device}\")\n",
    "print(f\"   Memory usage: {tourism_tensor.element_size() * tourism_tensor.numel()} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tensor creation functions\n",
    "print(\"üéØ Special Tensor Creation Functions\\n\")\n",
    "\n",
    "# Zeros tensor - useful for initialization\n",
    "zeros_tensor = torch.zeros(3, 4)\n",
    "print(f\"Zeros tensor (3x4):\\n{zeros_tensor}\")\n",
    "\n",
    "# Ones tensor - useful for masks and weights\n",
    "ones_tensor = torch.ones(2, 3)\n",
    "print(f\"\\nOnes tensor (2x3):\\n{ones_tensor}\")\n",
    "\n",
    "# Identity matrix - essential for linear algebra\n",
    "identity_tensor = torch.eye(4)\n",
    "print(f\"\\nIdentity tensor (4x4):\\n{identity_tensor}\")\n",
    "\n",
    "# Random tensors - crucial for neural network initialization\n",
    "print(\"\\nüé≤ Random Tensor Creation:\")\n",
    "\n",
    "# Random normal distribution (mean=0, std=1)\n",
    "random_normal = torch.randn(3, 3)\n",
    "print(f\"Random normal (3x3):\\n{random_normal}\")\n",
    "\n",
    "# Random uniform distribution [0, 1)\n",
    "random_uniform = torch.rand(2, 4)\n",
    "print(f\"\\nRandom uniform (2x4):\\n{random_uniform}\")\n",
    "\n",
    "# Random integers in a range\n",
    "random_int = torch.randint(0, 10, (3, 3))\n",
    "print(f\"\\nRandom integers 0-9 (3x3):\\n{random_int}\")\n",
    "\n",
    "# Australian-specific example: Random tourist group sizes\n",
    "print(\"\\nüöå Australian Tourism Example - Random Group Sizes:\")\n",
    "# Generate random tourist group sizes for different attractions (5-50 people)\n",
    "group_sizes = torch.randint(5, 51, (len(attractions),))\n",
    "for i, attraction in enumerate(attractions):\n",
    "    print(f\"   {attraction}: {group_sizes[i].item()} visitors\")\n",
    "\n",
    "# TensorFlow comparison\n",
    "print(\"\\nüìä TensorFlow vs PyTorch Random Tensors:\")\n",
    "print(\"   TensorFlow: tf.random.normal([3, 3])\")\n",
    "print(\"   PyTorch:    torch.randn(3, 3)\")\n",
    "print(\"   TensorFlow: tf.random.uniform([2, 4])\")\n",
    "print(\"   PyTorch:    torch.rand(2, 4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tensor Operations\n",
    "\n",
    "PyTorch provides a rich set of operations for tensor manipulation. Let's explore mathematical operations, matrix operations, and more using Australian tourism data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical operations with Australian weather data\n",
    "print(\"üå°Ô∏è Mathematical Operations: Australian Weather Data\\n\")\n",
    "\n",
    "# Australian cities average temperatures (Celsius) for different seasons\n",
    "# Cities: Sydney, Melbourne, Brisbane, Perth, Adelaide\n",
    "summer_temps = torch.tensor([26.5, 25.5, 28.0, 30.5, 28.5], dtype=torch.float32)\n",
    "winter_temps = torch.tensor([17.0, 14.0, 21.0, 18.5, 15.5], dtype=torch.float32)\n",
    "\n",
    "print(f\"Summer temperatures: {summer_temps}\")\n",
    "print(f\"Winter temperatures: {winter_temps}\")\n",
    "\n",
    "# Basic arithmetic operations\n",
    "temp_difference = summer_temps - winter_temps\n",
    "average_temps = (summer_temps + winter_temps) / 2\n",
    "temp_ratio = summer_temps / winter_temps\n",
    "\n",
    "print(f\"\\nTemperature differences: {temp_difference}\")\n",
    "print(f\"Average temperatures: {average_temps}\")\n",
    "print(f\"Summer/Winter ratio: {temp_ratio}\")\n",
    "\n",
    "# Element-wise operations\n",
    "print(\"\\nüî¢ Element-wise Operations:\")\n",
    "squared_temps = torch.pow(summer_temps, 2)\n",
    "sqrt_temps = torch.sqrt(summer_temps)\n",
    "rounded_temps = torch.round(average_temps)\n",
    "\n",
    "print(f\"Squared summer temps: {squared_temps}\")\n",
    "print(f\"Square root of summer temps: {sqrt_temps}\")\n",
    "print(f\"Rounded average temps: {rounded_temps}\")\n",
    "\n",
    "# TensorFlow comparison\n",
    "print(\"\\nüìä TensorFlow vs PyTorch Operations:\")\n",
    "print(\"   TensorFlow: tf.add(a, b) or a + b\")\n",
    "print(\"   PyTorch:    torch.add(a, b) or a + b\")\n",
    "print(\"   TensorFlow: tf.square(a)\")\n",
    "print(\"   PyTorch:    torch.pow(a, 2) or a.pow(2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix operations with Australian tourism data\n",
    "print(\"üèñÔ∏è Matrix Operations: Australian Tourism Analysis\\n\")\n",
    "\n",
    "# Tourist arrivals matrix (millions): rows=years, columns=cities\n",
    "# Years: 2019, 2020, 2021, 2022\n",
    "# Cities: Sydney, Melbourne, Brisbane, Perth\n",
    "tourist_arrivals = torch.tensor([\n",
    "    [4.5, 3.2, 2.8, 1.9],  # 2019\n",
    "    [2.1, 1.5, 1.2, 0.8],  # 2020 (COVID impact)\n",
    "    [1.8, 1.2, 1.0, 0.6],  # 2021\n",
    "    [3.8, 2.7, 2.3, 1.5]   # 2022 (recovery)\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(f\"Tourist arrivals matrix (millions):\\n{tourist_arrivals}\")\n",
    "print(f\"Shape: {tourist_arrivals.shape}\")\n",
    "\n",
    "# Matrix transpose\n",
    "arrivals_transposed = tourist_arrivals.transpose(0, 1)\n",
    "print(f\"\\nTransposed matrix (cities x years):\\n{arrivals_transposed}\")\n",
    "\n",
    "# Matrix-vector operations\n",
    "# Weight vector for different tourism spending per visitor (thousands AUD)\n",
    "spending_per_visitor = torch.tensor([8.5, 7.2, 6.8, 9.1], dtype=torch.float32)\n",
    "print(f\"\\nSpending per visitor (thousands AUD): {spending_per_visitor}\")\n",
    "\n",
    "# Calculate total tourism revenue for each year\n",
    "total_revenue = torch.matmul(tourist_arrivals, spending_per_visitor)\n",
    "print(f\"Total tourism revenue by year (millions AUD): {total_revenue}\")\n",
    "\n",
    "# Element-wise multiplication (Hadamard product)\n",
    "print(\"\\nüí∞ Revenue calculation (arrivals √ó avg spending):\")\n",
    "# Create spending matrix (same shape as arrivals)\n",
    "spending_matrix = spending_per_visitor.unsqueeze(0).repeat(4, 1)\n",
    "revenue_matrix = tourist_arrivals * spending_matrix\n",
    "print(f\"Revenue by city and year:\\n{revenue_matrix}\")\n",
    "\n",
    "# Matrix norms and statistics\n",
    "print(\"\\nüìä Matrix Statistics:\")\n",
    "print(f\"Total arrivals across all years/cities: {tourist_arrivals.sum().item():.1f} million\")\n",
    "print(f\"Average arrivals per city per year: {tourist_arrivals.mean().item():.1f} million\")\n",
    "print(f\"Max arrivals (single city/year): {tourist_arrivals.max().item():.1f} million\")\n",
    "print(f\"Min arrivals (single city/year): {tourist_arrivals.min().item():.1f} million\")\n",
    "\n",
    "# TensorFlow comparison\n",
    "print(\"\\nüìä TensorFlow vs PyTorch Matrix Operations:\")\n",
    "print(\"   TensorFlow: tf.matmul(a, b) or tf.linalg.matmul(a, b)\")\n",
    "print(\"   PyTorch:    torch.matmul(a, b) or torch.mm(a, b)\")\n",
    "print(\"   TensorFlow: tf.transpose(a)\")\n",
    "print(\"   PyTorch:    a.transpose() or a.T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduction operations and aggregations\n",
    "print(\"üìà Reduction Operations: Australian Economic Analysis\\n\")\n",
    "\n",
    "# Australian state GDP data (billions AUD, simplified)\n",
    "states = [\"NSW\", \"VIC\", \"QLD\", \"WA\", \"SA\", \"TAS\"]\n",
    "gdp_sectors = [\"Mining\", \"Manufacturing\", \"Services\", \"Agriculture\"]\n",
    "\n",
    "# GDP by state and sector (billions AUD)\n",
    "gdp_data = torch.tensor([\n",
    "    [45.2, 78.5, 385.2, 12.1],  # NSW\n",
    "    [12.8, 95.2, 325.5, 15.3],  # VIC\n",
    "    [85.6, 45.8, 185.4, 22.7],  # QLD\n",
    "    [165.2, 28.4, 125.8, 18.9], # WA\n",
    "    [8.5, 18.2, 65.4, 8.2],     # SA\n",
    "    [2.1, 4.5, 18.9, 3.8]       # TAS\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(f\"GDP data shape: {gdp_data.shape} (states √ó sectors)\")\n",
    "print(f\"GDP data:\\n{gdp_data}\")\n",
    "\n",
    "# Reduction along different dimensions\n",
    "print(\"\\nüéØ Reduction Operations:\")\n",
    "\n",
    "# Sum along states (dim=0) - total GDP by sector across Australia\n",
    "gdp_by_sector = torch.sum(gdp_data, dim=0)\n",
    "print(f\"\\nTotal GDP by sector (billions AUD):\")\n",
    "for i, sector in enumerate(gdp_sectors):\n",
    "    print(f\"   {sector}: ${gdp_by_sector[i]:.1f}B\")\n",
    "\n",
    "# Sum along sectors (dim=1) - total GDP by state\n",
    "gdp_by_state = torch.sum(gdp_data, dim=1)\n",
    "print(f\"\\nTotal GDP by state (billions AUD):\")\n",
    "for i, state in enumerate(states):\n",
    "    print(f\"   {state}: ${gdp_by_state[i]:.1f}B\")\n",
    "\n",
    "# Other reduction operations\n",
    "print(f\"\\nOther Statistics:\")\n",
    "print(f\"   Total Australian GDP: ${torch.sum(gdp_data).item():.1f}B\")\n",
    "print(f\"   Average sector GDP per state: ${torch.mean(gdp_data).item():.1f}B\")\n",
    "print(f\"   Largest sector in any state: ${torch.max(gdp_data).item():.1f}B\")\n",
    "print(f\"   Smallest sector in any state: ${torch.min(gdp_data).item():.1f}B\")\n",
    "\n",
    "# Find indices of max/min values\n",
    "max_indices = torch.argmax(gdp_data)\n",
    "max_state, max_sector = divmod(max_indices.item(), gdp_data.shape[1])\n",
    "print(f\"   Largest sector: {gdp_sectors[max_sector]} in {states[max_state]}\")\n",
    "\n",
    "# Standard deviation and variance\n",
    "print(f\"\\nüìä Variability:\")\n",
    "print(f\"   GDP standard deviation: ${torch.std(gdp_data).item():.1f}B\")\n",
    "print(f\"   GDP variance: ${torch.var(gdp_data).item():.1f}B¬≤\")\n",
    "\n",
    "# TensorFlow comparison\n",
    "print(\"\\nüìä TensorFlow vs PyTorch Reductions:\")\n",
    "print(\"   TensorFlow: tf.reduce_sum(x, axis=0)\")\n",
    "print(\"   PyTorch:    torch.sum(x, dim=0)\")\n",
    "print(\"   TensorFlow: tf.reduce_mean(x)\")\n",
    "print(\"   PyTorch:    torch.mean(x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tensor Indexing and Slicing\n",
    "\n",
    "Tensor indexing and slicing are essential for data manipulation in deep learning. Let's explore these concepts using Australian text and linguistic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic indexing with Australian text data\n",
    "print(\"üìù Basic Indexing: Australian Text Analysis\\n\")\n",
    "\n",
    "# Create a tensor representing word frequencies in Australian tourism reviews\n",
    "# Words: [\"beautiful\", \"expensive\", \"crowded\", \"peaceful\", \"accessible\"]\n",
    "# Reviews for: Sydney Opera House, Great Barrier Reef, Uluru, Bondi Beach\n",
    "word_frequencies = torch.tensor([\n",
    "    [85, 45, 72, 28, 35],  # Sydney Opera House\n",
    "    [95, 78, 25, 88, 15],  # Great Barrier Reef\n",
    "    [92, 32, 18, 95, 22],  # Uluru\n",
    "    [78, 12, 65, 45, 85]   # Bondi Beach\n",
    "], dtype=torch.float32)\n",
    "\n",
    "attractions = [\"Sydney Opera House\", \"Great Barrier Reef\", \"Uluru\", \"Bondi Beach\"]\n",
    "words = [\"beautiful\", \"expensive\", \"crowded\", \"peaceful\", \"accessible\"]\n",
    "\n",
    "print(f\"Word frequency tensor shape: {word_frequencies.shape}\")\n",
    "print(f\"Tensor:\\n{word_frequencies}\")\n",
    "\n",
    "# Basic indexing\n",
    "print(\"\\nüéØ Basic Indexing Examples:\")\n",
    "\n",
    "# Access single element\n",
    "opera_house_beautiful = word_frequencies[0, 0]\n",
    "print(f\"'Beautiful' mentions for Sydney Opera House: {opera_house_beautiful.item()}\")\n",
    "\n",
    "# Access entire row (all words for one attraction)\n",
    "uluru_words = word_frequencies[2]\n",
    "print(f\"\\nAll word frequencies for Uluru: {uluru_words}\")\n",
    "\n",
    "# Access entire column (one word for all attractions)\n",
    "expensive_mentions = word_frequencies[:, 1]\n",
    "print(f\"'Expensive' mentions across attractions: {expensive_mentions}\")\n",
    "\n",
    "# Negative indexing (last elements)\n",
    "last_attraction = word_frequencies[-1]\n",
    "last_word_all_attractions = word_frequencies[:, -1]\n",
    "print(f\"\\nLast attraction frequencies: {last_attraction}\")\n",
    "print(f\"Last word ('accessible') across all: {last_word_all_attractions}\")\n",
    "\n",
    "# TensorFlow comparison\n",
    "print(\"\\nüìä TensorFlow vs PyTorch Indexing:\")\n",
    "print(\"   TensorFlow: tensor[0, 1] or tf.gather(tensor, [0])\")\n",
    "print(\"   PyTorch:    tensor[0, 1] (same syntax!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced slicing operations\n",
    "print(\"‚úÇÔ∏è Advanced Slicing: Australian Language Analysis\\n\")\n",
    "\n",
    "# Create a more complex tensor: sentence sentiment scores\n",
    "# Dimensions: [languages, sentences, sentiment_aspects]\n",
    "# Languages: English, Vietnamese\n",
    "# Sentences: 6 tourism review sentences per language\n",
    "# Sentiment aspects: [positive, negative, neutral]\n",
    "\n",
    "# Sample sentences:\n",
    "english_sentences = [\n",
    "    \"Sydney Opera House is absolutely stunning!\",\n",
    "    \"The prices in Melbourne are quite high.\",\n",
    "    \"Bondi Beach has perfect weather today.\",\n",
    "    \"The crowds at Uluru were overwhelming.\",\n",
    "    \"Brisbane offers good value for money.\",\n",
    "    \"The Great Barrier Reef is worth the trip.\"\n",
    "]\n",
    "\n",
    "vietnamese_sentences = [\n",
    "    \"Nh√† h√°t Opera Sydney th·∫≠t tuy·ªát v·ªùi!\",\n",
    "    \"Gi√° c·∫£ ·ªü Melbourne kh√° ƒë·∫Øt.\",\n",
    "    \"B√£i bi·ªÉn Bondi c√≥ th·ªùi ti·∫øt ho√†n h·∫£o h√¥m nay.\",\n",
    "    \"ƒê√°m ƒë√¥ng ·ªü Uluru th·∫≠t √°p ƒë·∫£o.\",\n",
    "    \"Brisbane cung c·∫•p gi√° tr·ªã t·ªët cho ti·ªÅn.\",\n",
    "    \"R·∫°n san h√¥ Great Barrier Reef ƒë√°ng ƒë·ªÉ ƒëi.\"\n",
    "]\n",
    "\n",
    "# Sentiment scores (0-1 scale)\n",
    "sentiment_data = torch.tensor([\n",
    "    # English sentences [positive, negative, neutral]\n",
    "    [[0.95, 0.02, 0.03],  # \"absolutely stunning\"\n",
    "     [0.15, 0.65, 0.20],  # \"quite high prices\"\n",
    "     [0.88, 0.05, 0.07],  # \"perfect weather\"\n",
    "     [0.10, 0.78, 0.12],  # \"overwhelming crowds\"\n",
    "     [0.75, 0.15, 0.10],  # \"good value\"\n",
    "     [0.85, 0.08, 0.07]], # \"worth the trip\"\n",
    "    \n",
    "    # Vietnamese sentences [positive, negative, neutral]\n",
    "    [[0.92, 0.03, 0.05],  # \"th·∫≠t tuy·ªát v·ªùi\"\n",
    "     [0.18, 0.62, 0.20],  # \"kh√° ƒë·∫Øt\"\n",
    "     [0.90, 0.04, 0.06],  # \"ho√†n h·∫£o\"\n",
    "     [0.12, 0.75, 0.13],  # \"√°p ƒë·∫£o\"\n",
    "     [0.72, 0.18, 0.10],  # \"gi√° tr·ªã t·ªët\"\n",
    "     [0.83, 0.09, 0.08]]  # \"ƒë√°ng ƒë·ªÉ ƒëi\"\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(f\"Sentiment tensor shape: {sentiment_data.shape} (languages √ó sentences √ó aspects)\")\n",
    "\n",
    "# Advanced slicing examples\n",
    "print(\"\\nüîç Advanced Slicing Examples:\")\n",
    "\n",
    "# Extract only English data\n",
    "english_sentiment = sentiment_data[0]\n",
    "print(f\"English sentiment shape: {english_sentiment.shape}\")\n",
    "\n",
    "# Extract only positive sentiment scores for both languages\n",
    "positive_scores = sentiment_data[:, :, 0]\n",
    "print(f\"\\nPositive sentiment scores:\")\n",
    "print(f\"English: {positive_scores[0]}\")\n",
    "print(f\"Vietnamese: {positive_scores[1]}\")\n",
    "\n",
    "# Extract first 3 sentences for both languages\n",
    "first_three = sentiment_data[:, :3, :]\n",
    "print(f\"\\nFirst 3 sentences shape: {first_three.shape}\")\n",
    "\n",
    "# Skip every other sentence\n",
    "every_other = sentiment_data[:, ::2, :]\n",
    "print(f\"Every other sentence shape: {every_other.shape}\")\n",
    "\n",
    "# Complex slicing: negative sentiment for Vietnamese sentences 2-4\n",
    "vietnamese_negative_subset = sentiment_data[1, 2:5, 1]\n",
    "print(f\"\\nVietnamese negative sentiment (sentences 2-4): {vietnamese_negative_subset}\")\n",
    "\n",
    "print(\"\\nüìä TensorFlow vs PyTorch Slicing:\")\n",
    "print(\"   TensorFlow: tensor[0:3, :, 1] or tf.slice(tensor, [0, 0, 1], [3, -1, 1])\")\n",
    "print(\"   PyTorch:    tensor[0:3, :, 1] (same syntax!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean indexing and advanced selection\n",
    "print(\"üéØ Boolean Indexing: Australian Tourism Filtering\\n\")\n",
    "\n",
    "# Australian tourism data: attractions with various metrics\n",
    "attraction_names = [\"Sydney Opera House\", \"Great Barrier Reef\", \"Uluru\", \"Bondi Beach\", \n",
    "                   \"Melbourne Laneways\", \"Blue Mountains\", \"Gold Coast Theme Parks\", \"Kangaroo Island\"]\n",
    "\n",
    "# Metrics: [visitor_satisfaction, price_rating, accessibility, family_friendly]\n",
    "# Scale: 1-10 for all metrics\n",
    "attraction_metrics = torch.tensor([\n",
    "    [9.5, 6.0, 7.5, 8.0],  # Sydney Opera House\n",
    "    [9.8, 4.0, 5.5, 7.5],  # Great Barrier Reef\n",
    "    [9.2, 5.5, 6.0, 6.5],  # Uluru\n",
    "    [8.8, 9.0, 9.5, 9.2],  # Bondi Beach\n",
    "    [8.5, 8.5, 9.0, 7.0],  # Melbourne Laneways\n",
    "    [8.0, 8.0, 7.0, 8.5],  # Blue Mountains\n",
    "    [7.5, 6.5, 8.0, 9.5],  # Gold Coast Theme Parks\n",
    "    [8.2, 7.0, 6.5, 8.0]   # Kangaroo Island\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(f\"Attraction metrics shape: {attraction_metrics.shape}\")\n",
    "print(\"Metrics: [satisfaction, price_rating, accessibility, family_friendly]\\n\")\n",
    "\n",
    "# Boolean indexing examples\n",
    "print(\"üîç Boolean Indexing Examples:\")\n",
    "\n",
    "# Find highly satisfying attractions (satisfaction > 9.0)\n",
    "high_satisfaction = attraction_metrics[:, 0] > 9.0\n",
    "high_satisfaction_attractions = attraction_metrics[high_satisfaction]\n",
    "print(f\"High satisfaction mask: {high_satisfaction}\")\n",
    "print(f\"High satisfaction attractions count: {high_satisfaction.sum().item()}\")\n",
    "print(\"High satisfaction attractions:\")\n",
    "for i, is_high in enumerate(high_satisfaction):\n",
    "    if is_high:\n",
    "        print(f\"   {attraction_names[i]}: {attraction_metrics[i].tolist()}\")\n",
    "\n",
    "# Find budget-friendly and family-friendly attractions\n",
    "budget_friendly = attraction_metrics[:, 1] >= 7.0  # price_rating >= 7\n",
    "family_friendly = attraction_metrics[:, 3] >= 8.0  # family_friendly >= 8\n",
    "budget_and_family = budget_friendly & family_friendly\n",
    "\n",
    "print(f\"\\nBudget & family-friendly attractions:\")\n",
    "for i, is_suitable in enumerate(budget_and_family):\n",
    "    if is_suitable:\n",
    "        print(f\"   {attraction_names[i]}: {attraction_metrics[i].tolist()}\")\n",
    "\n",
    "# Advanced filtering: find attractions with balanced scores (all metrics > 7.0)\n",
    "balanced_attractions = torch.all(attraction_metrics > 7.0, dim=1)\n",
    "print(f\"\\nBalanced attractions (all metrics > 7.0):\")\n",
    "for i, is_balanced in enumerate(balanced_attractions):\n",
    "    if is_balanced:\n",
    "        print(f\"   {attraction_names[i]}: {attraction_metrics[i].tolist()}\")\n",
    "\n",
    "# Using torch.where for conditional selection\n",
    "print(f\"\\nüí° Using torch.where for conditional operations:\")\n",
    "# Replace low accessibility scores (< 7) with \"Needs Improvement\" (encoded as 0)\n",
    "improved_accessibility = torch.where(attraction_metrics[:, 2] < 7.0, \n",
    "                                   torch.tensor(0.0), \n",
    "                                   attraction_metrics[:, 2])\n",
    "print(f\"Original accessibility: {attraction_metrics[:, 2]}\")\n",
    "print(f\"Improved accessibility: {improved_accessibility}\")\n",
    "\n",
    "print(\"\\nüìä TensorFlow vs PyTorch Boolean Indexing:\")\n",
    "print(\"   TensorFlow: tf.boolean_mask(tensor, condition)\")\n",
    "print(\"   PyTorch:    tensor[condition]\")\n",
    "print(\"   TensorFlow: tf.where(condition, x, y)\")\n",
    "print(\"   PyTorch:    torch.where(condition, x, y)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping and view operations\n",
    "print(\"üîÑ Reshaping and View Operations: Text Processing\\n\")\n",
    "\n",
    "# Create a tensor representing tokenized text\n",
    "# Simulate tokenized Australian tourism reviews\n",
    "print(\"Example: Processing tokenized Australian tourism reviews\")\n",
    "print(\"Original text: 'Sydney Opera House offers stunning harbor views and excellent performances'\")\n",
    "print(\"Vietnamese: 'Nh√† h√°t Opera Sydney cung c·∫•p t·∫ßm nh√¨n c·∫£ng tuy·ªát ƒë·∫πp v√† c√°c bu·ªïi bi·ªÉu di·ªÖn xu·∫•t s·∫Øc'\\n\")\n",
    "\n",
    "# Token IDs for the sentence (simplified vocabulary)\n",
    "original_tokens = torch.tensor([\n",
    "    15, 67, 89, 23, 156, 78, 234, 45, 167, 98, 134, 56\n",
    "], dtype=torch.long)\n",
    "\n",
    "print(f\"Original token sequence: {original_tokens}\")\n",
    "print(f\"Shape: {original_tokens.shape}\")\n",
    "\n",
    "# Reshape into matrix (e.g., for batch processing)\n",
    "print(\"\\nüîÑ Reshaping Examples:\")\n",
    "\n",
    "# Reshape to 3x4 matrix\n",
    "reshaped_3x4 = original_tokens.reshape(3, 4)\n",
    "print(f\"Reshaped to 3x4:\\n{reshaped_3x4}\")\n",
    "\n",
    "# Reshape to 2x6 matrix\n",
    "reshaped_2x6 = original_tokens.reshape(2, 6)\n",
    "print(f\"\\nReshaped to 2x6:\\n{reshaped_2x6}\")\n",
    "\n",
    "# Use -1 for automatic dimension calculation\n",
    "reshaped_auto = original_tokens.reshape(4, -1)\n",
    "print(f\"\\nReshaped to 4x? (auto-calculated):\\n{reshaped_auto}\")\n",
    "print(f\"Auto shape: {reshaped_auto.shape}\")\n",
    "\n",
    "# View vs Reshape\n",
    "print(\"\\nüëÅÔ∏è View vs Reshape:\")\n",
    "viewed_tensor = original_tokens.view(3, 4)\n",
    "print(f\"View (shares memory): {viewed_tensor.shape}\")\n",
    "print(f\"Original data pointer same as view: {original_tokens.data_ptr() == viewed_tensor.data_ptr()}\")\n",
    "\n",
    "# Adding and removing dimensions\n",
    "print(\"\\nüìê Adding/Removing Dimensions:\")\n",
    "\n",
    "# Add dimension (unsqueeze)\n",
    "with_batch_dim = original_tokens.unsqueeze(0)  # Add batch dimension\n",
    "print(f\"With batch dimension: {with_batch_dim.shape}\")\n",
    "\n",
    "with_channel_dim = original_tokens.unsqueeze(1)  # Add channel dimension\n",
    "print(f\"With channel dimension: {with_channel_dim.shape}\")\n",
    "\n",
    "# Remove dimension (squeeze)\n",
    "squeezed = with_batch_dim.squeeze(0)  # Remove batch dimension\n",
    "print(f\"After squeezing batch dim: {squeezed.shape}\")\n",
    "\n",
    "# Flatten tensor\n",
    "flattened = reshaped_3x4.flatten()\n",
    "print(f\"\\nFlattened tensor: {flattened}\")\n",
    "print(f\"Flattened shape: {flattened.shape}\")\n",
    "\n",
    "# Practical NLP example: preparing for embedding layer\n",
    "print(\"\\nüí° Practical NLP Example: Preparing for Embedding Layer\")\n",
    "# Simulate batch of sentences with different lengths (padded)\n",
    "batch_sentences = torch.tensor([\n",
    "    [15, 67, 89, 23, 0, 0],    # Sentence 1 (4 real tokens + 2 padding)\n",
    "    [156, 78, 234, 45, 167, 98], # Sentence 2 (6 real tokens)\n",
    "    [134, 56, 12, 0, 0, 0]      # Sentence 3 (3 real tokens + 3 padding)\n",
    "], dtype=torch.long)\n",
    "\n",
    "print(f\"Batch of sentences: {batch_sentences.shape} (batch_size √ó seq_length)\")\n",
    "print(f\"Batch:\\n{batch_sentences}\")\n",
    "\n",
    "# Flatten for processing\n",
    "flattened_batch = batch_sentences.flatten()\n",
    "print(f\"\\nFlattened for lookup: {flattened_batch.shape}\")\n",
    "print(f\"Flattened: {flattened_batch}\")\n",
    "\n",
    "# TensorFlow comparison\n",
    "print(\"\\nüìä TensorFlow vs PyTorch Reshaping:\")\n",
    "print(\"   TensorFlow: tf.reshape(x, [3, 4])\")\n",
    "print(\"   PyTorch:    x.reshape(3, 4) or x.view(3, 4)\")\n",
    "print(\"   TensorFlow: tf.expand_dims(x, axis=0)\")\n",
    "print(\"   PyTorch:    x.unsqueeze(0)\")\n",
    "print(\"   TensorFlow: tf.squeeze(x, axis=0)\")\n",
    "print(\"   PyTorch:    x.squeeze(0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bridge with NumPy\n",
    "\n",
    "One of PyTorch's strengths is its seamless integration with NumPy. Let's explore converting between PyTorch tensors and NumPy arrays using multilingual Australian text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting between PyTorch tensors and NumPy arrays\n",
    "print(\"üîÑ PyTorch ‚Üî NumPy Conversion: Multilingual Text Analysis\\n\")\n",
    "\n",
    "# Start with NumPy array - character frequencies in Australian text\n",
    "print(\"üìù Character Frequency Analysis: English vs Vietnamese\")\n",
    "print(\"English: 'Sydney beaches are amazing for surfing and swimming'\")\n",
    "print(\"Vietnamese: 'B√£i bi·ªÉn Sydney th·∫≠t tuy·ªát v·ªùi cho l∆∞·ªõt s√≥ng v√† b∆°i l·ªôi'\\n\")\n",
    "\n",
    "# Character frequency data (simplified)\n",
    "# Characters: ['a', 'e', 'i', 'o', 'u', 'n', 's', 't']\n",
    "english_char_freq = np.array([6, 7, 4, 2, 2, 8, 5, 3], dtype=np.float32)\n",
    "vietnamese_char_freq = np.array([4, 5, 3, 4, 1, 6, 4, 7], dtype=np.float32)\n",
    "\n",
    "print(f\"English char frequencies (NumPy): {english_char_freq}\")\n",
    "print(f\"Vietnamese char frequencies (NumPy): {vietnamese_char_freq}\")\n",
    "print(f\"NumPy array type: {type(english_char_freq)}\")\n",
    "print(f\"NumPy dtype: {english_char_freq.dtype}\")\n",
    "\n",
    "# Convert NumPy to PyTorch\n",
    "print(\"\\nüîÑ NumPy ‚Üí PyTorch Conversion:\")\n",
    "english_tensor = torch.from_numpy(english_char_freq)\n",
    "vietnamese_tensor = torch.from_numpy(vietnamese_char_freq)\n",
    "\n",
    "print(f\"English tensor: {english_tensor}\")\n",
    "print(f\"Vietnamese tensor: {vietnamese_tensor}\")\n",
    "print(f\"Tensor type: {type(english_tensor)}\")\n",
    "print(f\"Tensor dtype: {english_tensor.dtype}\")\n",
    "\n",
    "# Check memory sharing\n",
    "print(f\"\\nüß† Memory Sharing Check:\")\n",
    "print(f\"Shares memory: {english_tensor.data_ptr() == english_char_freq.__array_interface__['data'][0]}\")\n",
    "print(\"Note: from_numpy() creates a tensor that shares memory with the NumPy array\")\n",
    "\n",
    "# Demonstrate shared memory\n",
    "original_value = english_char_freq[0]\n",
    "print(f\"\\nBefore modification - NumPy[0]: {english_char_freq[0]}, Tensor[0]: {english_tensor[0]}\")\n",
    "english_char_freq[0] = 999  # Modify NumPy array\n",
    "print(f\"After modifying NumPy - NumPy[0]: {english_char_freq[0]}, Tensor[0]: {english_tensor[0]}\")\n",
    "english_char_freq[0] = original_value  # Restore original value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PyTorch to NumPy\n",
    "print(\"üîÑ PyTorch ‚Üí NumPy Conversion: Tourism Data Analysis\\n\")\n",
    "\n",
    "# Create PyTorch tensor with Australian tourism spending data\n",
    "# Categories: Accommodation, Food, Transport, Activities, Shopping\n",
    "spending_categories = [\"Accommodation\", \"Food\", \"Transport\", \"Activities\", \"Shopping\"]\n",
    "daily_spending_aud = torch.tensor([180.50, 85.25, 45.75, 120.00, 95.30], dtype=torch.float32)\n",
    "\n",
    "print(f\"Daily spending (PyTorch): {daily_spending_aud}\")\n",
    "print(f\"Tensor type: {type(daily_spending_aud)}\")\n",
    "\n",
    "# Convert to NumPy using .numpy()\n",
    "spending_numpy = daily_spending_aud.numpy()\n",
    "print(f\"\\nDaily spending (NumPy): {spending_numpy}\")\n",
    "print(f\"NumPy type: {type(spending_numpy)}\")\n",
    "\n",
    "# Alternative conversion using .detach().numpy() (important for tensors with gradients)\n",
    "spending_detached = daily_spending_aud.detach().numpy()\n",
    "print(f\"Detached NumPy: {spending_detached}\")\n",
    "\n",
    "# Create a detailed analysis using NumPy\n",
    "print(\"\\nüìä Analysis using NumPy operations:\")\n",
    "total_daily = np.sum(spending_numpy)\n",
    "average_category = np.mean(spending_numpy)\n",
    "max_category_idx = np.argmax(spending_numpy)\n",
    "min_category_idx = np.argmin(spending_numpy)\n",
    "\n",
    "print(f\"Total daily spending: ${total_daily:.2f} AUD\")\n",
    "print(f\"Average per category: ${average_category:.2f} AUD\")\n",
    "print(f\"Highest spending: {spending_categories[max_category_idx]} (${spending_numpy[max_category_idx]:.2f})\")\n",
    "print(f\"Lowest spending: {spending_categories[min_category_idx]} (${spending_numpy[min_category_idx]:.2f})\")\n",
    "\n",
    "# Convert back to PyTorch for further processing\n",
    "processed_tensor = torch.from_numpy(spending_numpy * 1.1)  # 10% increase\n",
    "print(f\"\\nAfter 10% increase (back to PyTorch): {processed_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with different devices and NumPy\n",
    "print(\"üîß Device Considerations: CPU vs GPU Tensors\\n\")\n",
    "\n",
    "# Create tensor on CPU\n",
    "cpu_tensor = torch.tensor([1.0, 2.0, 3.0, 4.0], device='cpu')\n",
    "print(f\"CPU tensor: {cpu_tensor}\")\n",
    "print(f\"Device: {cpu_tensor.device}\")\n",
    "\n",
    "# Convert CPU tensor to NumPy (works directly)\n",
    "cpu_numpy = cpu_tensor.numpy()\n",
    "print(f\"CPU ‚Üí NumPy: {cpu_numpy}\")\n",
    "\n",
    "# Device handling demonstration\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\nüöÄ CUDA available - GPU tensor demonstration:\")\n",
    "    gpu_tensor = cpu_tensor.to('cuda')\n",
    "    print(f\"GPU tensor: {gpu_tensor}\")\n",
    "    print(f\"Device: {gpu_tensor.device}\")\n",
    "    \n",
    "    # Must move to CPU before converting to NumPy\n",
    "    gpu_to_numpy = gpu_tensor.cpu().numpy()\n",
    "    print(f\"GPU ‚Üí CPU ‚Üí NumPy: {gpu_to_numpy}\")\n",
    "    \n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    print(\"\\nüçé MPS available - Apple Silicon demonstration:\")\n",
    "    mps_tensor = cpu_tensor.to('mps')\n",
    "    print(f\"MPS tensor: {mps_tensor}\")\n",
    "    print(f\"Device: {mps_tensor.device}\")\n",
    "    \n",
    "    # Must move to CPU before converting to NumPy\n",
    "    mps_to_numpy = mps_tensor.cpu().numpy()\n",
    "    print(f\"MPS ‚Üí CPU ‚Üí NumPy: {mps_to_numpy}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nüíª No GPU/MPS acceleration available - using CPU only\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Important Notes:\")\n",
    "print(\"- NumPy arrays are always on CPU\")\n",
    "print(\"- GPU/MPS tensors must be moved to CPU before .numpy()\")\n",
    "print(\"- from_numpy() always creates CPU tensors\")\n",
    "print(\"- Use .to(device) to move tensors between devices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical example: Multilingual text processing pipeline\n",
    "print(\"üåè Practical Example: Multilingual Australian Tourism Processing\\n\")\n",
    "\n",
    "# Simulate processing pipeline for English-Vietnamese tourism reviews\n",
    "print(\"Processing tourism reviews for sentiment analysis...\")\n",
    "print(\"English: 'The Sydney harbour cruise was absolutely magnificent!'\")\n",
    "print(\"Vietnamese: 'Chuy·∫øn du thuy·ªÅn c·∫£ng Sydney th·∫≠t tuy·ªát v·ªùi!'\\n\")\n",
    "\n",
    "# Step 1: Start with NumPy arrays (common in data preprocessing)\n",
    "# Simulated word embeddings (300-dimensional)\n",
    "np.random.seed(16)  # For reproducible results\n",
    "english_embeddings = np.random.randn(8, 300).astype(np.float32)  # 8 words\n",
    "vietnamese_embeddings = np.random.randn(7, 300).astype(np.float32)  # 7 words\n",
    "\n",
    "print(f\"English embeddings shape (NumPy): {english_embeddings.shape}\")\n",
    "print(f\"Vietnamese embeddings shape (NumPy): {vietnamese_embeddings.shape}\")\n",
    "\n",
    "# Step 2: Convert to PyTorch for deep learning processing\n",
    "english_tensor = torch.from_numpy(english_embeddings)\n",
    "vietnamese_tensor = torch.from_numpy(vietnamese_embeddings)\n",
    "\n",
    "print(f\"\\nConverted to PyTorch tensors:\")\n",
    "print(f\"English tensor shape: {english_tensor.shape}\")\n",
    "print(f\"Vietnamese tensor shape: {vietnamese_tensor.shape}\")\n",
    "\n",
    "# Step 3: Move to appropriate device for processing\n",
    "device, _ = detect_device()\n",
    "english_tensor = english_tensor.to(device)\n",
    "vietnamese_tensor = vietnamese_tensor.to(device)\n",
    "\n",
    "print(f\"\\nMoved to device: {device}\")\n",
    "\n",
    "# Step 4: Perform PyTorch operations (simulated sentiment analysis)\n",
    "# Calculate average embeddings (sentence representations)\n",
    "english_sentence_repr = torch.mean(english_tensor, dim=0)\n",
    "vietnamese_sentence_repr = torch.mean(vietnamese_tensor, dim=0)\n",
    "\n",
    "# Calculate similarity (dot product)\n",
    "similarity = torch.dot(english_sentence_repr, vietnamese_sentence_repr)\n",
    "print(f\"\\nCross-lingual similarity score: {similarity.item():.4f}\")\n",
    "\n",
    "# Step 5: Convert back to NumPy for visualization/further analysis\n",
    "english_final = english_sentence_repr.cpu().numpy()\n",
    "vietnamese_final = vietnamese_sentence_repr.cpu().numpy()\n",
    "\n",
    "print(f\"\\nFinal sentence representations (NumPy):\")\n",
    "print(f\"English sentence vector: {english_final[:5]}... (showing first 5 dims)\")\n",
    "print(f\"Vietnamese sentence vector: {vietnamese_final[:5]}... (showing first 5 dims)\")\n",
    "\n",
    "# Step 6: Use NumPy for analysis and visualization\n",
    "cosine_similarity = np.dot(english_final, vietnamese_final) / (\n",
    "    np.linalg.norm(english_final) * np.linalg.norm(vietnamese_final)\n",
    ")\n",
    "print(f\"\\nCosine similarity (NumPy calculation): {cosine_similarity:.4f}\")\n",
    "print(f\"Interpretation: {'Very similar' if cosine_similarity > 0.8 else 'Moderately similar' if cosine_similarity > 0.5 else 'Different'} semantic content\")\n",
    "\n",
    "# TensorFlow comparison\n",
    "print(\"\\nüìä TensorFlow vs PyTorch NumPy Integration:\")\n",
    "print(\"   TensorFlow: tf.convert_to_tensor(numpy_array)\")\n",
    "print(\"   PyTorch:    torch.from_numpy(numpy_array)\")\n",
    "print(\"   TensorFlow: tensor.numpy()\")\n",
    "print(\"   PyTorch:    tensor.numpy() or tensor.detach().numpy()\")\n",
    "print(\"\\nüí° PyTorch Advantage: Seamless memory sharing with NumPy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. NLP-Focused Tensor Applications\n",
    "\n",
    "Let's apply our tensor knowledge to common NLP tasks, preparing for neural network implementation and Hugging Face integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text tokenization and vocabulary mapping\n",
    "print(\"üìù Text Tokenization with Tensors: Australian Tourism Reviews\\n\")\n",
    "\n",
    "# Sample Australian tourism reviews (English and Vietnamese)\n",
    "reviews = {\n",
    "    'english': [\n",
    "        \"Sydney Opera House is stunning and iconic\",\n",
    "        \"Bondi Beach has perfect waves for surfing\",\n",
    "        \"Melbourne coffee culture is world famous\",\n",
    "        \"Great Barrier Reef offers amazing snorkeling\",\n",
    "        \"Uluru sunset views are absolutely breathtaking\"\n",
    "    ],\n",
    "    'vietnamese': [\n",
    "        \"Nh√† h√°t Opera Sydney tuy·ªát ƒë·∫πp v√† mang t√≠nh bi·ªÉu t∆∞·ª£ng\",\n",
    "        \"B√£i bi·ªÉn Bondi c√≥ s√≥ng ho√†n h·∫£o ƒë·ªÉ l∆∞·ªõt s√≥ng\",\n",
    "        \"VƒÉn h√≥a c√† ph√™ Melbourne n·ªïi ti·∫øng th·∫ø gi·ªõi\",\n",
    "        \"R·∫°n san h√¥ Great Barrier Reef cung c·∫•p l·∫∑n tuy·ªát v·ªùi\",\n",
    "        \"C·∫£nh ho√†ng h√¥n Uluru th·∫≠t ngo·∫°n m·ª•c\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Simple tokenization (split by spaces)\n",
    "def simple_tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Build vocabulary from all reviews\n",
    "all_tokens = []\n",
    "for lang_reviews in reviews.values():\n",
    "    for review in lang_reviews:\n",
    "        all_tokens.extend(simple_tokenize(review))\n",
    "\n",
    "vocab = sorted(set(all_tokens))\n",
    "vocab_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_vocab = {idx: word for word, idx in vocab_to_idx.items()}\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Sample vocabulary: {vocab[:10]}\")\n",
    "print(f\"Sample Vietnamese words: {[w for w in vocab if any(c in '√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠ƒë√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµ' for c in w)][:5]}\")\n",
    "\n",
    "# Convert text to tensor of token indices\n",
    "def text_to_tensor(text, vocab_to_idx, max_length=None):\n",
    "    tokens = simple_tokenize(text)\n",
    "    indices = [vocab_to_idx.get(token, 0) for token in tokens]  # 0 for unknown\n",
    "    \n",
    "    if max_length:\n",
    "        if len(indices) < max_length:\n",
    "            indices.extend([0] * (max_length - len(indices)))  # Pad with 0\n",
    "        else:\n",
    "            indices = indices[:max_length]  # Truncate\n",
    "    \n",
    "    return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "# Convert first review to tensor\n",
    "first_english = reviews['english'][0]\n",
    "first_vietnamese = reviews['vietnamese'][0]\n",
    "\n",
    "english_tensor = text_to_tensor(first_english, vocab_to_idx, max_length=10)\n",
    "vietnamese_tensor = text_to_tensor(first_vietnamese, vocab_to_idx, max_length=10)\n",
    "\n",
    "print(f\"\\nFirst English review: '{first_english}'\")\n",
    "print(f\"Tokenized tensor: {english_tensor}\")\n",
    "print(f\"First Vietnamese review: '{first_vietnamese}'\")\n",
    "print(f\"Tokenized tensor: {vietnamese_tensor}\")\n",
    "\n",
    "# Reconstruct text from tensor\n",
    "def tensor_to_text(tensor, idx_to_vocab):\n",
    "    tokens = [idx_to_vocab.get(idx.item(), '<UNK>') for idx in tensor if idx.item() != 0]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "reconstructed_english = tensor_to_text(english_tensor, idx_to_vocab)\n",
    "print(f\"\\nReconstructed English: '{reconstructed_english}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing and padding for NLP\n",
    "print(\"üì¶ Batch Processing: Preparing for Neural Networks\\n\")\n",
    "\n",
    "# Convert all reviews to tensors\n",
    "all_reviews_text = reviews['english'] + reviews['vietnamese']\n",
    "all_labels = [0] * len(reviews['english']) + [1] * len(reviews['vietnamese'])  # 0=English, 1=Vietnamese\n",
    "\n",
    "print(f\"Total reviews: {len(all_reviews_text)}\")\n",
    "print(f\"Labels (0=English, 1=Vietnamese): {all_labels}\")\n",
    "\n",
    "# Find maximum sequence length\n",
    "max_seq_length = max(len(simple_tokenize(review)) for review in all_reviews_text)\n",
    "print(f\"Maximum sequence length: {max_seq_length}\")\n",
    "\n",
    "# Convert all to padded tensors\n",
    "review_tensors = []\n",
    "for review in all_reviews_text:\n",
    "    tensor = text_to_tensor(review, vocab_to_idx, max_length=max_seq_length)\n",
    "    review_tensors.append(tensor)\n",
    "\n",
    "# Stack into batch tensor\n",
    "batch_reviews = torch.stack(review_tensors)\n",
    "batch_labels = torch.tensor(all_labels, dtype=torch.long)\n",
    "\n",
    "print(f\"\\nBatch tensor shape: {batch_reviews.shape} (batch_size √ó seq_length)\")\n",
    "print(f\"Labels shape: {batch_labels.shape}\")\n",
    "print(f\"Batch tensor:\\n{batch_reviews}\")\n",
    "print(f\"Labels: {batch_labels}\")\n",
    "\n",
    "# Create attention masks (for transformer models)\n",
    "attention_masks = (batch_reviews != 0).float()  # 1 for real tokens, 0 for padding\n",
    "print(f\"\\nAttention masks shape: {attention_masks.shape}\")\n",
    "print(f\"Attention masks:\\n{attention_masks}\")\n",
    "\n",
    "# Simulate embedding lookup (preparing for neural networks)\n",
    "print(f\"\\nüîç Simulating Embedding Lookup:\")\n",
    "embedding_dim = 50\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Random embedding matrix (in real use, this would be learned)\n",
    "torch.manual_seed(16)  # For reproducible results\n",
    "embedding_matrix = torch.randn(vocab_size, embedding_dim)\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape} (vocab_size √ó embedding_dim)\")\n",
    "\n",
    "# Lookup embeddings for first review\n",
    "first_review_indices = batch_reviews[0]\n",
    "first_review_embeddings = embedding_matrix[first_review_indices]\n",
    "print(f\"\\nFirst review embeddings shape: {first_review_embeddings.shape} (seq_length √ó embedding_dim)\")\n",
    "print(f\"First few embedding values: {first_review_embeddings[0, :5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing for Hugging Face integration\n",
    "print(\"ü§ó Preparing for Hugging Face Integration\\n\")\n",
    "\n",
    "# Simulate data format expected by Hugging Face transformers\n",
    "print(\"Creating data structures compatible with Hugging Face tokenizers...\")\n",
    "\n",
    "# Prepare data in the format expected by transformers\n",
    "def prepare_for_transformers(texts, labels, max_length=64):\n",
    "    \"\"\"\n",
    "    Prepare text data in format compatible with Hugging Face transformers.\n",
    "    In practice, you'd use a proper tokenizer like AutoTokenizer.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in texts:\n",
    "        # Simulate tokenizer output (normally done by transformers tokenizer)\n",
    "        tokens = simple_tokenize(text)\n",
    "        \n",
    "        # Convert to indices (add special tokens)\n",
    "        token_ids = [1] + [vocab_to_idx.get(token, 0) for token in tokens] + [2]  # [CLS] + tokens + [SEP]\n",
    "        \n",
    "        # Pad or truncate\n",
    "        if len(token_ids) < max_length:\n",
    "            attention_mask = [1] * len(token_ids) + [0] * (max_length - len(token_ids))\n",
    "            token_ids.extend([0] * (max_length - len(token_ids)))\n",
    "        else:\n",
    "            token_ids = token_ids[:max_length]\n",
    "            attention_mask = [1] * max_length\n",
    "        \n",
    "        input_ids.append(token_ids)\n",
    "        attention_masks.append(attention_mask)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "        'attention_mask': torch.tensor(attention_masks, dtype=torch.long),\n",
    "        'labels': torch.tensor(labels, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "# Prepare sample data\n",
    "sample_texts = [\n",
    "    \"Sydney Opera House is magnificent\",\n",
    "    \"Nh√† h√°t Opera Sydney tuy·ªát v·ªùi\"\n",
    "]\n",
    "sample_labels = [0, 1]  # English, Vietnamese\n",
    "\n",
    "transformer_data = prepare_for_transformers(sample_texts, sample_labels)\n",
    "\n",
    "print(f\"Transformer-ready data structure:\")\n",
    "for key, value in transformer_data.items():\n",
    "    print(f\"  {key}: {value.shape}\")\n",
    "    print(f\"    {value}\")\n",
    "\n",
    "# Show what this would look like with real Hugging Face usage\n",
    "print(f\"\\nüí° Real Hugging Face Usage:\")\n",
    "print(f\"\"\"# With actual Hugging Face tokenizer:\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "encoded = tokenizer(\n",
    "    sample_texts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=64,\n",
    "    return_tensors='pt'  # Return PyTorch tensors!\n",
    ")\n",
    "# Returns: input_ids, attention_mask, token_type_ids (all as tensors)\"\"\")\n",
    "\n",
    "print(f\"\\nüéØ Key Tensor Concepts for NLP:\")\n",
    "print(f\"  ‚Ä¢ input_ids: Token indices for transformer input\")\n",
    "print(f\"  ‚Ä¢ attention_mask: Padding mask (1=real token, 0=padding)\")\n",
    "print(f\"  ‚Ä¢ labels: Target classes or values for training\")\n",
    "print(f\"  ‚Ä¢ embeddings: Dense vector representations of tokens\")\n",
    "print(f\"  ‚Ä¢ batch_size: Number of examples processed together\")\n",
    "print(f\"  ‚Ä¢ sequence_length: Maximum number of tokens per example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "Congratulations! You've mastered the fundamentals of PyTorch tensors. Let's summarize key concepts and prepare for the next steps in your PyTorch journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key tensor operations\n",
    "print(\"üéì PyTorch Tensor Mastery Summary\\n\")\n",
    "\n",
    "print(\"‚úÖ What You've Learned:\")\n",
    "print(\"  1. üîß Environment Setup: Cross-platform PyTorch installation\")\n",
    "print(\"  2. üì¶ Tensor Creation: From lists, NumPy, and special functions\")\n",
    "print(\"  3. ‚ûï Tensor Operations: Math, matrix ops, and reductions\")\n",
    "print(\"  4. üéØ Indexing & Slicing: Data selection and manipulation\")\n",
    "print(\"  5. üîÑ NumPy Bridge: Seamless interoperability\")\n",
    "print(\"  6. üìù NLP Applications: Text processing and tokenization\")\n",
    "\n",
    "print(\"\\nüìä Key Differences: TensorFlow ‚Üí PyTorch\")\n",
    "comparison_table = [\n",
    "    [\"Execution\", \"Static/Eager\", \"Always Eager\"],\n",
    "    [\"Tensor Creation\", \"tf.constant()\", \"torch.tensor()\"],\n",
    "    [\"Device Management\", \"Automatic\", \"Explicit .to(device)\"],\n",
    "    [\"Reshaping\", \"tf.reshape()\", \"tensor.view() or .reshape()\"],\n",
    "    [\"Random Tensors\", \"tf.random.normal()\", \"torch.randn()\"],\n",
    "    [\"Matrix Multiply\", \"tf.matmul()\", \"torch.matmul()\"],\n",
    "    [\"NumPy Conversion\", \"tf.convert_to_tensor()\", \"torch.from_numpy()\"]\n",
    "]\n",
    "\n",
    "print(f\"{'Operation':<20} {'TensorFlow':<25} {'PyTorch':<25}\")\n",
    "print(\"-\" * 70)\n",
    "for row in comparison_table:\n",
    "    print(f\"{row[0]:<20} {row[1]:<25} {row[2]:<25}\")\n",
    "\n",
    "print(\"\\nüåü PyTorch Advantages for NLP:\")\n",
    "print(\"  ‚Ä¢ Dynamic graphs: Perfect for variable-length sequences\")\n",
    "print(\"  ‚Ä¢ Pythonic syntax: Easy debugging and experimentation\")\n",
    "print(\"  ‚Ä¢ Hugging Face ecosystem: State-of-the-art NLP models\")\n",
    "print(\"  ‚Ä¢ Research-friendly: Most academic papers use PyTorch\")\n",
    "print(\"  ‚Ä¢ Memory sharing with NumPy: Efficient data processing\")\n",
    "\n",
    "print(\"\\nüéØ Next Steps in Your PyTorch Journey:\")\n",
    "print(\"  1. üß† Neural Networks: Learn nn.Module and autograd\")\n",
    "print(\"  2. üìö Data Loading: Master DataLoader and Dataset\")\n",
    "print(\"  3. üèãÔ∏è Training Loops: Implement optimization and backprop\")\n",
    "print(\"  4. ü§ó Hugging Face: Use pre-trained transformers\")\n",
    "print(\"  5. üöÄ Advanced Topics: Custom layers, mixed precision\")\n",
    "\n",
    "print(\"\\nüìù Australian Context Examples You've Mastered:\")\n",
    "australian_examples = [\n",
    "    \"Population data for major Australian cities\",\n",
    "    \"Tourism ratings for iconic attractions\",\n",
    "    \"Weather analysis across different states\",\n",
    "    \"Economic data processing (GDP by sector)\",\n",
    "    \"Multilingual text processing (English-Vietnamese)\",\n",
    "    \"Character frequency analysis for language detection\",\n",
    "    \"Tourism spending categorization and analysis\"\n",
    "]\n",
    "\n",
    "for i, example in enumerate(australian_examples, 1):\n",
    "    print(f\"  {i}. {example}\")\n",
    "\n",
    "print(\"\\nüèÜ You're now ready to build neural networks with PyTorch!\")\n",
    "print(\"Next recommended notebook: Neural Network fundamentals with nn.Module\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final practical exercise: Create your own tensor operations\n",
    "print(\"üõ†Ô∏è Final Exercise: Australian Wine Rating Analysis\\n\")\n",
    "\n",
    "# Create a practical tensor exercise for students\n",
    "print(\"üç∑ Exercise: Analyze Australian wine ratings across regions\")\n",
    "print(\"Your task: Use tensor operations to find insights from wine data\\n\")\n",
    "\n",
    "# Wine regions and their ratings (out of 100)\n",
    "regions = [\"Barossa Valley\", \"Hunter Valley\", \"Margaret River\", \"Yarra Valley\", \"Clare Valley\"]\n",
    "wine_types = [\"Shiraz\", \"Chardonnay\", \"Cabernet Sauvignon\", \"Pinot Noir\"]\n",
    "\n",
    "# Wine ratings matrix: regions √ó wine_types\n",
    "wine_ratings = torch.tensor([\n",
    "    [95, 88, 92, 85],  # Barossa Valley\n",
    "    [89, 94, 87, 91],  # Hunter Valley\n",
    "    [91, 90, 96, 88],  # Margaret River\n",
    "    [87, 92, 89, 94],  # Yarra Valley\n",
    "    [93, 86, 90, 87]   # Clare Valley\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(f\"Wine ratings tensor: {wine_ratings.shape} (regions √ó wine_types)\")\n",
    "print(f\"Regions: {regions}\")\n",
    "print(f\"Wine types: {wine_types}\")\n",
    "print(f\"Ratings:\\n{wine_ratings}\")\n",
    "\n",
    "# Exercise solutions\n",
    "print(\"\\nüìä Analysis Results:\")\n",
    "\n",
    "# 1. Best wine type overall\n",
    "avg_by_wine = torch.mean(wine_ratings, dim=0)\n",
    "best_wine_idx = torch.argmax(avg_by_wine)\n",
    "print(f\"1. Best wine type overall: {wine_types[best_wine_idx]} (avg: {avg_by_wine[best_wine_idx]:.1f})\")\n",
    "\n",
    "# 2. Best region overall\n",
    "avg_by_region = torch.mean(wine_ratings, dim=1)\n",
    "best_region_idx = torch.argmax(avg_by_region)\n",
    "print(f\"2. Best region overall: {regions[best_region_idx]} (avg: {avg_by_region[best_region_idx]:.1f})\")\n",
    "\n",
    "# 3. Highest single rating\n",
    "max_rating = torch.max(wine_ratings)\n",
    "max_indices = torch.where(wine_ratings == max_rating)\n",
    "max_region = regions[max_indices[0][0]]\n",
    "max_wine = wine_types[max_indices[1][0]]\n",
    "print(f\"3. Highest rating: {max_rating.item()} ({max_wine} from {max_region})\")\n",
    "\n",
    "# 4. Most consistent region (lowest std deviation)\n",
    "region_stds = torch.std(wine_ratings, dim=1)\n",
    "most_consistent_idx = torch.argmin(region_stds)\n",
    "print(f\"4. Most consistent region: {regions[most_consistent_idx]} (std: {region_stds[most_consistent_idx]:.2f})\")\n",
    "\n",
    "# 5. Regions with all wines above 90\n",
    "excellent_regions = torch.all(wine_ratings >= 90, dim=1)\n",
    "print(f\"5. Regions with all wines ‚â•90:\")\n",
    "for i, is_excellent in enumerate(excellent_regions):\n",
    "    if is_excellent:\n",
    "        print(f\"   ‚Ä¢ {regions[i]}\")\n",
    "\n",
    "print(\"\\nüéâ Congratulations! You've completed your PyTorch tensor journey!\")\n",
    "print(\"You now have the foundation to build neural networks and work with Hugging Face transformers.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
