#!/usr/bin/env python3
"""
Script to create a comprehensive PyTorch Captum notebook with Australian context
"""

import json

# Complete notebook structure
notebook = {
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PyTorch Model Understanding with Captum: Australian Tourism Image Analysis\n",
                "\n",
                "This notebook demonstrates **Captum**, PyTorch's open-source library for model interpretability, using Australian tourism imagery and multilingual examples. Learn how to understand and explain your PyTorch models' behavior through various attribution techniques.\n",
                "\n",
                "## Learning Objectives\n",
                "- Understand core Captum concepts: Feature, Layer, and Neuron Attribution\n",
                "- Implement **Integrated Gradients** for identifying important input features\n",
                "- Use **Occlusion** analysis for perturbation-based explanations\n",
                "- Apply **Grad-CAM** for layer-level interpretability\n",
                "- Create interactive visualizations with **Captum Insights**\n",
                "- Analyze Australian tourism images and multilingual content\n",
                "\n",
                "## Australian Context Examples\n",
                "We'll analyze images and content related to:\n",
                "- üèõÔ∏è Sydney Opera House and Harbour Bridge\n",
                "- üèñÔ∏è Gold Coast beaches and tourism\n",
                "- üê® Australian wildlife (cats, native animals)\n",
                "- üó£Ô∏è English-Vietnamese tourism descriptions\n",
                "\n",
                "**Captum Documentation**: https://captum.ai\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Setup and Runtime Detection\n",
                "\n",
                "Following PyTorch best practices for cross-platform compatibility:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Environment Detection and Setup\n",
                "import sys\n",
                "import subprocess\n",
                "import os\n",
                "import time\n",
                "\n",
                "# Detect the runtime environment\n",
                "IS_COLAB = \"google.colab\" in sys.modules\n",
                "IS_KAGGLE = \"kaggle_secrets\" in sys.modules or \"kaggle\" in os.environ.get('KAGGLE_URL_BASE', '')\n",
                "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
                "\n",
                "print(f\"üåê Environment detected:\")\n",
                "print(f\"  - Local: {IS_LOCAL}\")\n",
                "print(f\"  - Google Colab: {IS_COLAB}\")\n",
                "print(f\"  - Kaggle: {IS_KAGGLE}\")\n",
                "\n",
                "# Platform-specific system setup\n",
                "if IS_COLAB:\n",
                "    print(\"\\nüîß Setting up Google Colab environment...\")\n",
                "    # Colab usually has PyTorch pre-installed\n",
                "elif IS_KAGGLE:\n",
                "    print(\"\\nüîß Setting up Kaggle environment...\")\n",
                "    # Kaggle usually has most packages pre-installed\n",
                "else:\n",
                "    print(\"\\nüîß Setting up local environment...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages based on platform\n",
                "required_packages = [\n",
                "    \"torch\",\n",
                "    \"torchvision\", \n",
                "    \"captum\",\n",
                "    \"matplotlib\",\n",
                "    \"seaborn\",\n",
                "    \"numpy\",\n",
                "    \"pandas\",\n",
                "    \"tensorboard\",\n",
                "    \"tqdm\"\n",
                "]\n",
                "\n",
                "print(\"üì¶ Installing required packages...\")\n",
                "for package in required_packages:\n",
                "    if IS_COLAB or IS_KAGGLE:\n",
                "        # Use IPython magic commands for notebook environments\n",
                "        try:\n",
                "            exec(f\"!pip install -q {package}\")\n",
                "            print(f\"‚úÖ {package}\")\n",
                "        except:\n",
                "            print(f\"‚ö†Ô∏è {package} (may already be installed)\")\n",
                "    else:\n",
                "        try:\n",
                "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
                "                          capture_output=True, check=True)\n",
                "            print(f\"‚úÖ {package}\")\n",
                "        except subprocess.CalledProcessError:\n",
                "            print(f\"‚ö†Ô∏è {package} (may already be installed)\")\n",
                "\n",
                "print(\"\\nüéâ Package installation completed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify PyTorch and Captum installation\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import DataLoader\n",
                "from torch.utils.tensorboard import SummaryWriter\n",
                "\n",
                "import torchvision\n",
                "import torchvision.transforms as transforms\n",
                "import torchvision.models as models\n",
                "\n",
                "# Captum imports\n",
                "import captum\n",
                "from captum.attr import (\n",
                "    IntegratedGradients,\n",
                "    Occlusion,\n",
                "    LayerGradCam,\n",
                "    LayerAttribution\n",
                ")\n",
                "from captum.attr import visualization as viz\n",
                "\n",
                "# Additional libraries\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.image as mpimg\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "from datetime import datetime\n",
                "import tempfile\n",
                "import json\n",
                "from PIL import Image\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(f\"üî• PyTorch {torch.__version__} ready!\")\n",
                "print(f\"üéØ Captum {captum.__version__} ready!\")\n",
                "print(f\"üñ•Ô∏è CUDA available: {torch.cuda.is_available()}\")\n",
                "print(f\"üéØ Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

# Add more comprehensive cells
additional_cells = [
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## 2. Device Detection and Compatibility\n",
            "\n",
            "Following repository standards for intelligent device management:"
        ]
    },
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "import platform\n",
            "\n",
            "def detect_device():\n",
            "    \"\"\"\n",
            "    Detect the best available PyTorch device with comprehensive hardware support.\n",
            "    \n",
            "    Priority order:\n",
            "    1. CUDA (NVIDIA GPUs) - Best performance for deep learning\n",
            "    2. MPS (Apple Silicon) - Optimized for M1/M2/M3 Macs  \n",
            "    3. CPU (Universal) - Always available fallback\n",
            "    \n",
            "    Returns:\n",
            "        torch.device: The optimal device for PyTorch operations\n",
            "        str: Human-readable device description for logging\n",
            "    \"\"\"\n",
            "    # Check for CUDA (NVIDIA GPU)\n",
            "    if torch.cuda.is_available():\n",
            "        device = torch.device(\"cuda\")\n",
            "        gpu_name = torch.cuda.get_device_name(0)\n",
            "        device_info = f\"CUDA GPU: {gpu_name}\"\n",
            "        \n",
            "        # Additional CUDA info for optimization\n",
            "        cuda_version = torch.version.cuda\n",
            "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
            "        \n",
            "        print(f\"üöÄ Using CUDA acceleration\")\n",
            "        print(f\"   GPU: {gpu_name}\")\n",
            "        print(f\"   CUDA Version: {cuda_version}\")\n",
            "        print(f\"   GPU Memory: {gpu_memory:.1f} GB\")\n",
            "        \n",
            "        return device, device_info\n",
            "    \n",
            "    # Check for MPS (Apple Silicon)\n",
            "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
            "        device = torch.device(\"mps\")\n",
            "        device_info = \"Apple Silicon MPS\"\n",
            "        \n",
            "        # Get system info for Apple Silicon\n",
            "        system_info = platform.uname()\n",
            "        \n",
            "        print(f\"üçé Using Apple Silicon MPS acceleration\")\n",
            "        print(f\"   System: {system_info.system} {system_info.release}\")\n",
            "        print(f\"   Machine: {system_info.machine}\")\n",
            "        print(f\"   Processor: {system_info.processor}\")\n",
            "        \n",
            "        return device, device_info\n",
            "    \n",
            "    # Fallback to CPU\n",
            "    else:\n",
            "        device = torch.device(\"cpu\")\n",
            "        device_info = \"CPU (No GPU acceleration available)\"\n",
            "        \n",
            "        # Get CPU info for optimization guidance\n",
            "        cpu_count = torch.get_num_threads()\n",
            "        system_info = platform.uname()\n",
            "        \n",
            "        print(f\"üíª Using CPU (no GPU acceleration detected)\")\n",
            "        print(f\"   Processor: {system_info.processor}\")\n",
            "        print(f\"   PyTorch Threads: {cpu_count}\")\n",
            "        print(f\"   System: {system_info.system} {system_info.release}\")\n",
            "        \n",
            "        # Provide optimization suggestions for CPU-only setups\n",
            "        print(f\"\\nüí° CPU Optimization Tips:\")\n",
            "        print(f\"   ‚Ä¢ Reduce batch size to prevent memory issues\")\n",
            "        print(f\"   ‚Ä¢ Consider using smaller models for faster inference\")\n",
            "        print(f\"   ‚Ä¢ Enable PyTorch optimizations: torch.set_num_threads({cpu_count})\")\n",
            "        \n",
            "        return device, device_info\n",
            "\n",
            "# Usage in the notebook\n",
            "device, device_info = detect_device()\n",
            "print(f\"\\n‚úÖ PyTorch device selected: {device}\")\n",
            "print(f\"üìä Device info: {device_info}\")\n",
            "\n",
            "# Set global device for the notebook\n",
            "DEVICE = device"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## 3. TensorBoard Setup for Captum Analysis\n",
            "\n",
            "Following repository standards for comprehensive logging:"
        ]
    },
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Platform-specific TensorBoard log directory setup\n",
            "def get_run_logdir(run_name=\"captum_analysis\"):\n",
            "    \"\"\"Generate unique log directory for this Captum analysis run.\"\"\"\n",
            "    \n",
            "    if IS_COLAB:\n",
            "        # Google Colab: Save logs to /content/tensorboard_logs\n",
            "        root_logdir = \"/content/tensorboard_logs\"\n",
            "    elif IS_KAGGLE:\n",
            "        # Kaggle: Save logs to ./tensorboard_logs/\n",
            "        root_logdir = \"./tensorboard_logs\"\n",
            "    else:\n",
            "        # Local: Save logs to ./tensorboard_logs/\n",
            "        root_logdir = \"./tensorboard_logs\"\n",
            "    \n",
            "    # Create directory if it doesn't exist\n",
            "    os.makedirs(root_logdir, exist_ok=True)\n",
            "    \n",
            "    # Generate unique run directory with timestamp\n",
            "    now = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
            "    run_logdir = os.path.join(root_logdir, f\"{run_name}_{now}\")\n",
            "    \n",
            "    return run_logdir\n",
            "\n",
            "# Generate unique log directory for this Captum session\n",
            "log_dir = get_run_logdir(\"australian_captum_analysis\")\n",
            "writer = SummaryWriter(log_dir=log_dir)\n",
            "\n",
            "print(f\"üìä TensorBoard logging initialized\")\n",
            "print(f\"üìÅ Log directory: {log_dir}\")\n",
            "print(f\"\\nüí° To view logs after running:\")\n",
            "if IS_COLAB:\n",
            "    print(f\"   In Google Colab:\")\n",
            "    print(f\"   1. Run: %load_ext tensorboard\")\n",
            "    print(f\"   2. Run: %tensorboard --logdir {log_dir}\")\n",
            "elif IS_KAGGLE:\n",
            "    print(f\"   In Kaggle:\")\n",
            "    print(f\"   1. Download logs from: {log_dir}\")\n",
            "    print(f\"   2. Run locally: tensorboard --logdir ./tensorboard_logs\")\n",
            "else:\n",
            "    print(f\"   Locally:\")\n",
            "    print(f\"   1. Run: tensorboard --logdir {log_dir}\")\n",
            "    print(f\"   2. Open http://localhost:6006 in browser\")"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## 4. Core Concepts of Captum: Attribution Types\n",
            "\n",
            "Understanding the three main types of attribution in Captum:"
        ]
    },
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Create a visual overview of Captum attribution types\n",
            "plt.figure(figsize=(14, 8))\n",
            "plt.suptitle('üéØ Captum Attribution Types for Model Interpretability', fontsize=16, fontweight='bold')\n",
            "\n",
            "# Define attribution types with Australian examples\n",
            "attribution_info = {\n",
            "    'Feature Attribution': {\n",
            "        'description': 'Which parts of input most important?',\n",
            "        'examples': ['Which pixels identify Sydney Opera House?', \n",
            "                    'Which words indicate positive tourism review?',\n",
            "                    'Which features predict Melbourne vs Sydney?'],\n",
            "        'algorithms': ['Integrated Gradients', 'Occlusion', 'SHAP', 'Saliency'],\n",
            "        'color': '#FF6B35'  # PyTorch Orange\n",
            "    },\n",
            "    'Layer Attribution': {\n",
            "        'description': 'How do hidden layers contribute?',\n",
            "        'examples': ['Which conv layer detects harbor features?',\n",
            "                    'How does BERT layer process \"Uluru\"?', \n",
            "                    'Which neurons activate for beach scenes?'],\n",
            "        'algorithms': ['Grad-CAM', 'Layer Gradients', 'Layer Conductance'],\n",
            "        'color': '#004E89'  # PyTorch Navy\n",
            "    },\n",
            "    'Neuron Attribution': {\n",
            "        'description': 'Individual neuron contribution?',\n",
            "        'examples': ['Which neuron fires for kangaroo detection?',\n",
            "                    'How does specific neuron respond to \"Sydney\"?',\n",
            "                    'Individual neuron role in classification?'],\n",
            "        'algorithms': ['Neuron Gradients', 'Neuron Conductance', 'Neuron Integrated Gradients'],\n",
            "        'color': '#6C757D'  # Neutral Gray\n",
            "    }\n",
            "}\n",
            "\n",
            "# Create subplots for each attribution type\n",
            "for i, (attr_type, info) in enumerate(attribution_info.items(), 1):\n",
            "    plt.subplot(1, 3, i)\n",
            "    \n",
            "    # Create a simple visual representation\n",
            "    y_pos = np.arange(len(info['examples']))\n",
            "    plt.barh(y_pos, [0.8, 0.6, 0.9], color=info['color'], alpha=0.7)\n",
            "    \n",
            "    plt.yticks(y_pos, [f\"Ex {j+1}\" for j in range(len(info['examples']))])\n",
            "    plt.xlabel('Relevance Score')\n",
            "    plt.title(f\"{attr_type}\\n{info['description']}\", fontweight='bold')\n",
            "    plt.xlim(0, 1)\n",
            "    \n",
            "    # Add algorithm info at the bottom\n",
            "    algorithms_text = ', '.join(info['algorithms'][:2]) + '...'\n",
            "    plt.text(0.5, -0.8, f\"Algorithms: {algorithms_text}\", \n",
            "             transform=plt.gca().transAxes, ha='center', fontsize=8, \n",
            "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=info['color'], alpha=0.2))\n",
            "\n",
            "plt.tight_layout()\n",
            "plt.show()\n",
            "\n",
            "print(\"üìö Captum Attribution Types Overview:\")\n",
            "print(\"\\nüéØ In this notebook, we'll focus on:\")\n",
            "print(\"   1. Feature Attribution - Understanding input importance\")\n",
            "print(\"   2. Layer Attribution - Analyzing hidden layer contributions\")\n",
            "print(\"   3. Interactive Captum Insights - Browser-based exploration\")\n",
            "\n",
            "# Log the attribution overview to TensorBoard\n",
            "writer.add_text('Captum Overview', \n",
            "                'This session analyzes Australian tourism images using Captum attribution methods')\n",
            "\n",
            "# Create multilingual context examples\n",
            "attribution_examples = {\n",
            "    'english': [\n",
            "        \"Sydney Opera House architectural features detection\",\n",
            "        \"Melbourne coffee shop sentiment analysis\", \n",
            "        \"Gold Coast beach scene classification\"\n",
            "    ],\n",
            "    'vietnamese': [\n",
            "        \"Ph√°t hi·ªán ƒë·∫∑c ƒëi·ªÉm ki·∫øn tr√∫c Nh√† h√°t Opera Sydney\",\n",
            "        \"Ph√¢n t√≠ch c·∫£m x√∫c qu√°n c√† ph√™ Melbourne\",\n",
            "        \"Ph√¢n lo·∫°i c·∫£nh b√£i bi·ªÉn Gold Coast\"\n",
            "    ]\n",
            "}\n",
            "\n",
            "print(\"\\nüåè Multilingual Examples (English-Vietnamese):\")\n",
            "for eng, vie in zip(attribution_examples['english'], attribution_examples['vietnamese']):\n",
            "    print(f\"   EN: {eng}\")\n",
            "    print(f\"   VI: {vie}\")\n",
            "    print()"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## 5. Prepare Pre-trained Model and Australian Sample Images\n",
            "\n",
            "We'll use a pre-trained ResNet model to analyze images with Australian context:"
        ]
    },
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Load pre-trained ResNet model for image classification\n",
            "print(\"üîÑ Loading pre-trained ResNet-18 model...\")\n",
            "\n",
            "# Load model and move to device\n",
            "model = models.resnet18(pretrained=True)\n",
            "model = model.to(DEVICE)\n",
            "model.eval()  # Set to evaluation mode\n",
            "\n",
            "print(f\"‚úÖ ResNet-18 loaded successfully on {DEVICE}\")\n",
            "print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
            "\n",
            "# Define ImageNet preprocessing transforms\n",
            "# These are the standard ImageNet normalization values\n",
            "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
            "                                std=[0.229, 0.224, 0.225])\n",
            "\n",
            "transform = transforms.Compose([\n",
            "    transforms.Resize((224, 224)),\n",
            "    transforms.ToTensor(),\n",
            "    normalize\n",
            "])\n",
            "\n",
            "# Also create transform without normalization for visualization\n",
            "transform_no_norm = transforms.Compose([\n",
            "    transforms.Resize((224, 224)),\n",
            "    transforms.ToTensor()\n",
            "])\n",
            "\n",
            "print(\"üñºÔ∏è Image preprocessing transforms ready\")\n",
            "print(\"   ‚Ä¢ Resize to 224x224\")\n",
            "print(\"   ‚Ä¢ Convert to tensor\")\n",
            "print(\"   ‚Ä¢ Normalize with ImageNet statistics\")"
        ]
    }
]

# Add the additional cells to the notebook
notebook['cells'].extend(additional_cells)

# Save the notebook
notebook_path = "/home/runner/work/pytorch-mastery/pytorch-mastery/examples/pytorch-tutorials/07_pytorch_captum.ipynb"

# Remove the existing incomplete file
import os
if os.path.exists(notebook_path):
    os.remove(notebook_path)

# Write the complete notebook
with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, indent=2, ensure_ascii=False)

print(f"‚úÖ Created comprehensive Captum notebook at: {notebook_path}")
print(f"üìù Notebook contains {len(notebook['cells'])} cells")