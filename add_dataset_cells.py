#!/usr/bin/env python3
"""
Complete the Character-Level RNN notebook with all remaining cells
"""

import json

def complete_char_rnn_notebook():
    """Add the remaining cells to complete the notebook."""
    
    # Load existing notebook
    with open("examples/pytorch-nlp/classify-names-character-level-RNN.ipynb", "r") as f:
        notebook = json.load(f)
    
    # Cell 6: Australian Dataset Creation
    notebook["cells"].append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## üá¶üá∫ Australian Names and Locations Dataset\n",
            "\n",
            "We'll create a comprehensive dataset featuring Australian names classified by ethnic origin, and Australian locations classified by type. This follows the repository's Australian context policy while providing practical multilingual examples."
        ]
    })
    
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "def create_australian_names_dataset():\n",
            "    \"\"\"\n",
            "    Create Australian names dataset with ethnic origin classification.\n",
            "    Includes both English and Vietnamese names commonly found in Australia.\n",
            "    \"\"\"\n",
            "    \n",
            "    # Australian names by ethnic origin\n",
            "    names_by_origin = {\n",
            "        'English': [\n",
            "            'Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Wilson', 'Davis', 'Taylor',\n",
            "            'Anderson', 'Thomas', 'Jackson', 'White', 'Harris', 'Martin', 'Thompson',\n",
            "            'Garcia', 'Martinez', 'Robinson', 'Clark', 'Lewis', 'Walker', 'Hall',\n",
            "            'Allen', 'Young', 'King', 'Wright', 'Lopez', 'Hill', 'Scott', 'Green',\n",
            "            'Adams', 'Baker', 'Gonzalez', 'Nelson', 'Carter', 'Mitchell', 'Perez'\n",
            "        ],\n",
            "        'Irish': [\n",
            "            'Murphy', 'Kelly', 'Sullivan', 'Walsh', 'Smith', 'OBrien', 'Byrne', 'Ryan',\n",
            "            'Connor', 'ONeill', 'Reilly', 'Doyle', 'McCarthy', 'Gallagher', 'Doherty',\n",
            "            'Kennedy', 'Lynch', 'Murray', 'Quinn', 'Moore', 'McLaughlin', 'Carroll',\n",
            "            'Connolly', 'Daly', 'Connell', 'Wilson', 'Dunne', 'Brennan', 'Burke',\n",
            "            'Collins', 'Campbell', 'Clarke', 'Johnston', 'Hughes', 'Farrell'\n",
            "        ],\n",
            "        'Italian': [\n",
            "            'Rossi', 'Russo', 'Ferrari', 'Esposito', 'Bianchi', 'Romano', 'Colombo',\n",
            "            'Ricci', 'Marino', 'Greco', 'Bruno', 'Gallo', 'Conti', 'DeLuca',\n",
            "            'Mancini', 'Costa', 'Giordano', 'Rizzo', 'Lombardi', 'Moretti',\n",
            "            'Barbieri', 'Fontana', 'Santoro', 'Mariani', 'Rinaldi', 'Caruso',\n",
            "            'Ferrara', 'Galli', 'Martini', 'Leone', 'Longo', 'Gentile', 'Martinelli'\n",
            "        ],\n",
            "        'Greek': [\n",
            "            'Papadopoulos', 'Georgiou', 'Dimitriou', 'Andreou', 'Nikolaou', 'Christou',\n",
            "            'Ioannou', 'Constantinou', 'Antoniou', 'Savva', 'Charalambous', 'Stylianou',\n",
            "            'Petrou', 'Michaelidou', 'Hadjisavvas', 'Kokkinos', 'Stavrou', 'Loizou',\n",
            "            'Panayiotou', 'Economou', 'Demetriou', 'Philippou', 'Vassiliou', 'Kyprianou',\n",
            "            'Theodorou', 'Christodoulou', 'Anastasiadou', 'Hadjiconstantinou'\n",
            "        ],\n",
            "        'Vietnamese': [\n",
            "            'Nguyen', 'Tran', 'Le', 'Pham', 'Hoang', 'Huynh', 'Vo', 'Vu', 'Dang', 'Bui',\n",
            "            'Do', 'Ho', 'Ngo', 'Duong', 'Ly', 'Trinh', 'Dinh', 'Thai', 'Cao', 'Lam',\n",
            "            'Phan', 'Truong', 'Tang', 'Doan', 'Mai', 'Ton', 'Ha', 'Chau', 'Bach', 'Kim',\n",
            "            'Luu', 'Ong', 'Tong', 'Quan', 'Dam', 'Khang', 'Thang', 'Phung', 'Duc', 'Vinh'\n",
            "        ],\n",
            "        'Chinese': [\n",
            "            'Wang', 'Li', 'Zhang', 'Liu', 'Chen', 'Yang', 'Huang', 'Zhao', 'Wu', 'Zhou',\n",
            "            'Xu', 'Sun', 'Ma', 'Zhu', 'Hu', 'Guo', 'He', 'Lin', 'Gao', 'Luo',\n",
            "            'Zheng', 'Liang', 'Xie', 'Tang', 'Song', 'Xu', 'Han', 'Feng', 'Deng', 'Cao',\n",
            "            'Peng', 'Zeng', 'Xiao', 'Tian', 'Pan', 'Cheng', 'Wei', 'Jiang', 'Yu', 'Shi'\n",
            "        ]\n",
            "    }\n",
            "    \n",
            "    # Flatten the dataset\n",
            "    names_data = []\n",
            "    for origin, names in names_by_origin.items():\n",
            "        for name in names:\n",
            "            names_data.append((name, origin))\n",
            "    \n",
            "    return names_data\n",
            "\n",
            "def create_australian_locations_dataset():\n",
            "    \"\"\"\n",
            "    Create Australian locations dataset classified by type.\n",
            "    Includes cities, suburbs, landmarks, and natural features.\n",
            "    \"\"\"\n",
            "    \n",
            "    locations_by_type = {\n",
            "        'City': [\n",
            "            'Sydney', 'Melbourne', 'Brisbane', 'Perth', 'Adelaide', 'Darwin', 'Hobart', 'Canberra',\n",
            "            'Newcastle', 'Wollongong', 'Geelong', 'Townsville', 'Cairns', 'Ballarat', 'Bendigo',\n",
            "            'Albury', 'Wodonga', 'Shepparton', 'Wagga', 'Rockhampton', 'Bundaberg', 'Hervey',\n",
            "            'Toowoomba', 'Mackay', 'Gladstone', 'Warrnambool', 'Mildura', 'Launceston'\n",
            "        ],\n",
            "        'Suburb': [\n",
            "            'Bondi', 'Manly', 'Paddington', 'Surry', 'Newtown', 'Leichhardt', 'Balmain',\n",
            "            'Toorak', 'Brighton', 'Camberwell', 'Hawthorn', 'Richmond', 'Fitzroy', 'Carlton',\n",
            "            'Southbank', 'Docklands', 'Fortitude', 'Paddington', 'Milton', 'Ascot',\n",
            "            'Cottesloe', 'Subiaco', 'Fremantle', 'Scarborough', 'Joondalup', 'Midland'\n",
            "        ],\n",
            "        'Landmark': [\n",
            "            'Opera', 'Harbour', 'Luna', 'Royal', 'Federation', 'Parliament', 'Story', 'Shrine',\n",
            "            'Botanic', 'Observatory', 'Anzac', 'War', 'National', 'Australian', 'Museum',\n",
            "            'Gallery', 'Library', 'University', 'Stadium', 'Arena', 'Centre', 'Tower',\n",
            "            'Bridge', 'Wharf', 'Market', 'Square', 'Gardens', 'Reserve', 'Park'\n",
            "        ],\n",
            "        'Natural': [\n",
            "            'Uluru', 'Kakadu', 'Daintree', 'Grampians', 'Flinders', 'Cradle', 'Freycinet',\n",
            "            'Wilsons', 'Kosciuszko', 'Alpine', 'Snowy', 'Murray', 'Darling', 'Cooper',\n",
            "            'Murrumbidgee', 'Lachlan', 'Macquarie', 'Hawkesbury', 'Yarra', 'Maribyrnong',\n",
            "            'Barwon', 'Goulburn', 'Campaspe', 'Loddon', 'Wimmera', 'Glenelg', 'Torrens'\n",
            "        ]\n",
            "    }\n",
            "    \n",
            "    # Flatten the dataset\n",
            "    locations_data = []\n",
            "    for location_type, locations in locations_by_type.items():\n",
            "        for location in locations:\n",
            "            locations_data.append((location, location_type))\n",
            "    \n",
            "    return locations_data\n",
            "\n",
            "# Create the datasets\n",
            "names_data = create_australian_names_dataset()\n",
            "locations_data = create_australian_locations_dataset()\n",
            "\n",
            "print(\"üá¶üá∫ Australian Names and Locations Dataset Created\")\n",
            "print(\"=\" * 55)\n",
            "print(f\"   Names dataset: {len(names_data)} entries\")\n",
            "print(f\"   Locations dataset: {len(locations_data)} entries\")\n",
            "\n",
            "# Show sample data\n",
            "print(\"\\nüìù Sample Names by Origin:\")\n",
            "names_df = pd.DataFrame(names_data, columns=['Name', 'Origin'])\n",
            "for origin in ['English', 'Vietnamese', 'Greek']:\n",
            "    samples = names_df[names_df['Origin'] == origin]['Name'].head(3).tolist()\n",
            "    print(f\"   {origin}: {', '.join(samples)}\")\n",
            "\n",
            "print(\"\\nüèõÔ∏è Sample Locations by Type:\")\n",
            "locations_df = pd.DataFrame(locations_data, columns=['Location', 'Type'])\n",
            "for loc_type in ['City', 'Suburb', 'Landmark', 'Natural']:\n",
            "    samples = locations_df[locations_df['Type'] == loc_type]['Location'].head(3).tolist()\n",
            "    print(f\"   {loc_type}: {', '.join(samples)}\")\n",
            "\n",
            "# Distribution visualization\n",
            "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
            "\n",
            "# Names distribution\n",
            "sns.countplot(data=names_df, x='Origin', ax=ax1)\n",
            "ax1.set_title('Australian Names by Ethnic Origin')\n",
            "ax1.tick_params(axis='x', rotation=45)\n",
            "\n",
            "# Locations distribution\n",
            "sns.countplot(data=locations_df, x='Type', ax=ax2)\n",
            "ax2.set_title('Australian Locations by Type')\n",
            "ax2.tick_params(axis='x', rotation=45)\n",
            "\n",
            "plt.tight_layout()\n",
            "plt.show()\n",
            "\n",
            "print(\"\\n‚úÖ Datasets prepared for character-level RNN training!\")"
        ]
    })
    
    # Cell 8: Character Processing
    notebook["cells"].append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## üî§ Character-Level Text Processing\n",
            "\n",
            "Character-level processing is fundamental to our RNN. We'll create utilities to:\n",
            "\n",
            "1. **Build character vocabulary** from both English and Vietnamese text\n",
            "2. **Convert text to tensors** and vice versa\n",
            "3. **Handle Unicode characters** for multilingual support\n",
            "4. **Normalize text** for consistent processing"
        ]
    })
    
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "class CharacterProcessor:\n",
            "    \"\"\"\n",
            "    Character-level text processing for Australian multilingual names and locations.\n",
            "    \n",
            "    Handles both ASCII and Unicode characters for English-Vietnamese support.\n",
            "    Comparable to TensorFlow's text preprocessing but with explicit character control.\n",
            "    \"\"\"\n",
            "    \n",
            "    def __init__(self):\n",
            "        self.all_letters = string.ascii_letters + \" .,;'-\"\n",
            "        self.n_letters = len(self.all_letters)\n",
            "        self.letter_to_index = {}\n",
            "        self.index_to_letter = {}\n",
            "        \n",
            "    def unicode_to_ascii(self, text):\n",
            "        \"\"\"\n",
            "        Convert Unicode characters to ASCII for Vietnamese names.\n",
            "        \n",
            "        Examples:\n",
            "        - 'Nguy·ªÖn' -> 'Nguyen'\n",
            "        - 'Tr·∫ßn' -> 'Tran'\n",
            "        \"\"\"\n",
            "        return ''.join(\n",
            "            c for c in unicodedata.normalize('NFD', text)\n",
            "            if unicodedata.category(c) != 'Mn'\n",
            "            and c in self.all_letters\n",
            "        )\n",
            "    \n",
            "    def build_vocabulary(self, text_data):\n",
            "        \"\"\"\n",
            "        Build character vocabulary from text data.\n",
            "        \n",
            "        Args:\n",
            "            text_data: List of (text, label) tuples\n",
            "        \"\"\"\n",
            "        all_characters = set()\n",
            "        \n",
            "        # Collect all unique characters\n",
            "        for text, _ in text_data:\n",
            "            normalized = self.unicode_to_ascii(text)\n",
            "            all_characters.update(normalized)\n",
            "        \n",
            "        # Sort for consistency\n",
            "        self.all_letters = ''.join(sorted(all_characters))\n",
            "        self.n_letters = len(self.all_letters)\n",
            "        \n",
            "        # Build mappings\n",
            "        self.letter_to_index = {letter: i for i, letter in enumerate(self.all_letters)}\n",
            "        self.index_to_letter = {i: letter for i, letter in enumerate(self.all_letters)}\n",
            "        \n",
            "        print(f\"üìù Character vocabulary built:\")\n",
            "        print(f\"   Unique characters: {self.n_letters}\")\n",
            "        print(f\"   Character set: {self.all_letters[:50]}{'...' if len(self.all_letters) > 50 else ''}\")\n",
            "    \n",
            "    def text_to_tensor(self, text):\n",
            "        \"\"\"\n",
            "        Convert text to PyTorch tensor.\n",
            "        \n",
            "        TensorFlow equivalent:\n",
            "            tf.strings.unicode_decode(text, 'UTF-8')\n",
            "        \n",
            "        Args:\n",
            "            text: Input text string\n",
            "            \n",
            "        Returns:\n",
            "            torch.Tensor: Character indices tensor\n",
            "        \"\"\"\n",
            "        normalized = self.unicode_to_ascii(text)\n",
            "        indices = [self.letter_to_index.get(char, 0) for char in normalized]\n",
            "        return torch.tensor(indices, dtype=torch.long)\n",
            "    \n",
            "    def tensor_to_text(self, tensor):\n",
            "        \"\"\"\n",
            "        Convert tensor back to text string.\n",
            "        \n",
            "        Args:\n",
            "            tensor: PyTorch tensor of character indices\n",
            "            \n",
            "        Returns:\n",
            "            str: Reconstructed text\n",
            "        \"\"\"\n",
            "        indices = tensor.cpu().numpy() if tensor.is_cuda else tensor.numpy()\n",
            "        return ''.join([self.index_to_letter.get(int(idx), '') for idx in indices])\n",
            "    \n",
            "    def char_to_onehot(self, char_index, device=None):\n",
            "        \"\"\"\n",
            "        Convert character index to one-hot vector.\n",
            "        \n",
            "        Args:\n",
            "            char_index: Index of character\n",
            "            device: PyTorch device for tensor\n",
            "            \n",
            "        Returns:\n",
            "            torch.Tensor: One-hot encoded vector\n",
            "        \"\"\"\n",
            "        if device is None:\n",
            "            device = torch.device('cpu')\n",
            "            \n",
            "        onehot = torch.zeros(self.n_letters, device=device)\n",
            "        if 0 <= char_index < self.n_letters:\n",
            "            onehot[char_index] = 1\n",
            "        return onehot\n",
            "    \n",
            "    def text_to_onehot_sequence(self, text, device=None):\n",
            "        \"\"\"\n",
            "        Convert text to sequence of one-hot vectors.\n",
            "        \n",
            "        Args:\n",
            "            text: Input text\n",
            "            device: PyTorch device\n",
            "            \n",
            "        Returns:\n",
            "            torch.Tensor: Sequence tensor [seq_len, vocab_size]\n",
            "        \"\"\"\n",
            "        if device is None:\n",
            "            device = torch.device('cpu')\n",
            "            \n",
            "        normalized = self.unicode_to_ascii(text)\n",
            "        sequence_length = len(normalized)\n",
            "        \n",
            "        # Create tensor to hold the sequence\n",
            "        onehot_sequence = torch.zeros(sequence_length, self.n_letters, device=device)\n",
            "        \n",
            "        for i, char in enumerate(normalized):\n",
            "            char_idx = self.letter_to_index.get(char, 0)\n",
            "            onehot_sequence[i][char_idx] = 1\n",
            "            \n",
            "        return onehot_sequence\n",
            "\n",
            "# Create character processor and build vocabulary\n",
            "char_processor = CharacterProcessor()\n",
            "\n",
            "# Combine both datasets for vocabulary building\n",
            "all_data = names_data + locations_data\n",
            "char_processor.build_vocabulary(all_data)\n",
            "\n",
            "# Test character processing\n",
            "print(\"\\nüß™ Testing Character Processing:\")\n",
            "test_names = ['Nguyen', 'Papadopoulos', 'Sydney', 'Uluru']\n",
            "for name in test_names:\n",
            "    tensor = char_processor.text_to_tensor(name)\n",
            "    reconstructed = char_processor.tensor_to_text(tensor)\n",
            "    print(f\"   '{name}' -> {tensor.tolist()} -> '{reconstructed}'\")\n",
            "\n",
            "# Show character vocabulary details\n",
            "print(f\"\\nüìö Character Vocabulary Details:\")\n",
            "print(f\"   Total characters: {char_processor.n_letters}\")\n",
            "print(f\"   Character mapping sample: {dict(list(char_processor.letter_to_index.items())[:10])}\")\n",
            "\n",
            "print(\"\\n‚úÖ Character processing system ready!\")"
        ]
    })
    
    # Save the updated notebook
    with open("examples/pytorch-nlp/classify-names-character-level-RNN.ipynb", "w") as f:
        json.dump(notebook, f, indent=2)
    
    print(f"‚úÖ Updated Character-Level RNN notebook with dataset and processing cells")

if __name__ == "__main__":
    complete_char_rnn_notebook()