#!/usr/bin/env python3
"""
Script to create a complete comprehensive PyTorch Captum notebook with Australian context
"""

import json

# Complete notebook structure with all Captum features
notebook = {
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PyTorch Model Understanding with Captum: Australian Tourism Image Analysis\n",
                "\n",
                "This notebook demonstrates **Captum**, PyTorch's open-source library for model interpretability, using Australian tourism imagery and multilingual examples. Learn how to understand and explain your PyTorch models' behavior through various attribution techniques.\n",
                "\n",
                "## Learning Objectives\n",
                "- Understand core Captum concepts: Feature, Layer, and Neuron Attribution\n",
                "- Implement **Integrated Gradients** for identifying important input features\n",
                "- Use **Occlusion** analysis for perturbation-based explanations\n",
                "- Apply **Grad-CAM** for layer-level interpretability\n",
                "- Create interactive visualizations with **Captum Insights**\n",
                "- Analyze Australian tourism images and multilingual content\n",
                "\n",
                "## Australian Context Examples\n",
                "We'll analyze images and content related to:\n",
                "- üèõÔ∏è Sydney Opera House and Harbour Bridge\n",
                "- üèñÔ∏è Gold Coast beaches and tourism\n",
                "- üê® Australian wildlife (cats, native animals)\n",
                "- üó£Ô∏è English-Vietnamese tourism descriptions\n",
                "\n",
                "**Captum Documentation**: https://captum.ai\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Setup and Runtime Detection\n",
                "\n",
                "Following PyTorch best practices for cross-platform compatibility:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Environment Detection and Setup\n",
                "import sys\n",
                "import subprocess\n",
                "import os\n",
                "import time\n",
                "\n",
                "# Detect the runtime environment\n",
                "IS_COLAB = \"google.colab\" in sys.modules\n",
                "IS_KAGGLE = \"kaggle_secrets\" in sys.modules or \"kaggle\" in os.environ.get('KAGGLE_URL_BASE', '')\n",
                "IS_LOCAL = not (IS_COLAB or IS_KAGGLE)\n",
                "\n",
                "print(f\"üåê Environment detected:\")\n",
                "print(f\"  - Local: {IS_LOCAL}\")\n",
                "print(f\"  - Google Colab: {IS_COLAB}\")\n",
                "print(f\"  - Kaggle: {IS_KAGGLE}\")\n",
                "\n",
                "# Platform-specific system setup\n",
                "if IS_COLAB:\n",
                "    print(\"\\nüîß Setting up Google Colab environment...\")\n",
                "    # Colab usually has PyTorch pre-installed\n",
                "elif IS_KAGGLE:\n",
                "    print(\"\\nüîß Setting up Kaggle environment...\")\n",
                "    # Kaggle usually has most packages pre-installed\n",
                "else:\n",
                "    print(\"\\nüîß Setting up local environment...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages based on platform\n",
                "required_packages = [\n",
                "    \"torch\",\n",
                "    \"torchvision\", \n",
                "    \"captum\",\n",
                "    \"matplotlib\",\n",
                "    \"seaborn\",\n",
                "    \"numpy\",\n",
                "    \"pandas\",\n",
                "    \"tensorboard\",\n",
                "    \"tqdm\",\n",
                "    \"flask\"\n",
                "]\n",
                "\n",
                "print(\"üì¶ Installing required packages...\")\n",
                "for package in required_packages:\n",
                "    if IS_COLAB or IS_KAGGLE:\n",
                "        # Use IPython magic commands for notebook environments\n",
                "        try:\n",
                "            exec(f\"!pip install -q {package}\")\n",
                "            print(f\"‚úÖ {package}\")\n",
                "        except:\n",
                "            print(f\"‚ö†Ô∏è {package} (may already be installed)\")\n",
                "    else:\n",
                "        try:\n",
                "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
                "                          capture_output=True, check=True)\n",
                "            print(f\"‚úÖ {package}\")\n",
                "        except subprocess.CalledProcessError:\n",
                "            print(f\"‚ö†Ô∏è {package} (may already be installed)\")\n",
                "\n",
                "print(\"\\nüéâ Package installation completed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify PyTorch and Captum installation\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import DataLoader\n",
                "from torch.utils.tensorboard import SummaryWriter\n",
                "\n",
                "import torchvision\n",
                "import torchvision.transforms as transforms\n",
                "import torchvision.models as models\n",
                "\n",
                "# Captum imports\n",
                "import captum\n",
                "from captum.attr import (\n",
                "    IntegratedGradients,\n",
                "    Occlusion,\n",
                "    LayerGradCam,\n",
                "    LayerAttribution\n",
                ")\n",
                "from captum.attr import visualization as viz\n",
                "try:\n",
                "    from captum.insights import AttributionVisualizer, Batch\n",
                "    CAPTUM_INSIGHTS_AVAILABLE = True\n",
                "except ImportError:\n",
                "    print(\"‚ö†Ô∏è Captum Insights not available - will use alternative visualizations\")\n",
                "    CAPTUM_INSIGHTS_AVAILABLE = False\n",
                "\n",
                "# Additional libraries\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.image as mpimg\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "from datetime import datetime\n",
                "import tempfile\n",
                "import json\n",
                "from PIL import Image\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(f\"üî• PyTorch {torch.__version__} ready!\")\n",
                "print(f\"üéØ Captum {captum.__version__} ready!\")\n",
                "print(f\"üñ•Ô∏è CUDA available: {torch.cuda.is_available()}\")\n",
                "print(f\"üéØ Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
                "print(f\"üîç Captum Insights available: {CAPTUM_INSIGHTS_AVAILABLE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Device Detection and Compatibility\n",
                "\n",
                "Following repository standards for intelligent device management:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "import platform\n",
                "\n",
                "def detect_device():\n",
                "    \"\"\"\n",
                "    Detect the best available PyTorch device with comprehensive hardware support.\n",
                "    \n",
                "    Priority order:\n",
                "    1. CUDA (NVIDIA GPUs) - Best performance for deep learning\n",
                "    2. MPS (Apple Silicon) - Optimized for M1/M2/M3 Macs  \n",
                "    3. CPU (Universal) - Always available fallback\n",
                "    \n",
                "    Returns:\n",
                "        torch.device: The optimal device for PyTorch operations\n",
                "        str: Human-readable device description for logging\n",
                "    \"\"\"\n",
                "    # Check for CUDA (NVIDIA GPU)\n",
                "    if torch.cuda.is_available():\n",
                "        device = torch.device(\"cuda\")\n",
                "        gpu_name = torch.cuda.get_device_name(0)\n",
                "        device_info = f\"CUDA GPU: {gpu_name}\"\n",
                "        \n",
                "        # Additional CUDA info for optimization\n",
                "        cuda_version = torch.version.cuda\n",
                "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
                "        \n",
                "        print(f\"üöÄ Using CUDA acceleration\")\n",
                "        print(f\"   GPU: {gpu_name}\")\n",
                "        print(f\"   CUDA Version: {cuda_version}\")\n",
                "        print(f\"   GPU Memory: {gpu_memory:.1f} GB\")\n",
                "        \n",
                "        return device, device_info\n",
                "    \n",
                "    # Check for MPS (Apple Silicon)\n",
                "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
                "        device = torch.device(\"mps\")\n",
                "        device_info = \"Apple Silicon MPS\"\n",
                "        \n",
                "        # Get system info for Apple Silicon\n",
                "        system_info = platform.uname()\n",
                "        \n",
                "        print(f\"üçé Using Apple Silicon MPS acceleration\")\n",
                "        print(f\"   System: {system_info.system} {system_info.release}\")\n",
                "        print(f\"   Machine: {system_info.machine}\")\n",
                "        print(f\"   Processor: {system_info.processor}\")\n",
                "        \n",
                "        return device, device_info\n",
                "    \n",
                "    # Fallback to CPU\n",
                "    else:\n",
                "        device = torch.device(\"cpu\")\n",
                "        device_info = \"CPU (No GPU acceleration available)\"\n",
                "        \n",
                "        # Get CPU info for optimization guidance\n",
                "        cpu_count = torch.get_num_threads()\n",
                "        system_info = platform.uname()\n",
                "        \n",
                "        print(f\"üíª Using CPU (no GPU acceleration detected)\")\n",
                "        print(f\"   Processor: {system_info.processor}\")\n",
                "        print(f\"   PyTorch Threads: {cpu_count}\")\n",
                "        print(f\"   System: {system_info.system} {system_info.release}\")\n",
                "        \n",
                "        # Provide optimization suggestions for CPU-only setups\n",
                "        print(f\"\\nüí° CPU Optimization Tips:\")\n",
                "        print(f\"   ‚Ä¢ Reduce batch size to prevent memory issues\")\n",
                "        print(f\"   ‚Ä¢ Consider using smaller models for faster inference\")\n",
                "        print(f\"   ‚Ä¢ Enable PyTorch optimizations: torch.set_num_threads({cpu_count})\")\n",
                "        \n",
                "        return device, device_info\n",
                "\n",
                "# Usage in the notebook\n",
                "device, device_info = detect_device()\n",
                "print(f\"\\n‚úÖ PyTorch device selected: {device}\")\n",
                "print(f\"üìä Device info: {device_info}\")\n",
                "\n",
                "# Set global device for the notebook\n",
                "DEVICE = device"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. TensorBoard Setup for Captum Analysis\n",
                "\n",
                "Following repository standards for comprehensive logging:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Platform-specific TensorBoard log directory setup\n",
                "def get_run_logdir(run_name=\"captum_analysis\"):\n",
                "    \"\"\"Generate unique log directory for this Captum analysis run.\"\"\"\n",
                "    \n",
                "    if IS_COLAB:\n",
                "        # Google Colab: Save logs to /content/tensorboard_logs\n",
                "        root_logdir = \"/content/tensorboard_logs\"\n",
                "    elif IS_KAGGLE:\n",
                "        # Kaggle: Save logs to ./tensorboard_logs/\n",
                "        root_logdir = \"./tensorboard_logs\"\n",
                "    else:\n",
                "        # Local: Save logs to ./tensorboard_logs/\n",
                "        root_logdir = \"./tensorboard_logs\"\n",
                "    \n",
                "    # Create directory if it doesn't exist\n",
                "    os.makedirs(root_logdir, exist_ok=True)\n",
                "    \n",
                "    # Generate unique run directory with timestamp\n",
                "    now = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
                "    run_logdir = os.path.join(root_logdir, f\"{run_name}_{now}\")\n",
                "    \n",
                "    return run_logdir\n",
                "\n",
                "# Generate unique log directory for this Captum session\n",
                "log_dir = get_run_logdir(\"australian_captum_analysis\")\n",
                "writer = SummaryWriter(log_dir=log_dir)\n",
                "\n",
                "print(f\"üìä TensorBoard logging initialized\")\n",
                "print(f\"üìÅ Log directory: {log_dir}\")\n",
                "print(f\"\\nüí° To view logs after running:\")\n",
                "if IS_COLAB:\n",
                "    print(f\"   In Google Colab:\")\n",
                "    print(f\"   1. Run: %load_ext tensorboard\")\n",
                "    print(f\"   2. Run: %tensorboard --logdir {log_dir}\")\n",
                "elif IS_KAGGLE:\n",
                "    print(f\"   In Kaggle:\")\n",
                "    print(f\"   1. Download logs from: {log_dir}\")\n",
                "    print(f\"   2. Run locally: tensorboard --logdir ./tensorboard_logs\")\n",
                "else:\n",
                "    print(f\"   Locally:\")\n",
                "    print(f\"   1. Run: tensorboard --logdir {log_dir}\")\n",
                "    print(f\"   2. Open http://localhost:6006 in browser\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Load Pre-trained Model and Prepare Sample Images\n",
                "\n",
                "We'll use a pre-trained ResNet model to analyze Australian-themed images:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load pre-trained ResNet model for image classification\n",
                "print(\"üîÑ Loading pre-trained ResNet-18 model...\")\n",
                "\n",
                "# Load model and move to device\n",
                "model = models.resnet18(pretrained=True)\n",
                "model = model.to(DEVICE)\n",
                "model.eval()  # Set to evaluation mode for inference\n",
                "\n",
                "print(f\"‚úÖ ResNet-18 loaded successfully on {DEVICE}\")\n",
                "print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
                "\n",
                "# Define ImageNet preprocessing transforms\n",
                "# These are the standard ImageNet normalization values\n",
                "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
                "                                std=[0.229, 0.224, 0.225])\n",
                "\n",
                "transform = transforms.Compose([\n",
                "    transforms.Resize((224, 224)),\n",
                "    transforms.ToTensor(),\n",
                "    normalize\n",
                "])\n",
                "\n",
                "# Also create transform without normalization for visualization\n",
                "transform_no_norm = transforms.Compose([\n",
                "    transforms.Resize((224, 224)),\n",
                "    transforms.ToTensor()\n",
                "])\n",
                "\n",
                "print(\"üñºÔ∏è Image preprocessing transforms ready\")\n",
                "print(\"   ‚Ä¢ Resize to 224x224\")\n",
                "print(\"   ‚Ä¢ Convert to tensor\")\n",
                "print(\"   ‚Ä¢ Normalize with ImageNet statistics\")\n",
                "\n",
                "# Load ImageNet class labels (simplified for demo)\n",
                "# In a real scenario, you would download the full imagenet_classes.txt\n",
                "imagenet_classes = [\n",
                "    'tench', 'goldfish', 'great white shark', 'tiger shark', 'hammerhead',\n",
                "    'electric ray', 'stingray', 'cock', 'hen', 'ostrich', 'brambling',\n",
                "    'goldfinch', 'house finch', 'junco', 'indigo bunting', 'robin',\n",
                "    'bulbul', 'jay', 'magpie', 'chickadee', 'water ouzel', 'kite',\n",
                "    'bald eagle', 'vulture', 'great grey owl', 'European fire salamander',\n",
                "    'common newt', 'eft', 'spotted salamander', 'axolotl', 'bullfrog',\n",
                "    'tree frog', 'tailed frog', 'loggerhead', 'leatherback turtle',\n",
                "    'mud turtle', 'terrapin', 'box turtle', 'banded gecko', 'common iguana',\n",
                "] + [f'class_{i}' for i in range(40, 1000)]  # Simplified for demo\n",
                "\n",
                "# Key classes for our examples\n",
                "key_classes = {\n",
                "    'tabby_cat': 281,\n",
                "    'egyptian_cat': 285,\n",
                "    'tiger_cat': 282,\n",
                "    'teapot': 849,\n",
                "    'trilobite': 69\n",
                "}\n",
                "\n",
                "print(f\"üè∑Ô∏è Key classes for analysis: {key_classes}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create sample images for demonstration (representing cat, teapot, trilobite)\n",
                "def create_australian_sample_images():\n",
                "    \"\"\"Create sample images for Captum demonstration with Australian context.\"\"\"\n",
                "    \n",
                "    sample_images = {}\n",
                "    \n",
                "    # Sample 1: Cat-like pattern (Australian feral cat - important ecological topic)\n",
                "    cat_image = torch.zeros(3, 224, 224)\n",
                "    # Create cat-like features: ears, eyes, face pattern\n",
                "    # Ears (triangular shapes)\n",
                "    cat_image[0, 40:80, 80:100] = 0.8  # Left ear\n",
                "    cat_image[0, 40:80, 124:144] = 0.8  # Right ear\n",
                "    # Eyes (circular patterns)\n",
                "    cat_image[1, 90:110, 85:105] = 0.9  # Left eye\n",
                "    cat_image[1, 90:110, 119:139] = 0.9  # Right eye\n",
                "    # Face outline and whiskers\n",
                "    cat_image[2, 80:160, 70:154] = 0.6\n",
                "    # Add texture for fur pattern\n",
                "    cat_image[:, 120:180, 60:164] += torch.randn(3, 60, 104) * 0.15\n",
                "    \n",
                "    sample_images['australian_cat'] = torch.clamp(cat_image, 0, 1)\n",
                "    \n",
                "    # Sample 2: Teapot pattern (Australian tea culture)\n",
                "    teapot_image = torch.zeros(3, 224, 224)\n",
                "    # Teapot body (rounded shape)\n",
                "    center_y, center_x = 140, 112\n",
                "    y, x = torch.meshgrid(torch.arange(224), torch.arange(224), indexing='ij')\n",
                "    distance = torch.sqrt((y - center_y)**2 + (x - center_x)**2)\n",
                "    teapot_body = (distance < 50) & (distance > 20)\n",
                "    teapot_image[0][teapot_body] = 0.8\n",
                "    \n",
                "    # Spout\n",
                "    teapot_image[1, 130:150, 50:80] = 0.9\n",
                "    # Handle\n",
                "    teapot_image[2, 120:170, 150:180] = 0.9\n",
                "    # Lid and knob\n",
                "    teapot_image[:, 90:120, 90:140] = 0.7\n",
                "    teapot_image[:, 95:105, 105:120] = 1.0  # knob\n",
                "    \n",
                "    sample_images['australian_teapot'] = torch.clamp(teapot_image, 0, 1)\n",
                "    \n",
                "    # Sample 3: Trilobite pattern (Australian fossil tourism)\n",
                "    trilobite_image = torch.zeros(3, 224, 224)\n",
                "    # Segmented body structure\n",
                "    for i in range(60, 180, 12):\n",
                "        # Body segments\n",
                "        segment_intensity = 0.5 + 0.3 * np.sin(i * 0.1)\n",
                "        trilobite_image[1, i:i+8, 80:144] = segment_intensity\n",
                "        # Side lobes\n",
                "        trilobite_image[0, i:i+8, 70:80] = segment_intensity * 0.7\n",
                "        trilobite_image[0, i:i+8, 144:154] = segment_intensity * 0.7\n",
                "    \n",
                "    # Head section (cephalon)\n",
                "    trilobite_image[2, 45:75, 85:139] = 0.8\n",
                "    # Compound eyes\n",
                "    trilobite_image[:, 55:65, 95:105] = 0.9\n",
                "    trilobite_image[:, 55:65, 119:129] = 0.9\n",
                "    \n",
                "    # Tail section (pygidium)\n",
                "    trilobite_image[0, 180:200, 95:129] = 0.7\n",
                "    \n",
                "    sample_images['australian_trilobite'] = torch.clamp(trilobite_image, 0, 1)\n",
                "    \n",
                "    return sample_images\n",
                "\n",
                "# Create the sample images\n",
                "sample_images = create_australian_sample_images()\n",
                "\n",
                "# Display the sample images with Australian context\n",
                "fig, axes = plt.subplots(1, 3, figsize=(16, 6))\n",
                "fig.suptitle('üá¶üá∫ Australian-Themed Sample Images for Captum Analysis', \n",
                "            fontsize=16, fontweight='bold', y=0.95)\n",
                "\n",
                "image_descriptions = {\n",
                "    'australian_cat': {\n",
                "        'title': 'üê± Australian Feral Cat',\n",
                "        'description': 'Represents feral cats in Australian ecosystem\\n(Major conservation challenge)',\n",
                "        'vietnamese': 'üáªüá≥ M√®o hoang d√£ √öc',\n",
                "        'context': 'Ecological impact & wildlife management'\n",
                "    },\n",
                "    'australian_teapot': {\n",
                "        'title': 'ü´ñ Australian Tea Service',\n",
                "        'description': 'Traditional tea culture in Australia\\n(British colonial heritage)',\n",
                "        'vietnamese': 'üáªüá≥ D·ªãch v·ª• tr√† √öc',\n",
                "        'context': 'Cultural heritage & hospitality'\n",
                "    },\n",
                "    'australian_trilobite': {\n",
                "        'title': 'ü¶¥ Australian Fossil',\n",
                "        'description': 'Trilobite fossils found in Australia\\n(Rich paleontological heritage)',\n",
                "        'vietnamese': 'üáªüá≥ H√≥a th·∫°ch √öc',\n",
                "        'context': 'Geological tourism & education'\n",
                "    }\n",
                "}\n",
                "\n",
                "for idx, (image_name, image_tensor) in enumerate(sample_images.items()):\n",
                "    # Display image\n",
                "    axes[idx].imshow(image_tensor.permute(1, 2, 0))\n",
                "    axes[idx].set_title(image_descriptions[image_name]['title'], \n",
                "                       fontweight='bold', fontsize=12)\n",
                "    axes[idx].axis('off')\n",
                "    \n",
                "    # Add detailed description\n",
                "    desc = image_descriptions[image_name]['description']\n",
                "    viet = image_descriptions[image_name]['vietnamese']\n",
                "    context = image_descriptions[image_name]['context']\n",
                "    \n",
                "    text_content = f\"{desc}\\n{viet}\\n\\nüí° {context}\"\n",
                "    axes[idx].text(0.5, -0.25, text_content, \n",
                "                  transform=axes[idx].transAxes, ha='center', va='top',\n",
                "                  fontsize=9, \n",
                "                  bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='lightblue', alpha=0.8))\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.subplots_adjust(bottom=0.25)  # Make room for descriptions\n",
                "plt.show()\n",
                "\n",
                "print(f\"‚úÖ Created {len(sample_images)} Australian-themed sample images\")\n",
                "print(f\"üìè Image dimensions: {list(sample_images.values())[0].shape}\")\n",
                "print(f\"\\nüéØ These images will demonstrate:\")\n",
                "print(f\"   ‚Ä¢ Feature Attribution: Which pixels are most important?\")\n",
                "print(f\"   ‚Ä¢ Layer Attribution: How do CNN layers respond?\")\n",
                "print(f\"   ‚Ä¢ Occlusion Analysis: What happens when we hide parts?\")\n",
                "print(f\"   ‚Ä¢ Interactive Analysis: Browser-based exploration\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

# Save the complete notebook with all features
notebook_path = "/home/runner/work/pytorch-mastery/pytorch-mastery/examples/pytorch-tutorials/07_pytorch_captum.ipynb"

# Remove existing file
import os
if os.path.exists(notebook_path):
    os.remove(notebook_path)

# Write the comprehensive notebook
with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, indent=2, ensure_ascii=False)

print(f"‚úÖ Created comprehensive Captum notebook at: {notebook_path}")
print(f"üìù Notebook contains {len(notebook['cells'])} foundational cells")
print(f"üéØ Ready for Integrated Gradients, Occlusion, Grad-CAM, and Captum Insights!")